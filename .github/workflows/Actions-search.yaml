name: Daily FactSet Search - Taiwan Financial Data

on:
  workflow_run:
    workflows: ["Update Stock Lists"]  # Trigger after Update Lists completes
    types:
      - completed

  workflow_dispatch:  # Allow manual trigger
    inputs:
      company_count:
        description: "Number of results per company"
        required: false
        default: "4"
        type: string
      min_quality:
        description: "Minimum quality threshold"
        required: false
        default: "7"
        type: string
      specific_companies:
        description: "Specific companies to search (comma-separated, leave empty for all)"
        required: false
        default: ""
        type: string

env:
  SEARCH_RATE_LIMIT_PER_SECOND: "1.0"
  SEARCH_DAILY_QUOTA: "500"
  MIN_QUALITY_THRESHOLD: "7"
  LOG_LEVEL: "INFO"

jobs:
  daily-search:
    runs-on: ubuntu-latest
    # Only run if Update Lists succeeded (or manual trigger)
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        batch: [1, 2, 3, 4]  # Split into 4 parallel jobs
    
    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 1
    
    - name: Setup Python Environment
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"
        cache: "pip"
    
    - name: Install Dependencies
      run: |
        echo "Installing Python dependencies..."
        pip install --upgrade pip
        pip install google-api-python-client requests beautifulsoup4 python-dotenv
        echo "Dependencies installed successfully"
    
    - name: Validate Required Files
      run: |
        echo "Checking required files..."
        if [ ! -f "StockID_TWSE_TPEX.csv" ]; then
          echo "StockID_TWSE_TPEX.csv not found!"
          exit 1
        fi
        if [ ! -f "search_group/search_cli.py" ]; then
          echo "search_cli.py not found!"
          exit 1
        fi
        echo "Required files found"
    
    - name: Create Directory Structure
      run: |
        echo "Creating directory structure..."
        mkdir -p data/md
        mkdir -p cache/search
        mkdir -p logs/search
        echo "Directories created"
    
    - name: Setup Environment Variables
      run: |
        echo "Setting up environment variables..."
        echo "GOOGLE_SEARCH_API_KEY=${{ secrets.GOOGLE_SEARCH_API_KEY }}" >> .env
        echo "GOOGLE_SEARCH_API_KEY1=${{ secrets.GOOGLE_SEARCH_API_KEY1 }}" >> .env
        echo "GOOGLE_SEARCH_API_KEY2=${{ secrets.GOOGLE_SEARCH_API_KEY2 }}" >> .env
        echo "GOOGLE_SEARCH_API_KEY3=${{ secrets.GOOGLE_SEARCH_API_KEY3 }}" >> .env
        echo "GOOGLE_SEARCH_API_KEY4=${{ secrets.GOOGLE_SEARCH_API_KEY4 }}" >> .env
        echo "GOOGLE_SEARCH_API_KEY5=${{ secrets.GOOGLE_SEARCH_API_KEY5 }}" >> .env
        echo "GOOGLE_SEARCH_API_KEY6=${{ secrets.GOOGLE_SEARCH_API_KEY6 }}" >> .env
        echo "GOOGLE_SEARCH_CSE_ID=${{ secrets.GOOGLE_SEARCH_CSE_ID }}" >> .env
        echo "GOOGLE_SEARCH_CSE_ID1=${{ secrets.GOOGLE_SEARCH_CSE_ID1 }}" >> .env
        echo "GOOGLE_SEARCH_CSE_ID2=${{ secrets.GOOGLE_SEARCH_CSE_ID2 }}" >> .env
        echo "GOOGLE_SEARCH_CSE_ID3=${{ secrets.GOOGLE_SEARCH_CSE_ID3 }}" >> .env
        echo "GOOGLE_SEARCH_CSE_ID4=${{ secrets.GOOGLE_SEARCH_CSE_ID4 }}" >> .env
        echo "GOOGLE_SEARCH_CSE_ID5=${{ secrets.GOOGLE_SEARCH_CSE_ID5 }}" >> .env
        echo "GOOGLE_SEARCH_CSE_ID6=${{ secrets.GOOGLE_SEARCH_CSE_ID6 }}" >> .env
        echo "SEARCH_RATE_LIMIT_PER_SECOND=${{ env.SEARCH_RATE_LIMIT_PER_SECOND }}" >> .env
        echo "SEARCH_DAILY_QUOTA=${{ env.SEARCH_DAILY_QUOTA }}" >> .env
        echo "MIN_QUALITY_THRESHOLD=${{ env.MIN_QUALITY_THRESHOLD }}" >> .env
        echo "LOG_LEVEL=${{ env.LOG_LEVEL }}" >> .env
        echo "Environment configured"
    
    - name: Validate API Setup
      continue-on-error: true
      run: |
        echo "Validating API setup..."
        python search_group/search_cli.py validate || echo "‚ö†Ô∏è Validation warning: Initial check failed, but search will proceed and attempt recovery."
        echo "API validation step finished"
        
    - name: Execute Daily Search
      id: search
      run: |
        echo "Starting daily Taiwan financial data search (Batch ${{ matrix.batch }}/4)..."
        
        COMPANY_COUNT="${{ github.event.inputs.company_count || '2' }}"
        MIN_QUALITY="${{ github.event.inputs.min_quality || '7' }}"
        SPECIFIC_COMPANIES="${{ github.event.inputs.specific_companies }}"
        
        echo "Search Parameters:"
        echo "  - Count per company: $COMPANY_COUNT"
        echo "  - Minimum quality: $MIN_QUALITY"
        
        # Calculate batch symbols if no specific companies provided
        if [ -n "$SPECIFIC_COMPANIES" ]; then
          TARGET_BATCH="$SPECIFIC_COMPANIES"
          echo "  - Target: Specific companies ($SPECIFIC_COMPANIES)"
        else
          # Extract all symbols from CSV (skipping header)
          # Use mapfile/readarray to handle lines properly
          mapfile -t ALL_SYMBOLS < <(tail -n +2 StockID_TWSE_TPEX.csv | cut -d',' -f1 | tr -d '\r')
          TOTAL_SYMBOLS=${#ALL_SYMBOLS[@]}
          
          # Calculate batch slice
          BATCH_NUM=${{ matrix.batch }}
          TOTAL_BATCHES=4
          
          # Calculate size per batch (ceiling division)
          BATCH_SIZE=$(( ($TOTAL_SYMBOLS + $TOTAL_BATCHES - 1) / $TOTAL_BATCHES ))
          
          START_IDX=$(( ($BATCH_NUM - 1) * $BATCH_SIZE ))
          
          # Extract slice
          BATCH_ARRAY=("${ALL_SYMBOLS[@]:$START_IDX:$BATCH_SIZE}")
          
          # Join with commas
          IFS=,
          TARGET_BATCH="${BATCH_ARRAY[*]}"
          
          echo "  - Target: Batch $BATCH_NUM/$TOTAL_BATCHES"
          echo "  - Total Symbols: $TOTAL_SYMBOLS"
          echo "  - Batch Size: ${#BATCH_ARRAY[@]}"
          echo "  - Start Index: $START_IDX"
        fi
        
        if [ -z "$TARGET_BATCH" ]; then
          echo "‚ö†Ô∏è No symbols found for this batch. Skipping search."
          exit 0
        fi
        
        # Run search with --batch argument
        echo "Executing search for batch..."
        python search_group/search_cli.py search --batch "$TARGET_BATCH" --count "$COMPANY_COUNT" --min-quality "$MIN_QUALITY"
    
    - name: Post-Search Analysis
      run: |
        echo "Post-search analysis..."

        echo ""
        echo "Final Status:"
        python search_group/search_cli.py status

        echo ""
        echo "Generated Files:"
        if [ -d "data/md" ]; then
          MD_COUNT=$(find data/md -name "*.md" | wc -l)
          echo "  - Total MD files: $MD_COUNT"

          if [ $MD_COUNT -gt 0 ]; then
            echo "  - Latest files:"
            find data/md -name "*.md" -type f -printf '%T@ %p\n' 2>/dev/null | sort -nr | head -5 | awk '{print "    - " $2}' || echo "    - (file listing unavailable)"

            echo ""
            echo "File Size Summary:"
            du -sh data/md/ 2>/dev/null | awk '{print "  - Total size: " $1}' || echo "  - Size calculation unavailable"
          fi
        fi

        echo ""
        echo "Log Summary:"
        if [ -d "logs/search" ]; then
          LOG_FILES=$(find logs/search -name "*.log" | wc -l)
          echo "  - Log files: $LOG_FILES"

          if find logs/search -name "*.log" -exec grep -l "ERROR\|CRITICAL" {} \; | head -1 >/dev/null 2>&1; then
            echo "  - Recent errors found in logs"
          else
            echo "  - No critical errors in logs"
          fi
        fi

    - name: Generate CSV for Quarantine Detection
      run: |
        echo "üìä Generating CSV for quarantine detection..."
        python process_group/process_cli.py generate-csv
    
    - name: Prepare Artifacts
      if: always()
      run: |
        echo "Preparing artifacts..."
        
        mkdir -p artifacts/search-results
        mkdir -p artifacts/logs
        mkdir -p artifacts/cache
        
        if [ -d "data/md" ] && [ "$(ls -A data/md 2>/dev/null)" ]; then
          cp -r data/md/* artifacts/search-results/ 2>/dev/null || echo "No MD files to copy"
        fi
        
        if [ -d "logs/search" ] && [ "$(ls -A logs/search 2>/dev/null)" ]; then
          cp -r logs/search/* artifacts/logs/ 2>/dev/null || echo "No log files to copy"
        fi
        
        if [ -d "cache/search" ]; then
          find cache/search -name "*.json" -exec cp {} artifacts/cache/ \; 2>/dev/null || echo "No cache files to copy"
        fi
        
        TAIWAN_TIME=$(TZ='Asia/Taipei' date '+%Y-%m-%d %H:%M:%S %Z')
        UTC_TIME=$(date -u '+%Y-%m-%d %H:%M:%S %Z')
        MD_FILES=$(find artifacts/search-results -name "*.md" 2>/dev/null | wc -l)
        LOG_FILES=$(find artifacts/logs -name "*.log" 2>/dev/null | wc -l)
        CACHE_FILES=$(find artifacts/cache -name "*.json" 2>/dev/null | wc -l)
        TOTAL_SIZE=$(du -sh artifacts/ 2>/dev/null | awk '{print $1}')
        
        echo "Creating summary report..."
        {
          echo "FactSet Daily Search Summary"
          echo "==========================="
          echo "Search Date: $TAIWAN_TIME"
          echo "Taiwan Time: $TAIWAN_TIME"
          echo "UTC Time: $UTC_TIME"
          echo ""
          echo "Parameters:"
          echo "- Company Count: ${{ github.event.inputs.company_count || '2' }}"
          echo "- Min Quality: ${{ github.event.inputs.min_quality || '7' }}"
          echo "- Search Exit Code: ${{ steps.search.outputs.search_exit_code }}"
          echo ""
          echo "Results:"
          echo "- MD Files Generated: $MD_FILES"
          echo "- Log Files: $LOG_FILES"
          echo "- Cache Files: $CACHE_FILES"
          echo ""
          echo "Total Artifact Size: $TOTAL_SIZE"
        } > artifacts/search-summary.txt
        
        echo "Artifacts prepared"
    
    - name: Commit generated search results
      if: always() # Commit even if previous steps failed, to save partial results or for debugging
      run: |
        echo "Committing generated search results..."

        git config --global user.name "github-actions[bot]"
        git config --global user.email "41898282+github-actions[bot]@users.noreply.github.com"

        # Stage MD files and CSV
        git add data/md/*.md || true
        git add data/reports/factset_detailed_report_latest.csv || true

        if git diff --staged --quiet; then
          echo "No search changes to commit"
        else
          # Commit FIRST
          git commit -m "üîç Search Group v3.5.0-pure-hash Results + CSV - $(date +%Y-%m-%d\ %H:%M:%S)"

          # Force clean the working directory to remove untracked/modified files (logs, cache)
          # that might prevent rebase. This respects the commit we just made.
          git reset --hard

          # Now pull and rebase
          MAX_RETRIES=10
          count=0
          until git push; do
            count=$((count + 1))
            if [ $count -ge $MAX_RETRIES ]; then
              echo "Failed to push after $MAX_RETRIES attempts"
              exit 1
            fi
            echo "Push failed (attempt $count), pulling and retrying..."
            sleep $(( 5 + $RANDOM % 15 ))
            git pull --rebase
          done
          echo "Search results and CSV committed and pushed"
        fi
    
    - name: Notify on Failure
      if: failure()
      run: |
        echo "Search job failed!"
        echo "Exit code: ${{ steps.search.outputs.search_exit_code }}"
        echo "Please check the logs for details."
    
    - name: Success Summary
      if: success()
      run: |
        echo "Daily Taiwan financial data search completed successfully!"
        echo ""
        echo "Summary:"
        echo "  - Search time: $(TZ='Asia/Taipei' date '+%Y-%m-%d %H:%M %Z')"
        echo "  - Files generated: $(find data/md -name "*.md" 2>/dev/null | wc -l)"
        echo "  - Quality threshold: ${{ github.event.inputs.min_quality || '7' }}"
        echo "  - Count per company: ${{ github.event.inputs.company_count || '2' }}"
        echo ""
        echo "Next scheduled run: Tomorrow at 2:30 AM Taiwan time"
