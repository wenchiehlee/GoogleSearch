name: FactSet Pipeline v3.3.1 - Python-Enhanced Workflow

on:
  push:
    branches:
      - main
  workflow_dispatch:
    inputs:
      execution_mode:
        description: 'Execution strategy'
        required: false
        default: 'intelligent'
        type: choice
        options:
          - 'intelligent'     # Smart execution with v3.3.1 fixes
          - 'conservative'    # High priority only, with delays
          - 'process_only'    # Process existing data only
          - 'test_only'       # Test v3.3.1 components without search
          - 'enhanced'        # Full v3.3.1 features
      priority_focus:
        description: 'Search priority level'
        required: false
        default: 'high_only'
        type: choice
        options:
          - 'high_only'       # Top priority companies only
          - 'top_30'          # Top 30 companies
          - 'balanced'        # All companies (managed rate limiting)
      memory_limit:
        description: 'Memory limit (MB)'
        required: false
        default: '2048'
        type: string
      wait_for_rate_limits:
        description: 'Wait time before search (minutes)'
        required: false
        default: '15'
        type: string
  schedule:
    # Run daily at 2:10 AM UTC (optimal for rate limit resets)
    - cron: "10 2 * * *"

env:
  PYTHONIOENCODING: utf-8
  FACTSET_PIPELINE_DEBUG: true
  FACTSET_PIPELINE_VERSION: "3.3.1"

jobs:
  # Enhanced preflight validation using Python (FIXED #8)
  preflight_v331:
    runs-on: ubuntu-latest
    outputs:
      validation_status: ${{ steps.python_validation.outputs.status }}
      rate_limited: ${{ steps.python_validation.outputs.rate_limited }}
      data_quality: ${{ steps.python_validation.outputs.data_quality }}
      recommended_action: ${{ steps.python_validation.outputs.action }}
      companies_available: ${{ steps.python_validation.outputs.companies }}
    
    steps:
    - name: üì• Checkout Repository
      uses: actions/checkout@v4
    
    - name: üêç Set up Python Environment
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: üì¶ Install Dependencies
      run: |
        pip install --upgrade pip
        pip install requests pandas python-dotenv beautifulsoup4 markdownify
        pip install gspread google-auth validators psutil
        echo "‚úÖ Python dependencies installed for v3.3.1"
    
    - name: üß™ Enhanced Python Validation (v3.3.1 - FIXED #8)
      id: python_validation
      env:
        GOOGLE_SEARCH_API_KEY: ${{ secrets.GOOGLE_SEARCH_API_KEY }}
        GOOGLE_SEARCH_CSE_ID: ${{ secrets.GOOGLE_SEARCH_CSE_ID }}
        GOOGLE_SHEETS_CREDENTIALS: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}
        GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
      run: |
        echo "üîç Running enhanced Python-based validation (v3.3.1)..."
        python << 'EOF'
        import sys
        import os
        import json
        from datetime import datetime
        
        def main():
            print("üöÄ Enhanced FactSet Pipeline v3.3.1 Validation")
            print("=" * 60)
            
            validation_results = {
                "timestamp": datetime.now().isoformat(),
                "version": "3.3.1",
                "status": "unknown",
                "rate_limited": False,
                "data_quality": "unknown",
                "action": "unknown",
                "companies": 0,
                "issues": [],
                "recommendations": []
            }
            
            try:
                # Test 1: Module imports (FIXED #4)
                print("\nüìã Testing v3.3.1 module imports...")
                try:
                    import factset_pipeline
                    import factset_search  
                    import data_processor
                    import config
                    print("‚úÖ All v3.3.1 modules import successfully")
                    
                    # Check for v3.3.1 specific features
                    v331_features = []
                    if hasattr(factset_pipeline, 'EnhancedFactSetPipeline'):
                        v331_features.append("Enhanced Pipeline")
                    if hasattr(factset_search, 'UnifiedRateLimitProtector'):
                        v331_features.append("Unified Rate Limiter")
                    if hasattr(data_processor, 'MemoryManager'):
                        v331_features.append("Memory Management")
                    
                    print(f"‚úÖ v3.3.1 features detected: {', '.join(v331_features)}")
                    
                except ImportError as e:
                    validation_results["issues"].append(f"Module import error: {e}")
                    print(f"‚ùå Module import failed: {e}")
                
                # Test 2: Configuration and companies (FIXED #7)
                print("\nüìä Testing enhanced configuration...")
                try:
                    import config
                    
                    # Test enhanced config loading
                    if hasattr(config, 'load_config_v330'):
                        config_data = config.load_config_v330()
                        print("‚úÖ Enhanced configuration loaded")
                    else:
                        print("‚ö†Ô∏è Using fallback configuration")
                        config_data = {"target_companies": []}
                    
                    # Test enhanced company download
                    if hasattr(config, 'download_target_companies_v330'):
                        companies = config.download_target_companies_v330()
                        if companies:
                            validation_results["companies"] = len(companies)
                            print(f"‚úÖ Downloaded {len(companies)} companies")
                            if len(companies) >= 100:
                                validation_results["data_quality"] = "excellent"
                            elif len(companies) >= 50:
                                validation_results["data_quality"] = "good"
                            else:
                                validation_results["data_quality"] = "limited"
                        else:
                            validation_results["issues"].append("No companies downloaded")
                            print("‚ö†Ô∏è No companies downloaded")
                    
                except Exception as e:
                    validation_results["issues"].append(f"Configuration error: {e}")
                    print(f"‚ö†Ô∏è Configuration test failed: {e}")
                
                # Test 3: Rate limiting status (FIXED #3)
                print("\nüîç Testing rate limiting status...")
                try:
                    # Simple API test without using quota
                    api_key = os.environ.get('GOOGLE_SEARCH_API_KEY')
                    cse_id = os.environ.get('GOOGLE_SEARCH_CSE_ID')
                    
                    if api_key and cse_id:
                        print("‚úÖ Search API credentials available")
                        # Don't actually test to save quota
                        validation_results["rate_limited"] = False
                    else:
                        validation_results["issues"].append("Missing search API credentials")
                        print("‚ùå Search API credentials missing")
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Rate limiting test error: {e}")
                
                # Test 4: Data quality assessment (FIXED #5)
                print("\nüìÑ Assessing existing data quality...")
                try:
                    import os
                    from pathlib import Path
                    
                    md_dir = Path("data/md")
                    if md_dir.exists():
                        md_files = list(md_dir.glob("*.md"))
                        file_count = len(md_files)
                        
                        if file_count > 0:
                            # Sample file quality check
                            quality_files = 0
                            for md_file in md_files[:10]:  # Check first 10 files
                                try:
                                    with open(md_file, 'r', encoding='utf-8') as f:
                                        content = f.read()
                                    if any(keyword in content.lower() for keyword in 
                                          ['factset', 'eps', 'ÂàÜÊûêÂ∏´', 'ÁõÆÊ®ôÂÉπ']):
                                        quality_files += 1
                                except:
                                    continue
                            
                            quality_ratio = quality_files / min(10, file_count)
                            print(f"‚úÖ Found {file_count} MD files, {quality_ratio:.1%} have financial content")
                            
                            if file_count >= 100 and quality_ratio >= 0.7:
                                validation_results["data_quality"] = "excellent"
                            elif file_count >= 50 and quality_ratio >= 0.5:
                                validation_results["data_quality"] = "good"
                            elif file_count >= 20:
                                validation_results["data_quality"] = "moderate"
                            else:
                                validation_results["data_quality"] = "limited"
                        else:
                            print("‚ÑπÔ∏è No existing MD files found")
                            validation_results["data_quality"] = "none"
                    else:
                        print("‚ÑπÔ∏è MD directory not found")
                        validation_results["data_quality"] = "none"
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Data quality assessment failed: {e}")
                
                # Test 5: Memory and performance readiness (FIXED #9)
                print("\nüíæ Testing memory management...")
                try:
                    import psutil
                    memory = psutil.virtual_memory()
                    available_gb = memory.available / (1024**3)
                    
                    print(f"‚úÖ Available memory: {available_gb:.1f} GB")
                    
                    if available_gb >= 4:
                        print("‚úÖ Sufficient memory for enhanced processing")
                    elif available_gb >= 2:
                        print("‚ö†Ô∏è Limited memory - will use conservative settings")
                        validation_results["recommendations"].append("Use lower memory limits")
                    else:
                        print("‚ùå Insufficient memory for processing")
                        validation_results["issues"].append("Low memory")
                        
                except ImportError:
                    print("‚ö†Ô∏è psutil not available for memory monitoring")
                except Exception as e:
                    print(f"‚ö†Ô∏è Memory test failed: {e}")
                
                # Determine overall status and recommendations
                print("\nüéØ Determining execution strategy...")
                
                if len(validation_results["issues"]) == 0:
                    validation_results["status"] = "excellent"
                    validation_results["action"] = "full_pipeline"
                    print("‚úÖ All systems ready for full v3.3.1 pipeline")
                elif len(validation_results["issues"]) <= 2:
                    validation_results["status"] = "good"
                    if validation_results["data_quality"] in ["excellent", "good"]:
                        validation_results["action"] = "process_existing"
                    else:
                        validation_results["action"] = "conservative_search"
                    print("‚úÖ System ready with minor issues")
                else:
                    validation_results["status"] = "issues"
                    validation_results["action"] = "test_only"
                    print("‚ö†Ô∏è Multiple issues detected - recommend test mode")
                
                # Output for GitHub Actions
                print(f"\nüìä Validation Summary:")
                print(f"   Status: {validation_results['status']}")
                print(f"   Data Quality: {validation_results['data_quality']}")
                print(f"   Recommended Action: {validation_results['action']}")
                print(f"   Companies Available: {validation_results['companies']}")
                print(f"   Issues: {len(validation_results['issues'])}")
                
                # Set GitHub Actions outputs
                print(f"::set-output name=status::{validation_results['status']}")
                print(f"::set-output name=rate_limited::{validation_results['rate_limited']}")
                print(f"::set-output name=data_quality::{validation_results['data_quality']}")
                print(f"::set-output name=action::{validation_results['action']}")
                print(f"::set-output name=companies::{validation_results['companies']}")
                
                return validation_results["status"] != "issues"
                
            except Exception as e:
                print(f"‚ùå Validation failed with error: {e}")
                print(f"::set-output name=status::error")
                print(f"::set-output name=action::skip")
                return False
        
        if __name__ == "__main__":
            success = main()
            sys.exit(0 if success else 1)
        EOF

  # Enhanced main pipeline execution (v3.3.1)
  pipeline_v331:
    runs-on: ubuntu-latest
    needs: preflight_v331
    if: needs.preflight_v331.outputs.validation_status != 'error'
    
    steps:
    - name: üì• Checkout Repository
      uses: actions/checkout@v4
    
    - name: üêç Set up Python Environment
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: üîß Install System Dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y wkhtmltopdf || echo "wkhtmltopdf installation optional"
        echo "‚úÖ System dependencies ready"
    
    - name: üì¶ Install Enhanced Python Dependencies (v3.3.1)
      run: |
        pip install --upgrade pip
        
        # Core requirements for v3.3.1 fixes
        pip install requests pandas gspread google-auth python-dotenv
        pip install beautifulsoup4 markdownify validators
        pip install psutil  # For memory management (FIXED #9)
        
        # Optional enhanced dependencies
        pip install googlesearch-python || echo "‚ö†Ô∏è googlesearch-python optional"
        pip install numpy || echo "‚ö†Ô∏è numpy optional"
        
        echo "‚úÖ Enhanced Python dependencies installed for v3.3.1"
    
    - name: üîß Enhanced Configuration Setup (v3.3.1)
      run: |
        python << 'EOF'
        print("üîß Setting up v3.3.1 enhanced configuration...")
        
        import os
        from pathlib import Path
        
        # Create v3.3.1 directory structure
        directories = [
            "data/csv", "data/md", "data/pdf", "data/processed",
            "logs", "backups", "temp"
        ]
        
        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)
            print(f"‚úÖ Created/verified: {directory}")
        
        # Enhanced configuration validation
        try:
            import config
            if hasattr(config, 'load_config_v330'):
                config_data = config.load_config_v330()
                print("‚úÖ v3.3.1 enhanced configuration loaded")
            else:
                print("‚ö†Ô∏è Using fallback configuration")
        except Exception as e:
            print(f"‚ö†Ô∏è Configuration setup warning: {e}")
        
        print("‚úÖ v3.3.1 configuration setup completed")
        EOF
    
    - name: ‚è∞ Smart Rate Limit Wait (v3.3.1)
      if: github.event.inputs.wait_for_rate_limits && github.event.inputs.wait_for_rate_limits != '0'
      run: |
        wait_minutes="${{ github.event.inputs.wait_for_rate_limits }}"
        echo "‚è∞ Smart wait for rate limits: ${wait_minutes} minutes (v3.3.1 optimized)"
        sleep $((wait_minutes * 60))
    
    - name: üöÄ Enhanced Pipeline Execution (v3.3.1 - FIXED #1-9)
      env:
        GOOGLE_SEARCH_API_KEY: ${{ secrets.GOOGLE_SEARCH_API_KEY }}
        GOOGLE_SEARCH_CSE_ID: ${{ secrets.GOOGLE_SEARCH_CSE_ID }}
        GOOGLE_SHEETS_CREDENTIALS: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}
        GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
        FACTSET_MEMORY_LIMIT: ${{ github.event.inputs.memory_limit || '2048' }}
      run: |
        python << 'EOF'
        import sys
        import os
        from datetime import datetime
        
        def main():
            print("üöÄ FactSet Pipeline v3.3.1 Enhanced Execution")
            print("=" * 70)
            print(f"üìÖ Started: {datetime.now().isoformat()}")
            
            # Get execution parameters
            execution_mode = "${{ github.event.inputs.execution_mode || 'intelligent' }}"
            priority_focus = "${{ github.event.inputs.priority_focus || 'high_only' }}"
            memory_limit = int("${{ github.event.inputs.memory_limit || '2048' }}")
            validation_status = "${{ needs.preflight_v331.outputs.validation_status }}"
            data_quality = "${{ needs.preflight_v331.outputs.data_quality }}"
            recommended_action = "${{ needs.preflight_v331.outputs.recommended_action }}"
            
            print(f"üéØ Execution Mode: {execution_mode}")
            print(f"üéØ Priority Focus: {priority_focus}")
            print(f"üíæ Memory Limit: {memory_limit}MB")
            print(f"üìä Data Quality: {data_quality}")
            print(f"üîÑ Recommended Action: {recommended_action}")
            print()
            
            try:
                # Import v3.3.1 enhanced pipeline (FIXED #4 - no circular imports)
                import factset_pipeline
                
                # Check if we have the enhanced pipeline
                if hasattr(factset_pipeline, 'EnhancedFactSetPipeline'):
                    print("‚úÖ Using v3.3.1 Enhanced Pipeline")
                    
                    # Initialize enhanced pipeline with memory management (FIXED #9)
                    pipeline = factset_pipeline.EnhancedFactSetPipeline()
                    
                    # Determine execution strategy based on validation
                    if recommended_action == "full_pipeline" and execution_mode in ["enhanced", "intelligent"]:
                        print("üöÄ Running full enhanced pipeline...")
                        
                        success = pipeline.run_complete_pipeline_v331(
                            force_all=False,
                            skip_phases=[],
                            execution_mode=execution_mode
                        )
                        
                    elif recommended_action == "process_existing" or execution_mode == "process_only":
                        print("üìä Processing existing data with v3.3.1 enhancements...")
                        
                        success = pipeline.run_complete_pipeline_v331(
                            force_all=False,
                            skip_phases=["search"],
                            execution_mode="process_only"
                        )
                        
                    elif recommended_action == "conservative_search" or execution_mode == "conservative":
                        print("üéØ Conservative search with v3.3.1 rate limiting...")
                        
                        success = pipeline.run_complete_pipeline_v331(
                            force_all=False,
                            skip_phases=[],
                            execution_mode="conservative"
                        )
                        
                    elif execution_mode == "test_only":
                        print("üß™ Testing v3.3.1 components without search...")
                        
                        success = pipeline.run_complete_pipeline_v331(
                            force_all=False,
                            skip_phases=["search"],
                            execution_mode="test_only"
                        )
                        
                    else:
                        print("üîÑ Smart execution based on validation results...")
                        
                        # Intelligent mode - adapt based on validation
                        if data_quality in ["excellent", "good"]:
                            skip_phases = []
                        else:
                            skip_phases = []
                            
                        success = pipeline.run_complete_pipeline_v331(
                            force_all=False,
                            skip_phases=skip_phases,
                            execution_mode="intelligent"
                        )
                    
                    if success:
                        print("\nüéâ v3.3.1 Enhanced Pipeline completed successfully!")
                        return True
                    else:
                        print("\n‚ö†Ô∏è v3.3.1 Pipeline completed with issues")
                        return False
                        
                else:
                    print("‚ùå v3.3.1 Enhanced Pipeline not available")
                    print("üí° Falling back to basic execution...")
                    
                    # Fallback to basic processing
                    import data_processor
                    if hasattr(data_processor, 'process_all_data_v331'):
                        success = data_processor.process_all_data_v331(force=True)
                        print("‚úÖ Basic processing completed" if success else "‚ö†Ô∏è Basic processing failed")
                        return success
                    else:
                        print("‚ùå No compatible processor found")
                        return False
                
            except Exception as e:
                print(f"‚ùå Pipeline execution failed: {e}")
                print("üîç Attempting error recovery...")
                
                # Enhanced error recovery (v3.3.1)
                try:
                    import data_processor
                    if hasattr(data_processor, 'process_all_data_v331'):
                        print("üîÑ Attempting data processing recovery...")
                        success = data_processor.process_all_data_v331(force=True)
                        if success:
                            print("‚úÖ Recovery successful - basic processing completed")
                            return True
                except Exception as recovery_error:
                    print(f"‚ùå Recovery failed: {recovery_error}")
                
                return False
        
        if __name__ == "__main__":
            success = main()
            sys.exit(0 if success else 1)
        EOF
    
    - name: üìä Enhanced Data Validation (v3.3.1 - FIXED #5)
      if: always()
      run: |
        python << 'EOF'
        print("üìä Running enhanced data validation (v3.3.1)...")
        
        import os
        from pathlib import Path
        from datetime import datetime
        
        validation_results = {
            "timestamp": datetime.now().isoformat(),
            "version": "3.3.1",
            "validation_passed": False,
            "files_validated": 0,
            "data_quality_score": 0,
            "recommendations": []
        }
        
        try:
            # Enhanced file validation (replacing bash logic - FIXED #8)
            md_dir = Path("data/md")
            processed_dir = Path("data/processed")
            
            # Count and validate MD files
            if md_dir.exists():
                md_files = list(md_dir.glob("*.md"))
                valid_md_files = 0
                
                for md_file in md_files:
                    try:
                        if md_file.stat().st_size > 100:  # At least 100 bytes
                            with open(md_file, 'r', encoding='utf-8') as f:
                                content = f.read()
                            
                            # Check for financial content
                            if any(keyword in content.lower() for keyword in 
                                  ['factset', 'eps', 'ÂàÜÊûêÂ∏´', 'ÁõÆÊ®ôÂÉπ', 'È†ê‰º∞']):
                                valid_md_files += 1
                    except Exception:
                        continue
                
                validation_results["files_validated"] = len(md_files)
                print(f"‚úÖ MD files: {valid_md_files}/{len(md_files)} with financial content")
            
            # Validate processed files (v3.3.0 format compliance)
            processed_files_found = []
            expected_files = [
                "portfolio_summary.csv",
                "detailed_data.csv", 
                "statistics.json"
            ]
            
            for expected_file in expected_files:
                file_path = processed_dir / expected_file
                if file_path.exists() and file_path.stat().st_size > 0:
                    processed_files_found.append(expected_file)
                    print(f"‚úÖ Found: {expected_file}")
                else:
                    print(f"‚ö†Ô∏è Missing or empty: {expected_file}")
            
            # Calculate data quality score
            md_score = min(4, (valid_md_files // 25) + 1) if valid_md_files > 0 else 0
            processed_score = len(processed_files_found)
            
            validation_results["data_quality_score"] = min(4, md_score + processed_score)
            
            # Determine if validation passed
            if valid_md_files >= 10 and len(processed_files_found) >= 1:
                validation_results["validation_passed"] = True
                print(f"‚úÖ Data validation PASSED (Score: {validation_results['data_quality_score']}/4)")
            else:
                print(f"‚ö†Ô∏è Data validation PARTIAL (Score: {validation_results['data_quality_score']}/4)")
            
            # Enhanced recommendations based on results
            if valid_md_files < 20:
                validation_results["recommendations"].append("Consider running search with higher priority_focus")
            
            if "portfolio_summary.csv" not in processed_files_found:
                validation_results["recommendations"].append("Run data processor to generate portfolio summary")
            
            if validation_results["data_quality_score"] >= 3:
                validation_results["recommendations"].append("Data quality is good - ready for sheets upload")
            
            print(f"\nüìä Validation Summary:")
            print(f"   Quality Score: {validation_results['data_quality_score']}/4")
            print(f"   Files Validated: {validation_results['files_validated']}")
            print(f"   Passed: {validation_results['validation_passed']}")
            
        except Exception as e:
            print(f"‚ùå Validation error: {e}")
            validation_results["validation_passed"] = False
        EOF
    
    - name: üíæ Smart Commit Strategy (v3.3.1)
      if: always()
      run: |
        python << 'EOF'
        import os
        import subprocess
        from datetime import datetime
        from pathlib import Path
        
        print("üíæ Smart commit strategy (v3.3.1)...")
        
        # Configure git
        subprocess.run(['git', 'config', '--global', 'user.name', 'github-actions[bot]'])
        subprocess.run(['git', 'config', '--global', 'user.email', '41898282+github-actions[bot]@users.noreply.github.com'])
        
        try:
            # Enhanced validation using Python (FIXED #8)
            commit_worthy = False
            commit_message_parts = []
            
            # Count valid files
            md_dir = Path("data/md")
            processed_dir = Path("data/processed")
            
            valid_md_count = 0
            if md_dir.exists():
                for md_file in md_dir.glob("*.md"):
                    if md_file.stat().st_size > 100:
                        valid_md_count += 1
            
            processed_files = []
            for file_name in ["portfolio_summary.csv", "detailed_data.csv", "statistics.json"]:
                file_path = processed_dir / file_name
                if file_path.exists() and file_path.stat().st_size > 0:
                    processed_files.append(file_name)
            
            # Smart commit decision
            if valid_md_count >= 50 and len(processed_files) >= 2:
                commit_worthy = True
                commit_message_parts.append(f"üèÜ Premium v3.3.1 data: {valid_md_count} MD files + {len(processed_files)} processed files")
            elif valid_md_count >= 20 and len(processed_files) >= 1:
                commit_worthy = True
                commit_message_parts.append(f"‚úÖ Quality v3.3.1 data: {valid_md_count} MD files + {len(processed_files)} processed files")
            elif valid_md_count >= 10:
                commit_worthy = True
                commit_message_parts.append(f"üìä Acceptable v3.3.1 data: {valid_md_count} MD files")
            else:
                print(f"‚ÑπÔ∏è Insufficient data quality for commit ({valid_md_count} MD files)")
            
            if commit_worthy:
                # Add files to git
                if valid_md_count > 0:
                    subprocess.run(['git', 'add', 'data/md/'], check=False)
                
                for processed_file in processed_files:
                    subprocess.run(['git', 'add', f'data/processed/{processed_file}'], check=False)
                
                # Add supporting files
                subprocess.run(['git', 'add', 'logs/', 'ËßÄÂØüÂêçÂñÆ.csv'], check=False)
                
                # Create comprehensive commit message
                execution_mode = "${{ github.event.inputs.execution_mode || 'intelligent' }}"
                
                commit_message = f"""{commit_message_parts[0]}

        üìä v3.3.1 Execution Summary:
        - Mode: {execution_mode}
        - Priority: ${{ github.event.inputs.priority_focus || 'high_only' }}
        - Memory Limit: ${{ github.event.inputs.memory_limit || '2048' }}MB
        - Validation: ${{ needs.preflight_v331.outputs.validation_status }}
        - Data Quality: ${{ needs.preflight_v331.outputs.data_quality }}

        üîß v3.3.1 Fixes Applied:
        - FIXED #1: Search cascade failure protection
        - FIXED #2: Performance optimization (pre-compiled regex)
        - FIXED #3: Unified rate limiting
        - FIXED #4: Circular import resolution
        - FIXED #5: Enhanced data aggregation
        - FIXED #8: Python-based workflow validation
        - FIXED #9: Memory management implementation

        üìà Results:
        - MD Files: {valid_md_count} with financial content
        - Processed: {', '.join(processed_files)}
        - Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} UTC"""
                
                # Commit and push
                result = subprocess.run(['git', 'commit', '-m', commit_message], capture_output=True, text=True)
                
                if result.returncode == 0:
                    push_result = subprocess.run(['git', 'push'], capture_output=True, text=True)
                    if push_result.returncode == 0:
                        print("üéâ v3.3.1 data committed and pushed successfully!")
                        print(f"üìä Quality: {valid_md_count} MD files + {len(processed_files)} processed files")
                    else:
                        print("‚ö†Ô∏è Commit succeeded but push failed")
                        print(push_result.stderr)
                else:
                    print("‚ÑπÔ∏è No new changes to commit")
            else:
                print("‚ÑπÔ∏è Data quality insufficient for commit - preserving previous good data")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Commit process error: {e}")
        EOF
    
    - name: üìà Enhanced Sheets Upload (v3.3.1)
      if: env.GOOGLE_SHEET_ID != ''
      env:
        GOOGLE_SHEETS_CREDENTIALS: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}
        GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
      run: |
        python << 'EOF'
        print("üìà Enhanced sheets upload (v3.3.1)...")
        
        try:
            import sheets_uploader
            
            # Check for v3.3.1 enhanced uploader
            if hasattr(sheets_uploader, 'upload_all_sheets_v330'):
                print("‚úÖ Using v3.3.1 enhanced sheets uploader")
                
                # Load configuration
                config = {
                    "input": {
                        "summary_csv": "data/processed/portfolio_summary.csv",
                        "detailed_csv": "data/processed/detailed_data.csv", 
                        "stats_json": "data/processed/statistics.json"
                    },
                    "sheets": {
                        "auto_backup": True,
                        "create_missing_sheets": True,
                        "sheet_names": {
                            "portfolio": "Portfolio Summary v3.3.1",
                            "detailed": "Detailed Data v3.3.1",
                            "statistics": "Statistics v3.3.1"
                        }
                    }
                }
                
                success = sheets_uploader.upload_all_sheets_v330(config)
                
                if success:
                    print("üéâ v3.3.1 sheets upload completed successfully!")
                else:
                    print("‚ö†Ô∏è v3.3.1 sheets upload had issues")
            else:
                print("‚ö†Ô∏è Using fallback sheets uploader")
                # Fallback logic here
                
        except Exception as e:
            print(f"‚ö†Ô∏è Sheets upload error: {e}")
        EOF
    
    - name: üìä Final Status Report (v3.3.1)
      if: always()
      run: |
        echo "## üìä FactSet Pipeline v3.3.1 Execution Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Version**: v3.3.1 (Enhanced Python Workflow - FIXED #8)" >> $GITHUB_STEP_SUMMARY
        echo "**Execution Mode**: ${{ github.event.inputs.execution_mode || 'intelligent' }}" >> $GITHUB_STEP_SUMMARY
        echo "**Priority Focus**: ${{ github.event.inputs.priority_focus || 'high_only' }}" >> $GITHUB_STEP_SUMMARY
        echo "**Memory Limit**: ${{ github.event.inputs.memory_limit || '2048' }}MB" >> $GITHUB_STEP_SUMMARY
        echo "**Validation Status**: ${{ needs.preflight_v331.outputs.validation_status }}" >> $GITHUB_STEP_SUMMARY
        echo "**Data Quality**: ${{ needs.preflight_v331.outputs.data_quality }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ -n "${{ secrets.GOOGLE_SHEET_ID }}" ]; then
          echo "üìà **Live Dashboard**: [View v3.3.1 Results](https://docs.google.com/spreadsheets/d/${{ secrets.GOOGLE_SHEET_ID }}/edit)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "### üîß v3.3.1 Comprehensive Fixes Applied" >> $GITHUB_STEP_SUMMARY
        echo "- ‚úÖ **FIXED #1**: Search cascade failure - Individual error isolation" >> $GITHUB_STEP_SUMMARY
        echo "- ‚úÖ **FIXED #2**: Performance issues - Pre-compiled regex & batching" >> $GITHUB_STEP_SUMMARY
        echo "- ‚úÖ **FIXED #3**: Rate limiting logic - Unified rate limiter" >> $GITHUB_STEP_SUMMARY
        echo "- ‚úÖ **FIXED #4**: Module import issues - Lazy loading resolved" >> $GITHUB_STEP_SUMMARY
        echo "- ‚úÖ **FIXED #5**: Data aggregation errors - Enhanced deduplication" >> $GITHUB_STEP_SUMMARY
        echo "- ‚úÖ **FIXED #8**: GitHub Actions - Python-based validation" >> $GITHUB_STEP_SUMMARY
        echo "- ‚úÖ **FIXED #9**: Memory management - Resource limits implemented" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        echo "### üìÅ Generated Files (Python Validated)" >> $GITHUB_STEP_SUMMARY
        
        # Use Python for file counting (FIXED #8)
        python << 'EOF'
        from pathlib import Path
        
        md_count = len(list(Path("data/md").glob("*.md"))) if Path("data/md").exists() else 0
        processed_files = []
        for file_name in ["portfolio_summary.csv", "detailed_data.csv", "statistics.json"]:
            if Path(f"data/processed/{file_name}").exists():
                processed_files.append(file_name)
        
        print(f"- üìù MD files: {md_count}")
        print(f"- üìä Processed files: {len(processed_files)} ({', '.join(processed_files)})")
        
        if md_count >= 50:
            print("- üèÜ **Quality**: Premium (50+ companies)")
        elif md_count >= 20:
            print("- ‚úÖ **Quality**: Good (20+ companies)")
        elif md_count >= 10:
            print("- üìä **Quality**: Acceptable (10+ companies)")
        else:
            print("- ‚ö†Ô∏è **Quality**: Limited data")
        EOF >> $GITHUB_STEP_SUMMARY
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### üöÄ Next Steps" >> $GITHUB_STEP_SUMMARY
        echo "- View results in Google Sheets dashboard" >> $GITHUB_STEP_SUMMARY
        echo "- Monitor rate limiting status for next run" >> $GITHUB_STEP_SUMMARY
        echo "- Consider adjusting priority_focus if needed" >> $GITHUB_STEP_SUMMARY
        echo "- Review logs for any warnings or optimization opportunities" >> $GITHUB_STEP_SUMMARY

  # Enhanced recovery job (v3.3.1)
  recovery_v331:
    runs-on: ubuntu-latest
    needs: [preflight_v331, pipeline_v331]
    if: failure() || needs.preflight_v331.outputs.validation_status == 'error'
    
    steps:
    - name: üì• Checkout Repository
      uses: actions/checkout@v4
    
    - name: üêç Set up Python Environment
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: üì¶ Install Recovery Dependencies
      run: |
        pip install --upgrade pip
        pip install requests pandas python-dotenv beautifulsoup4 markdownify || echo "Some dependencies failed"
    
    - name: üîÑ Enhanced Recovery Strategy (v3.3.1)
      env:
        GOOGLE_SHEETS_CREDENTIALS: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}
        GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
      run: |
        python << 'EOF'
        print("üîÑ Enhanced recovery strategy (v3.3.1)...")
        
        from pathlib import Path
        from datetime import datetime
        
        recovery_success = False
        
        try:
            # Attempt data processing recovery
            md_dir = Path("data/md")
            if md_dir.exists() and list(md_dir.glob("*.md")):
                print("üìÑ Found existing MD files, attempting v3.3.1 processing...")
                
                try:
                    import data_processor
                    if hasattr(data_processor, 'process_all_data_v331'):
                        success = data_processor.process_all_data_v331(force=True)
                        if success:
                            print("‚úÖ Recovery processing successful")
                            recovery_success = True
                    else:
                        print("‚ö†Ô∏è v3.3.1 processor not available")
                except Exception as e:
                    print(f"‚ö†Ô∏è Recovery processing failed: {e}")
            
            # Attempt sheets upload if data exists
            processed_dir = Path("data/processed")
            if processed_dir.exists() and (processed_dir / "portfolio_summary.csv").exists():
                print("üìà Attempting recovery sheets upload...")
                try:
                    import sheets_uploader
                    config = {
                        "input": {
                            "summary_csv": "data/processed/portfolio_summary.csv",
                            "detailed_csv": "data/processed/detailed_data.csv",
                            "stats_json": "data/processed/statistics.json"
                        }
                    }
                    success = sheets_uploader.upload_all_sheets_v330(config)
                    if success:
                        print("‚úÖ Recovery sheets upload successful")
                        recovery_success = True
                except Exception as e:
                    print(f"‚ö†Ô∏è Recovery sheets upload failed: {e}")
            
            if recovery_success:
                print("üéâ v3.3.1 recovery completed successfully")
            else:
                print("‚ö†Ô∏è Recovery had limited success")
                
        except Exception as e:
            print(f"‚ùå Recovery strategy failed: {e}")
        EOF
    
    - name: üìã Recovery Report (v3.3.1)
      if: always()
      run: |
        echo "## üîÑ v3.3.1 Enhanced Recovery Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "The main v3.3.1 pipeline encountered issues and enhanced recovery was attempted." >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ needs.preflight_v331.outputs.validation_status }}" = "error" ]; then
          echo "üö® **Issue**: Pre-flight validation failed" >> $GITHUB_STEP_SUMMARY
          echo "üîß **Action**: Attempted basic data processing with v3.3.1 enhancements" >> $GITHUB_STEP_SUMMARY
        else
          echo "‚ö†Ô∏è **Issue**: Main pipeline execution failed" >> $GITHUB_STEP_SUMMARY
          echo "üîß **Action**: Attempted data recovery with v3.3.1 processing" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### üîß v3.3.1 Recovery Features Used" >> $GITHUB_STEP_SUMMARY
        echo "- Enhanced error isolation (FIXED #1)" >> $GITHUB_STEP_SUMMARY
        echo "- Performance-optimized processing (FIXED #2)" >> $GITHUB_STEP_SUMMARY
        echo "- Memory-managed operations (FIXED #9)" >> $GITHUB_STEP_SUMMARY
        echo "- Python-based validation (FIXED #8)" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### üéØ Troubleshooting Steps" >> $GITHUB_STEP_SUMMARY
        echo "1. Check execution logs above for specific error details" >> $GITHUB_STEP_SUMMARY
        echo "2. Verify API credentials are properly configured" >> $GITHUB_STEP_SUMMARY
        echo "3. Consider running with 'process_only' mode next time" >> $GITHUB_STEP_SUMMARY
        echo "4. Review memory usage if processing large datasets" >> $GITHUB_STEP_SUMMARY
        echo "5. Use 'conservative' mode if rate limiting is suspected" >> $GITHUB_STEP_SUMMARY