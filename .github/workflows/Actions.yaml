name: FactSet Pipeline - Current Status (Rate Limiting Aware)

on:
  push:
    branches:
      - main
  workflow_dispatch:
    inputs:
      execution_mode:
        description: 'Execution strategy'
        required: false
        default: 'conservative'
        type: choice
        options:
          - 'conservative'    # High priority only, with delays
          - 'process_only'    # Process existing data only
          - 'test_only'       # Test components without search
          - 'force_search'    # Force search (may hit rate limits)
      priority_focus:
        description: 'Search priority level'
        required: false
        default: 'high_only'
        type: choice
        options:
          - 'high_only'       # Top priority companies only
          - 'top_30'          # Top 30 companies
          - 'balanced'        # All companies (likely to hit rate limits)
      wait_for_rate_limits:
        description: 'Wait time before search (minutes)'
        required: false
        default: '30'
        type: string
  schedule:
    # Run daily at 2:10 AM UTC (when rate limits are most likely reset)
    - cron: "10 2 * * *"

env:
  PYTHONIOENCODING: utf-8
  FACTSET_PIPELINE_DEBUG: true

jobs:
  # Pre-flight checks to assess current status
  preflight:
    runs-on: ubuntu-latest
    outputs:
      rate_limited: ${{ steps.check_status.outputs.rate_limited }}
      has_existing_data: ${{ steps.check_status.outputs.has_existing_data }}
      recommended_action: ${{ steps.check_status.outputs.recommended_action }}
    
    steps:
    - name: ðŸ“¥ Checkout Repository
      uses: actions/checkout@v4
    
    - name: ðŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: ðŸ“¦ Install Dependencies
      run: |
        pip install --upgrade pip
        pip install requests pandas python-dotenv
    
    - name: ðŸ” Check Current Status
      id: check_status
      env:
        GOOGLE_SEARCH_API_KEY: ${{ secrets.GOOGLE_SEARCH_API_KEY }}
        GOOGLE_SEARCH_CSE_ID: ${{ secrets.GOOGLE_SEARCH_CSE_ID }}
      run: |
        echo "ðŸ” Checking pipeline status..."
        
        # Check for existing data
        existing_md_files=$(find data/md -name "*.md" 2>/dev/null | wc -l || echo "0")
        existing_csv_files=$(find data/csv -name "*.csv" 2>/dev/null | wc -l || echo "0")
        
        echo "ðŸ“Š Found $existing_md_files MD files, $existing_csv_files CSV files"
        
        if [ "$existing_md_files" -gt "5" ]; then
          echo "has_existing_data=true" >> $GITHUB_OUTPUT
        else
          echo "has_existing_data=false" >> $GITHUB_OUTPUT
        fi
        
        # Test rate limiting status with a simple search
        echo "ðŸ§ª Testing search access (rate limiting check)..."
        rate_limited="false"
        
        # Create a simple test script
        cat > test_search.py << 'EOF'
        import requests
        import os
        import sys
        
        api_key = os.environ.get('GOOGLE_SEARCH_API_KEY')
        cse_id = os.environ.get('GOOGLE_SEARCH_CSE_ID')
        
        if not api_key or not cse_id:
            print("API credentials not available")
            sys.exit(1)
        
        url = "https://www.googleapis.com/customsearch/v1"
        params = {
            'key': api_key,
            'cx': cse_id,
            'q': 'test',
            'num': 1
        }
        
        try:
            response = requests.get(url, params=params, timeout=30)
            if response.status_code == 429:
                print("RATE_LIMITED")
                sys.exit(2)
            elif response.status_code == 200:
                print("SEARCH_AVAILABLE")
                sys.exit(0)
            else:
                print(f"ERROR_{response.status_code}")
                sys.exit(3)
        except Exception as e:
            print(f"NETWORK_ERROR: {e}")
            sys.exit(4)
        EOF
        
        python test_search.py
        search_status=$?
        
        case $search_status in
          0)
            echo "âœ… Search API accessible"
            echo "rate_limited=false" >> $GITHUB_OUTPUT
            echo "recommended_action=search" >> $GITHUB_OUTPUT
            ;;
          2)
            echo "ðŸš¨ Rate limiting detected"
            echo "rate_limited=true" >> $GITHUB_OUTPUT
            echo "recommended_action=process_existing" >> $GITHUB_OUTPUT
            ;;
          *)
            echo "âš ï¸ Search API issues detected"
            echo "rate_limited=unknown" >> $GITHUB_OUTPUT
            echo "recommended_action=process_existing" >> $GITHUB_OUTPUT
            ;;
        esac

  # Main pipeline execution with intelligent strategy
  pipeline:
    runs-on: ubuntu-latest
    needs: preflight
    
    steps:
    - name: ðŸ“¥ Checkout Repository
      uses: actions/checkout@v4
    
    - name: ðŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: ðŸ”§ Install System Dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y wkhtmltopdf || echo "wkhtmltopdf installation failed (optional)"
        echo "âœ… System dependencies installed"
    
    - name: ðŸ“¦ Install Python Dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt || pip install requests pandas gspread google-auth python-dotenv markitdown
        echo "âœ… Python dependencies installed"
    
    - name: ðŸ”§ Setup Configuration
      run: |
        # Create directories
        mkdir -p data/{csv,md,pdf,processed} logs
        
        # Download target companies
        python config.py --download-csv || echo "âš ï¸ CSV download failed, will use existing"
        
        echo "âœ… Configuration setup completed"
    
    - name: ðŸ§ª Test Components
      env:
        GOOGLE_SEARCH_API_KEY: ${{ secrets.GOOGLE_SEARCH_API_KEY }}
        GOOGLE_SEARCH_CSE_ID: ${{ secrets.GOOGLE_SEARCH_CSE_ID }}
        GOOGLE_SHEETS_CREDENTIALS: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}
        GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
      run: |
        echo "ðŸ§ª Testing individual components..."
        
        # Test configuration
        python config.py --show || echo "âš ï¸ Config test failed"
        
        # Test utilities
        python utils.py || echo "âš ï¸ Utils test failed"
        
        # Test sheets connection (without search)
        python sheets_uploader.py --test-connection || echo "âš ï¸ Sheets test failed"
        
        echo "âœ… Component tests completed"
    
    - name: â° Wait for Rate Limiting (if specified)
      if: github.event.inputs.wait_for_rate_limits && github.event.inputs.wait_for_rate_limits != '0'
      run: |
        wait_minutes="${{ github.event.inputs.wait_for_rate_limits }}"
        echo "â° Waiting ${wait_minutes} minutes for rate limits to clear..."
        sleep $((wait_minutes * 60))
    
    - name: ðŸ“Š Process Existing Data
      if: needs.preflight.outputs.has_existing_data == 'true' || needs.preflight.outputs.rate_limited == 'true'
      env:
        GOOGLE_SEARCH_API_KEY: ${{ secrets.GOOGLE_SEARCH_API_KEY }}
        GOOGLE_SEARCH_CSE_ID: ${{ secrets.GOOGLE_SEARCH_CSE_ID }}
        GOOGLE_SHEETS_CREDENTIALS: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}
        GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
      run: |
        echo "ðŸ“Š Processing existing data..."
        
        # Check existing data
        python data_processor.py --check-data || echo "Data check completed with warnings"
        
        # Process MD files if they exist
        if [ -d "data/md" ] && [ "$(ls -A data/md 2>/dev/null)" ]; then
          echo "ðŸ“„ Processing existing MD files..."
          python data_processor.py --force --parse-md || echo "âš ï¸ MD processing had issues"
        fi
        
        # Process any existing data
        python data_processor.py --force || echo "âš ï¸ Data processing had issues"
        
        echo "âœ… Existing data processing completed"
    
    - name: ðŸ” Conservative Search (if safe)
      if: needs.preflight.outputs.rate_limited != 'true' && github.event.inputs.execution_mode != 'process_only'
      env:
        GOOGLE_SEARCH_API_KEY: ${{ secrets.GOOGLE_SEARCH_API_KEY }}
        GOOGLE_SEARCH_CSE_ID: ${{ secrets.GOOGLE_SEARCH_CSE_ID }}
        GOOGLE_SHEETS_CREDENTIALS: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}
        GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
      run: |
        echo "ðŸ” Attempting conservative search..."
        
        # Determine search parameters
        priority_focus="${{ github.event.inputs.priority_focus || 'high_only' }}"
        execution_mode="${{ github.event.inputs.execution_mode || 'conservative' }}"
        
        if [ "$execution_mode" = "force_search" ]; then
          echo "âš ï¸ Force search mode - may encounter rate limiting"
          python factset_search.py --priority-focus "$priority_focus" || echo "ðŸš¨ Search failed due to rate limiting"
        elif [ "$execution_mode" = "conservative" ]; then
          echo "ðŸŽ¯ Conservative search mode"
          # Try search with conservative settings
          timeout 600 python factset_search.py --priority-focus high_only || echo "ðŸš¨ Conservative search failed"
        else
          echo "â„¹ï¸ Skipping search in test-only mode"
        fi
    
    - name: ðŸ“Š Final Data Processing
      env:
        GOOGLE_SEARCH_API_KEY: ${{ secrets.GOOGLE_SEARCH_API_KEY }}
        GOOGLE_SEARCH_CSE_ID: ${{ secrets.GOOGLE_SEARCH_CSE_ID }}
        GOOGLE_SHEETS_CREDENTIALS: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}
        GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
      run: |
        echo "ðŸ“Š Final data processing..."
        
        # Process any new data
        python data_processor.py --force --parse-md || echo "Data processing completed with warnings"
        
        echo "âœ… Final processing completed"
    
    - name: ðŸ’¾ Validate and Commit Results (Safe Version)
      run: |
        echo "ðŸ’¾ Validating data before commit..."
        
        # Configure git for GitHub Actions bot
        git config --global user.name "github-actions[bot]"
        git config --global user.email "41898282+github-actions[bot]@users.noreply.github.com"
        
        # Validation checks
        echo "ðŸ” Running comprehensive data validation..."
        
        # Check 1: Count files with minimum size requirements
        md_count=$(find data/md -name '*.md' -size +100c 2>/dev/null | wc -l)
        csv_count=$(find data/csv -name '*.csv' -size +50c 2>/dev/null | wc -l)
        processed_count=$(find data/processed -name '*' -type f -size +10c 2>/dev/null | wc -l)
        
        echo "ðŸ“Š File counts: ${md_count} MD files (>100 bytes), ${csv_count} CSV files (>50 bytes), ${processed_count} processed files"
        
        # Check 2: Validate MD files contain actual financial content (not error pages)
        valid_md_count=0
        factset_md_count=0
        if [ -d "data/md" ]; then
          for file in data/md/*.md; do
            [ -f "$file" ] || continue
            # Check if file contains actual financial content
            if grep -q -E "(factset|EPS|é ä¼°|ç›®æ¨™åƒ¹|åˆ†æžå¸«|è²¡å ±|ç‡Ÿæ”¶)" "$file" 2>/dev/null; then
              valid_md_count=$((valid_md_count + 1))
              # Extra check for FactSet specific content
              if grep -q -i "factset" "$file" 2>/dev/null; then
                factset_md_count=$((factset_md_count + 1))
              fi
            fi
          done
        fi
        
        # Check 3: Validate CSV files have proper structure
        valid_csv_count=0
        if [ -d "data/csv" ]; then
          for file in data/csv/*.csv; do
            [ -f "$file" ] || continue
            # Check if CSV has proper headers and content
            if head -1 "$file" | grep -q -E "(Title|Link|å…¬å¸|è‚¡ç¥¨|åç¨±)" 2>/dev/null; then
              # Also check if it has data rows (more than just header)
              if [ "$(wc -l < "$file")" -gt "1" ]; then
                valid_csv_count=$((valid_csv_count + 1))
              fi
            fi
          done
        fi
        
        # Check 4: Validate processed files
        valid_processed_count=0
        has_summary=false
        has_consolidated=false
        if [ -f "data/processed/portfolio_summary.csv" ] && [ -s "data/processed/portfolio_summary.csv" ]; then
          has_summary=true
          valid_processed_count=$((valid_processed_count + 1))
        fi
        if [ -f "data/processed/consolidated_factset.csv" ] && [ -s "data/processed/consolidated_factset.csv" ]; then
          has_consolidated=true
          valid_processed_count=$((valid_processed_count + 1))
        fi
        
        echo "âœ… Validation results:"
        echo "   ðŸ“„ Valid MD files: ${valid_md_count} (${factset_md_count} with FactSet content)"
        echo "   ðŸ“Š Valid CSV files: ${valid_csv_count}"
        echo "   ðŸŽ¯ Valid processed files: ${valid_processed_count}"
        echo "   ðŸ’¼ Has portfolio summary: ${has_summary}"
        echo "   ðŸ“ˆ Has consolidated data: ${has_consolidated}"
        
        # Decision logic: Smart commit strategy
        should_commit="false"
        commit_reason=""
        commit_priority="low"
        
        # High priority commit conditions
        if [ "$valid_md_count" -gt "20" ] && [ "$factset_md_count" -gt "5" ] && [ "$valid_csv_count" -gt "2" ]; then
          should_commit="true"
          commit_priority="high"
          commit_reason="Excellent data: ${valid_md_count} MD (${factset_md_count} FactSet) + ${valid_csv_count} CSV files"
        
        # Medium priority commit conditions  
        elif [ "$valid_md_count" -gt "10" ] && [ "$valid_csv_count" -gt "1" ]; then
          should_commit="true"
          commit_priority="medium"
          commit_reason="Good data: ${valid_md_count} MD + ${valid_csv_count} CSV files"
        
        # Low priority commit conditions (but still valuable)
        elif [ "$valid_md_count" -gt "5" ] || [ "$has_summary" = "true" ] || [ "$has_consolidated" = "true" ]; then
          should_commit="true"
          commit_priority="low"
          commit_reason="Minimal viable data: ${valid_md_count} MD files, summary=${has_summary}, consolidated=${has_consolidated}"
        
        # First-time data collection (more lenient)
        elif [ "${{ needs.preflight.outputs.has_existing_data }}" = "false" ] && [ "$valid_md_count" -gt "0" ]; then
          should_commit="true"
          commit_priority="medium"
          commit_reason="First data collection: ${valid_md_count} MD files"
        
        # Skip commit if data quality is poor
        else
          should_commit="false"
          commit_reason="Insufficient valid data (${valid_md_count} MD, ${valid_csv_count} CSV, processed=${valid_processed_count})"
        fi
        
        echo "ðŸŽ¯ Decision: should_commit=${should_commit}, priority=${commit_priority}"
        echo "ðŸ“ Reason: ${commit_reason}"
        
        if [ "$should_commit" = "true" ]; then
          echo "âœ… Proceeding with validated commit..."
          
          # Create .gitattributes to handle large files properly
          cat > .gitattributes << 'EOF'
        # Handle large files with Git LFS
        *.pdf filter=lfs diff=lfs merge=lfs -text
        *.zip filter=lfs diff=lfs merge=lfs -text
        *.gz filter=lfs diff=lfs merge=lfs -text
        
        # Ensure proper handling of text files
        *.md text eol=lf
        *.csv text eol=lf
        *.json text eol=lf
        *.txt text eol=lf
        EOF
          
          # Clean up any problematic files before commit
          echo "ðŸ§¹ Cleaning up before commit..."
          
          # Remove files larger than 50MB (GitHub limit is 100MB)
          find data/ -size +50M -delete 2>/dev/null || true
          
          # Remove empty files
          find data/ -empty -delete 2>/dev/null || true
          
          # Remove obvious error files
          find data/ -name "*.md" -exec grep -l "404 Not Found\|403 Forbidden\|500 Internal Server Error" {} \; -delete 2>/dev/null || true
          
          # Add files selectively based on validation
          echo "ðŸ“ Adding validated files to git..."
          
          # Add valid MD files
          if [ "$valid_md_count" -gt "0" ]; then
            find data/md -name '*.md' -size +100c -exec sh -c '
              if grep -q -E "(factset|EPS|é ä¼°|ç›®æ¨™åƒ¹|åˆ†æžå¸«|è²¡å ±|ç‡Ÿæ”¶)" "$1" 2>/dev/null; then
                git add "$1"
              fi
            ' _ {} \;
            echo "âœ… Added ${valid_md_count} validated MD files"
          fi
          
          # Add valid CSV files
          if [ "$valid_csv_count" -gt "0" ]; then
            find data/csv -name '*.csv' -size +50c -exec sh -c '
              if head -1 "$1" | grep -q -E "(Title|Link|å…¬å¸|è‚¡ç¥¨|åç¨±)" 2>/dev/null && [ "$(wc -l < "$1")" -gt "1" ]; then
                git add "$1"
              fi
            ' _ {} \;
            echo "âœ… Added ${valid_csv_count} validated CSV files"
          fi
          
          # Add processed data if valid
          if [ "$valid_processed_count" -gt "0" ]; then
            if [ "$has_summary" = "true" ]; then
              git add data/processed/portfolio_summary.csv
            fi
            if [ "$has_consolidated" = "true" ]; then
              git add data/processed/consolidated_factset.csv
            fi
            # Add other processed files if they exist and are valid
            find data/processed -name '*.json' -size +10c -exec git add {} \; 2>/dev/null || true
            echo "âœ… Added ${valid_processed_count} processed data files"
          fi
          
          # Add supporting files
          git add logs/ 2>/dev/null || true
          git add "è§€å¯Ÿåå–®.csv" 2>/dev/null || true
          git add .gitattributes 2>/dev/null || true
          
          # Create detailed commit message with quality indicators
          execution_mode="${{ github.event.inputs.execution_mode || 'conservative' }}"
          priority_focus="${{ github.event.inputs.priority_focus || 'high_only' }}"
          
          # Add quality emoji based on priority
          quality_emoji="ðŸ“Š"
          case "$commit_priority" in
            "high") quality_emoji="ðŸ†" ;;
            "medium") quality_emoji="âœ…" ;;
            "low") quality_emoji="ðŸ“Š" ;;
          esac
          
          commit_message="--"
          
          # Commit and push with comprehensive error handling
          if git diff --staged --quiet; then
            echo "â„¹ï¸ No validated changes to commit (files may have been filtered out)"
          else
            echo "âœ… Committing validated data..."
            if git commit -m "$commit_message"; then
              echo "ðŸ“¤ Pushing to repository..."
              if git push; then
                echo "ðŸŽ‰ Validated results committed and pushed successfully!"
                echo "ðŸ“ˆ Quality level: ${commit_priority}"
                echo "ðŸ“Š Data summary: ${valid_md_count} MD + ${valid_csv_count} CSV + ${valid_processed_count} processed files"
              else
                echo "âš ï¸ Commit succeeded but push failed - changes are saved locally"
              fi
            else
              echo "âš ï¸ Commit failed - no changes were made to repository"
            fi
          fi
          
        else
          echo "âš ï¸ Skipping commit due to data quality issues"
          echo "ðŸ“ Reason: ${commit_reason}"
          echo "ðŸ’¡ Previous good data preserved in repository"
          
          # Create an execution log even when skipping commit
          mkdir -p logs
          echo "$(date '+%Y-%m-%d %H:%M:%S'): SKIPPED COMMIT - ${commit_reason}" >> logs/pipeline_execution.log
          echo "  - MD files found: ${md_count} total, ${valid_md_count} valid (${factset_md_count} with FactSet)" >> logs/pipeline_execution.log
          echo "  - CSV files found: ${csv_count} total, ${valid_csv_count} valid" >> logs/pipeline_execution.log
          echo "  - Processed files: ${processed_count} total, ${valid_processed_count} valid" >> logs/pipeline_execution.log
          echo "  - Execution mode: ${{ github.event.inputs.execution_mode || 'conservative' }}" >> logs/pipeline_execution.log
          echo "  - Priority focus: ${{ github.event.inputs.priority_focus || 'high_only' }}" >> logs/pipeline_execution.log
          echo "  - Rate limited: ${{ needs.preflight.outputs.rate_limited }}" >> logs/pipeline_execution.log
          
          # Commit just the execution log
          git add logs/pipeline_execution.log 2>/dev/null || true
          git commit -m "--" 2>/dev/null || true
          git push 2>/dev/null || true
          
          echo "ðŸ“‹ Execution logged for debugging purposes"
        fi
    
    - name: ðŸ“ˆ Upload to Google Sheets
      if: env.GOOGLE_SHEET_ID != ''
      env:
        GOOGLE_SEARCH_API_KEY: ${{ secrets.GOOGLE_SEARCH_API_KEY }}
        GOOGLE_SEARCH_CSE_ID: ${{ secrets.GOOGLE_SEARCH_CSE_ID }}
        GOOGLE_SHEETS_CREDENTIALS: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}
        GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
      run: |
        echo "ðŸ“ˆ Uploading to Google Sheets..."
        
        # Check if we have data to upload
        if [ -f "data/processed/portfolio_summary.csv" ] || [ -f "data/processed/consolidated_factset.csv" ]; then
          python sheets_uploader.py || echo "âš ï¸ Sheets upload had issues"
        else
          echo "â„¹ï¸ No processed data found for upload"
        fi
        
        echo "âœ… Upload attempt completed"
    
    - name: ðŸ“Š Generate Status Report
      if: always()
      run: |
        echo "ðŸ“Š Generating status report..."
        
        # Try to get pipeline status (may fail if methods not implemented)
        python factset_pipeline.py --status || echo "Status command not available"
        
        # Manual status check
        echo "=== Manual Status Check ==="
        echo "MD files: $(find data/md -name '*.md' 2>/dev/null | wc -l)"
        echo "CSV files: $(find data/csv -name '*.csv' 2>/dev/null | wc -l)"
        echo "Processed files: $(find data/processed -name '*' -type f 2>/dev/null | wc -l)"
        
        # Check for rate limiting indicators in logs
        if grep -r "429" logs/ 2>/dev/null; then
          echo "ðŸš¨ Rate limiting detected in logs"
        fi
    
    - name: ðŸ“„ Collect Artifacts
      if: always()
      run: |
        echo "ðŸ“„ Collecting artifacts..."
        
        # Create artifacts directory
        mkdir -p pipeline_artifacts
        
        # Copy data with error handling
        [ -d "data" ] && cp -r data pipeline_artifacts/ 2>/dev/null || echo "No data directory"
        [ -d "logs" ] && cp -r logs pipeline_artifacts/ 2>/dev/null || echo "No logs directory"
        
        # Copy specific files
        [ -f "è§€å¯Ÿåå–®.csv" ] && cp "è§€å¯Ÿåå–®.csv" pipeline_artifacts/ || echo "No watchlist file"
        
        # Create summary
        cat > pipeline_artifacts/EXECUTION_SUMMARY.md << EOF
        # Pipeline Execution Summary
        
        **Date**: $(date)
        **Execution Mode**: ${{ github.event.inputs.execution_mode || 'conservative' }}
        **Priority Focus**: ${{ github.event.inputs.priority_focus || 'high_only' }}
        **Trigger**: ${{ github.event_name }}
        **Rate Limited**: ${{ needs.preflight.outputs.rate_limited }}
        **Had Existing Data**: ${{ needs.preflight.outputs.has_existing_data }}
        
        ## File Counts
        - MD files: $(find data/md -name '*.md' 2>/dev/null | wc -l)
        - CSV files: $(find data/csv -name '*.csv' 2>/dev/null | wc -l)
        - Processed files: $(find data/processed -name '*' -type f 2>/dev/null | wc -l)
        
        ## Execution Status
        $(python factset_pipeline.py --status 2>/dev/null || echo "Pipeline status not available")
        
        ## Rate Limiting Check
        $(grep -r "429\|Too Many Requests" logs/ 2>/dev/null || echo "No rate limiting detected in logs")
        EOF
        
        echo "âœ… Artifacts collected"
    
    - name: ðŸ“¦ Upload Artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: factset-pipeline-${{ github.run_number }}-${{ github.event.inputs.execution_mode || 'conservative' }}
        path: pipeline_artifacts/
        retention-days: 14
    
    - name: ðŸ“Š Summary Report
      if: always()
      run: |
        echo "## ðŸ“Š FactSet Pipeline Execution Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Execution Mode**: ${{ github.event.inputs.execution_mode || 'conservative' }}" >> $GITHUB_STEP_SUMMARY
        echo "**Priority Focus**: ${{ github.event.inputs.priority_focus || 'high_only' }}" >> $GITHUB_STEP_SUMMARY
        echo "**Rate Limited**: ${{ needs.preflight.outputs.rate_limited }}" >> $GITHUB_STEP_SUMMARY
        echo "**Recommended Action**: ${{ needs.preflight.outputs.recommended_action }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Add dashboard link if available
        if [ -n "${{ secrets.GOOGLE_SHEET_ID }}" ]; then
          echo "ðŸ“ˆ **Dashboard**: [View Results](https://docs.google.com/spreadsheets/d/${{ secrets.GOOGLE_SHEET_ID }}/edit)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi
        
        # File statistics
        echo "### ðŸ“ Generated Files" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸ“ MD files: $(find data/md -name '*.md' 2>/dev/null | wc -l)" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸ“„ CSV files: $(find data/csv -name '*.csv' 2>/dev/null | wc -l)" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸ“Š Processed files: $(find data/processed -name '*' -type f 2>/dev/null | wc -l)" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Rate limiting status
        if grep -r "429\|Too Many Requests" logs/ 2>/dev/null >/dev/null; then
          echo "ðŸš¨ **Rate Limiting Detected**: Search requests were blocked" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ’¡ **Recommendation**: Wait 4-12 hours before next search attempt" >> $GITHUB_STEP_SUMMARY
        else
          echo "âœ… **Search Status**: No rate limiting detected" >> $GITHUB_STEP_SUMMARY
        fi

  # Recovery job for when main pipeline encounters issues
  recovery:
    runs-on: ubuntu-latest
    needs: [preflight, pipeline]
    if: failure() || needs.preflight.outputs.rate_limited == 'true'
    
    steps:
    - name: ðŸ“¥ Checkout Repository
      uses: actions/checkout@v4
    
    - name: ðŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: ðŸ“¦ Install Dependencies
      run: |
        pip install --upgrade pip
        pip install requests pandas gspread google-auth python-dotenv markitdown || echo "Some dependencies failed"
    
    - name: ðŸ”„ Recovery Strategy
      env:
        GOOGLE_SHEETS_CREDENTIALS: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}
        GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
      run: |
        echo "ðŸ”„ Implementing recovery strategy..."
        
        # Try to salvage any existing data
        if [ -d "data/md" ] && [ "$(ls -A data/md 2>/dev/null)" ]; then
          echo "ðŸ“„ Found existing MD files, attempting processing..."
          python data_processor.py --check-data || echo "Data check completed"
          python data_processor.py --force --parse-md || echo "MD processing attempted"
        fi
        
        # Try basic data processing
        python data_processor.py --force || echo "Basic processing attempted"
        
        # Try sheets upload if data exists
        if [ -f "data/processed/portfolio_summary.csv" ]; then
          echo "ðŸ“ˆ Attempting sheets upload with existing data..."
          python sheets_uploader.py || echo "Sheets upload attempted"
        fi
        
        echo "âœ… Recovery attempts completed"
    
    - name: ðŸ“‹ Recovery Report
      if: always()
      run: |
        echo "## ðŸ”„ Recovery Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "The main pipeline encountered issues and recovery was attempted." >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ needs.preflight.outputs.rate_limited }}" = "true" ]; then
          echo "ðŸš¨ **Issue**: Google Search rate limiting detected" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ’¡ **Action**: Processed existing data only" >> $GITHUB_STEP_SUMMARY
          echo "â° **Recommendation**: Wait 4-12 hours before attempting new searches" >> $GITHUB_STEP_SUMMARY
        else
          echo "âš ï¸ **Issue**: Pipeline execution failed" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ”§ **Action**: Attempted data recovery and processing" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Recovery Results" >> $GITHUB_STEP_SUMMARY
        echo "- MD files processed: $(find data/md -name '*.md' 2>/dev/null | wc -l)" >> $GITHUB_STEP_SUMMARY
        echo "- Data files generated: $(find data/processed -name '*' -type f 2>/dev/null | wc -l)" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Next Steps" >> $GITHUB_STEP_SUMMARY
        echo "1. Review execution logs in artifacts" >> $GITHUB_STEP_SUMMARY
        echo "2. Check for rate limiting indicators" >> $GITHUB_STEP_SUMMARY
        echo "3. Consider manual execution with conservative settings" >> $GITHUB_STEP_SUMMARY
        echo "4. Verify API credentials if search consistently fails" >> $GITHUB_STEP_SUMMARY