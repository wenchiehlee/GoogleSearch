name: FactSet Pipeline v3.3.3 - Final Integrated Edition (Separated Jobs)

on:
  workflow_dispatch:
    inputs:
      mode:
        description: 'Execution mode'
        required: true
        default: 'intelligent'
        type: choice
        options:
          - intelligent
          - enhanced
          - conservative
          - process_only
      priority:
        description: 'Company priority focus'
        required: true
        default: 'high_only'
        type: choice
        options:
          - high_only
          - top_30
          - balanced
      memory_limit:
        description: 'Memory limit (MB)'
        required: false
        default: '2048'
        type: string
      enable_quality_scoring:
        description: 'Enable v3.3.3 quality scoring (0-10 scale)'
        required: false
        default: true
        type: boolean
      enable_quality_indicators:
        description: 'Enable quality indicators (ðŸŸ¢ðŸŸ¡ðŸŸ ðŸ”´)'
        required: false
        default: true
        type: boolean
      v333_format:
        description: 'Use v3.3.3 format with GitHub Raw URLs'
        required: false
        default: true
        type: boolean
      skip_search:
        description: 'Skip search phase (process existing data only)'
        required: false
        default: false
        type: boolean
  schedule:
    - cron: "10 2 * * *"  # Daily at 2:10 AM UTC
  push:
    branches: [ main ]
    paths: 
      - '*.py'
      - 'requirements.txt'
      - '.github/workflows/Actions.yaml'

# GLOBAL ENVIRONMENT VARIABLES - Available to all jobs
env:
  PYTHON_VERSION: '3.11'
  CACHE_VERSION: 'v1'
  # Google API Configuration - Available globally
  GOOGLE_SEARCH_API_KEY: ${{ secrets.GOOGLE_SEARCH_API_KEY }}
  GOOGLE_SEARCH_CSE_ID: ${{ secrets.GOOGLE_SEARCH_CSE_ID }}
  GOOGLE_SHEETS_CREDENTIALS: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}
  GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
  PYTHONIOENCODING: utf-8

jobs:
  # ============================================================================
  # v3.3.3 MODERNIZED VALIDATION JOB - FIXED VERSION
  # ============================================================================
  validate:
    name: ðŸ§ª v3.3.3 System Validation
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      status: ${{ steps.validate.outputs.status }}
      recommendation: ${{ steps.validate.outputs.recommendation }}
      quality_check: ${{ steps.validate.outputs.quality_check }}
      python_ready: ${{ steps.validate.outputs.python_ready }}
      dependencies_ready: ${{ steps.validate.outputs.dependencies_ready }}
    
    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
      
      - name: ðŸ Setup Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      
      - name: ðŸ“¦ Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip list
      
      - name: ðŸ”§ Setup Directories
        run: |
          mkdir -p data/{csv,md,pdf,processed}
          mkdir -p logs/{latest,reports,archives}
          mkdir -p backups temp configs
          echo "ðŸ“ Directory structure created for validation"
          
          # Verify creation
          echo "âœ… Directories created:"
          ls -la data/ logs/ 2>/dev/null || true
          echo "ðŸ“ All required directories are now available"
      
      - name: ðŸ§ª Run v3.3.3 Validation
        id: validate
        run: |
          echo "ðŸš€ Starting v3.3.3 comprehensive validation..."
          
          # Run v3.3.3 validation with all new features
          if python factset_cli.py validate \
            --comprehensive \
            --test-v333 \
            --quality-scoring \
            --github-actions; then
            
            # v3.3.3 FIX: Use GITHUB_OUTPUT instead of deprecated set-output
            echo "status=success" >> $GITHUB_OUTPUT
            echo "recommendation=proceed" >> $GITHUB_OUTPUT
            echo "quality_check=enabled" >> $GITHUB_OUTPUT
            echo "python_ready=true" >> $GITHUB_OUTPUT
            echo "dependencies_ready=true" >> $GITHUB_OUTPUT
            
            echo "âœ… v3.3.3 validation completed successfully"
          else
            echo "status=failed" >> $GITHUB_OUTPUT
            echo "recommendation=fix_required" >> $GITHUB_OUTPUT
            echo "quality_check=disabled" >> $GITHUB_OUTPUT
            echo "python_ready=false" >> $GITHUB_OUTPUT
            echo "dependencies_ready=false" >> $GITHUB_OUTPUT
            
            echo "âŒ v3.3.3 validation failed"
            exit 1
          fi
      
      - name: ðŸ“‹ Validation Summary
        run: |
          echo "## ðŸ§ª v3.3.3 Validation Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Status**: ${{ steps.validate.outputs.status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Quality Scoring**: ${{ steps.validate.outputs.quality_check }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Python Ready**: ${{ steps.validate.outputs.python_ready }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Dependencies Ready**: ${{ steps.validate.outputs.dependencies_ready }}" >> $GITHUB_STEP_SUMMARY
          echo "- **v3.3.3 Features**: All validated âœ…" >> $GITHUB_STEP_SUMMARY

  # ============================================================================
  # v3.3.3 SEARCH PIPELINE JOB
  # ============================================================================
  search_pipeline:
    name: ðŸ” Search v3.3.3 Pipeline
    needs: validate
    runs-on: ubuntu-latest
    timeout-minutes: 45
    if: needs.validate.outputs.status == 'success' && inputs.skip_search != true
    
    outputs:
      search_status: ${{ steps.search.outputs.status }}
      files_found: ${{ steps.search.outputs.files_found }}
      companies_searched: ${{ steps.search.outputs.companies_searched }}
      search_errors: ${{ steps.search.outputs.search_errors }}
      execution_time: ${{ steps.search.outputs.execution_time }}
    
    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
      
      - name: ðŸ Setup Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      
      - name: ðŸ“¦ Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: ðŸ”§ Setup Directories
        run: |
          mkdir -p data/{csv,md,pdf,processed}
          mkdir -p logs/{latest,reports,archives}
          mkdir -p backups temp configs
          echo "ðŸ“ Directory structure created for search pipeline"
      
      - name: ðŸ“¥ Download Watchlist
        run: |
          echo "ðŸ“¥ Downloading company watchlist..."
          
          if python factset_cli.py download-watchlist --validate; then
            echo "âœ… Watchlist downloaded successfully"
          else
            echo "âš ï¸ Watchlist download failed, but continuing with existing data"
          fi
      
      - name: ðŸ” Execute v3.3.3 Search Phase
        id: search
        run: |
          echo "ðŸ” Starting FactSet Search Pipeline v3.3.3..."
          echo "ðŸ“Š Search Configuration:"
          echo "  - Mode: ${{ inputs.mode || 'enhanced' }}"
          echo "  - Priority: ${{ inputs.priority || 'high_only' }}"
          echo "  - Memory Limit: ${{ inputs.memory_limit || '2048' }}MB"
          
          START_TIME=$(date +%s)
          
          # Build search command
          CMD="python factset_cli.py search --mode=${{ inputs.mode || 'enhanced' }} --priority=${{ inputs.priority || 'high_only' }} --github-actions"
          
          echo "ðŸ”§ Executing: $CMD"
          
          # Execute search with error handling
          if eval $CMD; then
            echo "âœ… Search phase completed successfully"
            SEARCH_SUCCESS=true
          else
            echo "âŒ Search phase encountered errors (but may have partial results)"
            SEARCH_SUCCESS=false
          fi
          
          END_TIME=$(date +%s)
          EXECUTION_TIME=$((END_TIME - START_TIME))
          
          # Count results - use safer file counting
          if [ -d "data/md" ]; then
            MD_FILES_FOUND=$(find data/md -name "*.md" -type f 2>/dev/null | wc -l)
          else
            MD_FILES_FOUND=0
          fi
          
          # Check for companies processed by looking at search logs or data structure
          COMPANIES_SEARCHED=0
          if [ -f "logs/latest/search.log" ]; then
            COMPANIES_SEARCHED=$(grep -c "Processing company" "logs/latest/search.log" 2>/dev/null || echo "0")
          fi
          
          # Count errors safely
          SEARCH_ERRORS=0
          if [ -f "logs/latest/search.log" ]; then
            SEARCH_ERRORS=$(grep -c "ERROR" "logs/latest/search.log" 2>/dev/null || echo "0")
          fi
          
          # Determine overall search status
          if [ "$SEARCH_SUCCESS" = "true" ] && [ "$MD_FILES_FOUND" -gt 0 ]; then
            SEARCH_STATUS="success"
          elif [ "$MD_FILES_FOUND" -gt 0 ]; then
            SEARCH_STATUS="partial_success"  
          else
            SEARCH_STATUS="failed"
          fi
          
          # v3.3.3 FIX: Use GITHUB_OUTPUT with proper validation
          echo "status=$SEARCH_STATUS" >> $GITHUB_OUTPUT
          echo "files_found=$MD_FILES_FOUND" >> $GITHUB_OUTPUT
          echo "companies_searched=$COMPANIES_SEARCHED" >> $GITHUB_OUTPUT
          echo "search_errors=$SEARCH_ERRORS" >> $GITHUB_OUTPUT
          echo "execution_time=$EXECUTION_TIME" >> $GITHUB_OUTPUT
          
          echo "âœ… Search phase completed with status: $SEARCH_STATUS"
          echo "ðŸ“Š Results: $MD_FILES_FOUND files, $COMPANIES_SEARCHED companies, ${EXECUTION_TIME}s"
          
          # Debug output
          echo "ðŸ” Debug Info:"
          echo "  - data/md exists: $([ -d 'data/md' ] && echo 'yes' || echo 'no')"
          echo "  - logs/latest/search.log exists: $([ -f 'logs/latest/search.log' ] && echo 'yes' || echo 'no')"
          echo "  - Working directory: $(pwd)"
          echo "  - Directory contents: $(ls -la data/ 2>/dev/null || echo 'data directory not found')"
      
      - name: ðŸ“¤ Upload Search Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: search-results-v333
          path: |
            data/md/*.md
            logs/latest/search.log
            data/csv/*
          retention-days: 30
      
      - name: ðŸ’¾ Commit Search Results
        if: always()
        run: |
          echo "ðŸ’¾ Committing search results..."
          
          # Configure git
          git config --global user.name "github-actions[bot]"
          git config --global user.email "41898282+github-actions[bot]@users.noreply.github.com"
          
          # Add search results
          git add data/md/ data/csv/ logs/latest/search.log || true
          
          # Commit if there are changes
          if git diff --staged --quiet; then
            echo "â„¹ï¸ No search changes to commit"
          else
            git commit -m "ðŸ” Search Results v3.3.3 - $(date +%Y-%m-%d\ %H:%M:%S)" || true
            git push || true
            echo "âœ… Search results committed"
          fi
      
      - name: ðŸ“‹ Search Summary
        if: always()
        run: |
          echo "## ðŸ” v3.3.3 Search Pipeline Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Status**: ${{ steps.search.outputs.status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Files Found**: ${{ steps.search.outputs.files_found }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Companies Searched**: ${{ steps.search.outputs.companies_searched }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Search Errors**: ${{ steps.search.outputs.search_errors }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Execution Time**: ${{ steps.search.outputs.execution_time }}s" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸŽ¯ Search Phase Features:" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Enhanced Search Mode" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Priority-based Company Selection" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Rate Limiting Protection" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Error Recovery & Retry Logic" >> $GITHUB_STEP_SUMMARY

  # ============================================================================
  # v3.3.3 PROCESS PIPELINE JOB - RUNS REGARDLESS OF SEARCH STATUS
  # ============================================================================
  process_pipeline:
    name: ðŸ“Š Process v3.3.3 Pipeline
    needs: [validate, search_pipeline]
    runs-on: ubuntu-latest
    timeout-minutes: 30
    # CRITICAL: This job runs even if search fails - processes any available data
    if: always() && needs.validate.outputs.status == 'success'
    
    outputs:
      process_status: ${{ steps.process.outputs.status }}
      companies_processed: ${{ steps.process.outputs.companies_processed }}
      quality_average: ${{ steps.process.outputs.quality_average }}
      quality_distribution: ${{ steps.process.outputs.quality_distribution }}
      files_processed: ${{ steps.process.outputs.files_processed }}
      execution_time: ${{ steps.process.outputs.execution_time }}
      dashboard_url: ${{ steps.dashboard.outputs.dashboard_url }}
    
    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
      
      - name: ðŸ Setup Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      
      - name: ðŸ“¦ Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: ðŸ”§ Setup Directories
        run: |
          mkdir -p data/{csv,md,pdf,processed}
          mkdir -p logs/{latest,reports,archives}
          mkdir -p backups temp configs
          echo "ðŸ“ Directory structure created for process pipeline"
      
      - name: ðŸ“¤ Download Search Results (if available)
        if: needs.search_pipeline.result != 'skipped'
        uses: actions/download-artifact@v4
        with:
          name: search-results-v333
          path: .
        continue-on-error: true
      
      - name: ðŸ“Š Execute v3.3.3 Processing Phase
        id: process
        run: |
          echo "ðŸ“Š Starting FactSet Processing Pipeline v3.3.3..."
          echo "ðŸ“Š Processing Configuration:"
          echo "  - Mode: v333 (standardized)"
          echo "  - Memory Limit: ${{ inputs.memory_limit || '2048' }}MB"
          echo "  - Quality Scoring: ${{ inputs.enable_quality_scoring || 'true' }}"
          echo "  - Quality Indicators: ${{ inputs.enable_quality_indicators || 'true' }}"
          echo "  - v3.3.3 Format: ${{ inputs.v333_format || 'true' }}"
          echo "  - Search Status: ${{ needs.search_pipeline.outputs.search_status || 'not_run' }}"
          
          START_TIME=$(date +%s)
          
          # Check available data
          MD_FILES_AVAILABLE=$(find data/md -name "*.md" 2>/dev/null | wc -l || echo "0")
          echo "ðŸ“ Available MD files: ${MD_FILES_AVAILABLE}"
          
          if [ "$MD_FILES_AVAILABLE" -eq 0 ]; then
            echo "âš ï¸ No MD files found - checking for existing processed data or fallback data"
            
            # Look for any existing data to process
            if [ -f "data/processed/portfolio_summary.csv" ]; then
              echo "ðŸ“‹ Found existing processed data - will refresh analysis"
            elif [ -f "è§€å¯Ÿåå–®.csv" ]; then
              echo "ðŸ“‹ Found watchlist - will create basic analysis"
            else
              echo "âŒ No data available for processing"
              echo "status=no_data" >> $GITHUB_OUTPUT
              echo "companies_processed=0" >> $GITHUB_OUTPUT
              echo "quality_average=0" >> $GITHUB_OUTPUT
              echo "quality_distribution={}" >> $GITHUB_OUTPUT
              echo "files_processed=0" >> $GITHUB_OUTPUT
              echo "execution_time=0" >> $GITHUB_OUTPUT
              exit 0
            fi
          fi
          
          # Build processing command
          CMD="python factset_cli.py process --mode=v333 --memory-limit=${{ inputs.memory_limit || '2048' }} --github-actions"
          
          # Add quality scoring flags if enabled
          if [ "${{ inputs.enable_quality_scoring || 'true' }}" = "true" ]; then
            CMD="$CMD --quality-scoring --standardize-quality"
          fi
          
          echo "ðŸ”§ Executing: $CMD"
          
          # Execute processing with error handling
          if eval $CMD; then
            echo "âœ… Processing phase completed successfully"
            PROCESS_SUCCESS=true
          else
            echo "âŒ Processing phase encountered errors"
            PROCESS_SUCCESS=false
          fi
          
          END_TIME=$(date +%s)
          EXECUTION_TIME=$((END_TIME - START_TIME))
          
          # Extract processing statistics safely
          COMPANIES_PROCESSED=0
          QUALITY_AVERAGE=0
          QUALITY_DIST="{}"
          FILES_PROCESSED=0
          
          if [ -f "data/processed/statistics.json" ]; then
            echo "ðŸ“Š Found statistics.json, extracting metrics..."
            
            # Use Python to safely extract values
            COMPANIES_PROCESSED=$(python3 -c "
import json
try:
    with open('data/processed/statistics.json', 'r') as f:
        data = json.load(f)
    print(data.get('companies_with_data', 0))
except:
    print('0')
" 2>/dev/null || echo "0")
            
            QUALITY_AVERAGE=$(python3 -c "
import json
try:
    with open('data/processed/statistics.json', 'r') as f:
        data = json.load(f)
    print(data.get('quality_analysis_v333', {}).get('average_quality_score', 0))
except:
    print('0')
" 2>/dev/null || echo "0")
            
            FILES_PROCESSED=$(python3 -c "
import json
try:
    with open('data/processed/statistics.json', 'r') as f:
        data = json.load(f)
    print(data.get('files_processed', 0))
except:
    print('0')
" 2>/dev/null || echo "0")
            
            # Quality distribution as string (simpler)
            QUALITY_DIST_RAW=$(python3 -c "
import json
try:
    with open('data/processed/statistics.json', 'r') as f:
        data = json.load(f)
    dist = data.get('quality_analysis_v333', {}).get('quality_distribution', {})
    print(json.dumps(dist))
except:
    print('{}')
" 2>/dev/null || echo "{}")
            
            QUALITY_DIST="$QUALITY_DIST_RAW"
            
          else
            echo "âš ï¸ Statistics file not found, using basic metrics..."
            if [ -d "data/processed" ]; then
              COMPANIES_PROCESSED=$(find data/processed -name "*.csv" -type f 2>/dev/null | wc -l)
            fi
            FILES_PROCESSED=$MD_FILES_AVAILABLE
          fi
          
          # Check for key output files
          CSV_FILES_GENERATED=0
          if [ -f "data/processed/portfolio_summary.csv" ]; then
            echo "âœ… Generated: data/processed/portfolio_summary.csv"
            CSV_FILES_GENERATED=$((CSV_FILES_GENERATED + 1))
          fi
          if [ -f "data/processed/detailed_data.csv" ]; then
            echo "âœ… Generated: data/processed/detailed_data.csv"
            CSV_FILES_GENERATED=$((CSV_FILES_GENERATED + 1))
          fi
          
          # Determine overall processing status
          if [ "$PROCESS_SUCCESS" = "true" ] && [ "$CSV_FILES_GENERATED" -gt 0 ]; then
            PROCESS_STATUS="success"
          elif [ "$CSV_FILES_GENERATED" -gt 0 ]; then
            PROCESS_STATUS="partial_success"
          elif [ "$MD_FILES_AVAILABLE" -eq 0 ]; then
            PROCESS_STATUS="no_data"
          else
            PROCESS_STATUS="failed"
          fi
          
          # v3.3.3 FIX: Use GITHUB_OUTPUT with proper escaping
          echo "status=$PROCESS_STATUS" >> $GITHUB_OUTPUT
          echo "companies_processed=$COMPANIES_PROCESSED" >> $GITHUB_OUTPUT
          echo "quality_average=$QUALITY_AVERAGE" >> $GITHUB_OUTPUT
          echo "files_processed=$FILES_PROCESSED" >> $GITHUB_OUTPUT
          echo "execution_time=$EXECUTION_TIME" >> $GITHUB_OUTPUT
          
          # Handle quality distribution safely
          if [ "$QUALITY_DIST" != "{}" ] && [ -n "$QUALITY_DIST" ]; then
            echo "quality_distribution<<EOF" >> $GITHUB_OUTPUT
            echo "$QUALITY_DIST" >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          else
            echo "quality_distribution={}" >> $GITHUB_OUTPUT
          fi
          
          echo "âœ… Processing phase completed with status: ${PROCESS_STATUS}"
          echo "ðŸ“Š Results: ${COMPANIES_PROCESSED} companies, ${FILES_PROCESSED} files, ${EXECUTION_TIME}s"
          echo "ðŸŽ¯ Quality: ${QUALITY_AVERAGE}/10 average"
      
      - name: ðŸ“ˆ Upload to Google Sheets
        if: steps.process.outputs.status == 'success' || steps.process.outputs.status == 'partial_success'
        run: |
          echo "ðŸ“ˆ Uploading processed data to Google Sheets..."
          
          # Build upload command
          CMD="python factset_cli.py upload --sheets=all --github-actions"
          
          # Add v3.3.3 format flags if enabled
          if [ "${{ inputs.v333_format || 'true' }}" = "true" ]; then
            CMD="$CMD --v333-format"
          fi
          
          if [ "${{ inputs.enable_quality_indicators || 'true' }}" = "true" ]; then
            CMD="$CMD --quality-indicators"
          fi
          
          echo "ðŸ”§ Executing: $CMD"
          
          if eval $CMD; then
            echo "âœ… Data uploaded to Google Sheets successfully"
          else
            echo "âš ï¸ Upload encountered issues but processing completed"
          fi
      
      - name: ðŸ“Š Generate v3.3.3 Report
        if: always()
        run: |
          echo "ðŸ“Š Generating v3.3.3 comprehensive report..."
          
          # Try to generate report, fall back to manual summary if command fails
          if python factset_cli.py report --format=github-summary --v333-metrics 2>/dev/null; then
            echo "âœ… Report generated successfully"
          else
            echo "ðŸ“ Creating manual processing summary..."
            cat > processing_report_v333.md << EOF
          # ðŸ“Š FactSet Processing Pipeline v3.3.3 Report
          
          **Execution Date:** $(date)
          **Pipeline Version:** v3.3.3 Final Integrated Edition
          **Search Status:** ${{ needs.search_pipeline.outputs.search_status || 'not_run' }}
          **Processing Mode:** v333 (standardized)
          
          ## ðŸ“ˆ Processing Results
          - **Status:** ${{ steps.process.outputs.process_status }}
          - **Companies Processed:** ${{ steps.process.outputs.companies_processed }}
          - **Files Processed:** ${{ steps.process.outputs.files_processed }}
          - **Execution Time:** ${{ steps.process.outputs.execution_time }}s
          - **Average Quality:** ${{ steps.process.outputs.quality_average }}/10
          
          ## ðŸ” Search Phase Results (if run)
          - **Search Status:** ${{ needs.search_pipeline.outputs.search_status || 'skipped' }}
          - **Files Found:** ${{ needs.search_pipeline.outputs.files_found || '0' }}
          - **Companies Searched:** ${{ needs.search_pipeline.outputs.companies_searched || '0' }}
          
          ## ðŸ“ Generated Files
          \`\`\`
          $(find data/processed/ -name "*.csv" -o -name "*.json" | head -10)
          \`\`\`
          
          ## ðŸ”— Quick Links
          - [Live Dashboard](https://docs.google.com/spreadsheets/d/${{ env.GOOGLE_SHEET_ID }}/edit)
          - [GitHub Repository](https://github.com/${{ github.repository }})
          
          *Generated by FactSet Processing Pipeline v3.3.3*
          EOF
          fi
      
      - name: ðŸ’¾ Commit Processing Results
        if: always()
        run: |
          echo "ðŸ’¾ Committing processing results..."
          
          # Configure git
          git config --global user.name "github-actions[bot]"
          git config --global user.email "41898282+github-actions[bot]@users.noreply.github.com"
          
          # Add processed results
          git add data/processed/ processing_report_v333.md || true
          git add logs/latest/process.log || true
          
          # Commit if there are changes
          if git diff --staged --quiet; then
            echo "â„¹ï¸ No processing changes to commit"
          else
            git commit -m "ðŸ“Š Processing Results v3.3.3 - $(date +%Y-%m-%d\ %H:%M:%S)" || true
            git push || true
            echo "âœ… Processing results committed"
          fi
      
      - name: ðŸŒ Update Live Dashboard
        id: dashboard
        if: always()
        run: |
          echo "ðŸŒ Live dashboard updated with v3.3.3 processed data"
          DASHBOARD_URL="https://docs.google.com/spreadsheets/d/${{ env.GOOGLE_SHEET_ID }}/edit"
          echo "ðŸ“ˆ View Results: ${DASHBOARD_URL}"
          
          # v3.3.3 FIX: Correct Live Dashboard URL
          echo "dashboard_url=${DASHBOARD_URL}" >> $GITHUB_OUTPUT
      
      - name: ðŸ“¤ Upload v3.3.3 Processing Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: processing-results-v333
          path: |
            data/processed/*.csv
            data/processed/*.json
            logs/latest/process.log
            processing_report_v333.md
          retention-days: 30
      
      - name: ðŸ“‹ Processing Summary
        if: always()
        run: |
          echo "## ðŸ“Š v3.3.3 Processing Pipeline Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Processing Status**: ${{ steps.process.outputs.status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Search Status**: ${{ needs.search_pipeline.outputs.search_status || 'not_run' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Companies Processed**: ${{ steps.process.outputs.companies_processed }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Average Quality Score**: ${{ steps.process.outputs.quality_average }}/10" >> $GITHUB_STEP_SUMMARY
          echo "- **Files Processed**: ${{ steps.process.outputs.files_processed }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Execution Time**: ${{ steps.process.outputs.execution_time }}s" >> $GITHUB_STEP_SUMMARY
          echo "- **Dashboard**: [View Live Results](${{ steps.dashboard.outputs.dashboard_url }})" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸŽ¯ v3.3.3 Processing Features:" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Standardized Quality Scoring (0-10 scale)" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Quality Indicators (ðŸŸ¢ðŸŸ¡ðŸŸ ðŸ”´)" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… GitHub Raw URL MD Links" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Resilient Processing (runs regardless of search status)" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… v3.3.3 CSV Format & Statistics" >> $GITHUB_STEP_SUMMARY

  # ============================================================================
  # v3.3.3 QUALITY ANALYSIS JOB
  # ============================================================================
  quality_analysis:
    name: ðŸŽ¯ v3.3.3 Quality Analysis
    needs: [process_pipeline]
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: always() && (needs.process_pipeline.outputs.process_status == 'success' || needs.process_pipeline.outputs.process_status == 'partial_success')
    
    outputs:
      quality_report: ${{ steps.quality.outputs.report }}
      quality_trends: ${{ steps.quality.outputs.trends }}
    
    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4
      
      - name: ðŸ Setup Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: ðŸ“¦ Install Dependencies
        run: pip install -r requirements.txt
      
      - name: ðŸ“¤ Download Processing Results
        uses: actions/download-artifact@v4
        with:
          name: processing-results-v333
          path: .
        continue-on-error: true
      
      - name: ðŸŽ¯ Run v3.3.3 Quality Analysis
        id: quality
        run: |
          echo "ðŸŽ¯ Running comprehensive v3.3.3 quality analysis..."
          
          # Check if we have data to analyze
          if [ -f "data/processed/portfolio_summary.csv" ] || [ -f "data/processed/detailed_data.csv" ]; then
            # Try quality analysis command with fallback
            if python factset_cli.py quality --analyze --v333 2>/dev/null; then
              echo "âœ… Quality analysis completed"
              echo "report=generated" >> $GITHUB_OUTPUT
              echo "trends=analyzed" >> $GITHUB_OUTPUT
            else
              echo "ðŸ“Š Creating manual quality summary..."
              echo "Quality analysis completed with basic metrics" > quality_trends.json
              echo "report=manual" >> $GITHUB_OUTPUT
              echo "trends=basic" >> $GITHUB_OUTPUT
            fi
          else
            echo "âš ï¸ No processed data files found for quality analysis"
            echo "report=no_data" >> $GITHUB_OUTPUT
            echo "trends=no_data" >> $GITHUB_OUTPUT
          fi
      
      - name: ðŸ“Š Quality Summary
        run: |
          echo "## ðŸŽ¯ v3.3.3 Quality Analysis Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Quality Report**: ${{ steps.quality.outputs.report }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Trend Analysis**: ${{ steps.quality.outputs.trends }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Average Quality**: ${{ needs.process_pipeline.outputs.quality_average }}/10" >> $GITHUB_STEP_SUMMARY
          echo "- **Companies Analyzed**: ${{ needs.process_pipeline.outputs.companies_processed }}" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "quality_trends.json" ]; then
            echo "- **Quality Trends**: Available in artifacts" >> $GITHUB_STEP_SUMMARY
          fi

  # ============================================================================
  # v3.3.3 RECOVERY JOB (Only on failures)
  # ============================================================================
  recovery:
    name: ðŸ”„ v3.3.3 Smart Recovery
    needs: [validate, search_pipeline, process_pipeline]
    runs-on: ubuntu-latest
    timeout-minutes: 15
    if: failure() || (always() && (needs.search_pipeline.outputs.search_status == 'failed' || needs.process_pipeline.outputs.process_status == 'failed'))
    
    outputs:
      recovery_status: ${{ steps.recovery.outputs.status }}
      recovery_actions: ${{ steps.recovery.outputs.actions }}
    
    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4
      
      - name: ðŸ Setup Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: ðŸ“¦ Install Dependencies
        run: pip install -r requirements.txt
      
      - name: ðŸ”„ Execute v3.3.3 Recovery
        id: recovery
        run: |
          echo "ðŸ”„ Starting v3.3.3 intelligent recovery..."
          echo "ðŸ“Š Recovery Context:"
          echo "  - Validation: ${{ needs.validate.result || 'not_run' }}"
          echo "  - Search: ${{ needs.search_pipeline.result || 'not_run' }} (Status: ${{ needs.search_pipeline.outputs.search_status || 'unknown' }})"
          echo "  - Process: ${{ needs.process_pipeline.result || 'not_run' }} (Status: ${{ needs.process_pipeline.outputs.process_status || 'unknown' }})"
          
          # Create essential directories
          mkdir -p data/{csv,md,pdf,processed}
          mkdir -p logs/{latest,reports,archives}
          mkdir -p backups temp configs
          echo "ðŸ“ Essential directories created"
          
          # Determine recovery strategy based on what failed
          RECOVERY_TYPE="unknown"
          
          if [ "${{ needs.search_pipeline.result }}" = "failure" ] && [ "${{ needs.process_pipeline.result }}" = "success" ]; then
            echo "ðŸ” Search failed but processing succeeded - attempting conservative search"
            if python factset_cli.py search --mode=conservative --priority=top_30 2>/dev/null; then
              RECOVERY_TYPE="search_conservative_success"
            else
              RECOVERY_TYPE="search_failed_but_process_ok"
            fi
          
          elif [ "${{ needs.process_pipeline.result }}" = "failure" ] && [ "${{ needs.search_pipeline.result }}" = "success" ]; then
            echo "ðŸ“Š Processing failed but search succeeded - attempting basic processing"
            if python factset_cli.py process --mode=conservative --memory-limit=1024 2>/dev/null; then
              RECOVERY_TYPE="process_conservative_success"
            else
              RECOVERY_TYPE="process_failed_but_search_ok"
            fi
          
          elif [ "${{ needs.search_pipeline.result }}" = "failure" ] && [ "${{ needs.process_pipeline.result }}" = "failure" ]; then
            echo "âŒ Both search and processing failed - attempting full conservative recovery"
            if python factset_cli.py pipeline --mode=conservative --memory-limit=1024 2>/dev/null; then
              RECOVERY_TYPE="full_conservative_success"
            else
              RECOVERY_TYPE="full_recovery_failed"
            fi
          
          else
            echo "ðŸ”§ Attempting general recovery procedures"
            if python factset_cli.py recover --analyze --fix-common-issues 2>/dev/null; then
              RECOVERY_TYPE="general_recovery_success"
            else
              RECOVERY_TYPE="general_recovery_failed"
            fi
          fi
          
          # v3.3.3 FIX: Use GITHUB_OUTPUT
          echo "status=${RECOVERY_TYPE}" >> $GITHUB_OUTPUT
          echo "actions=intelligent_recovery_v333" >> $GITHUB_OUTPUT
          
          echo "âœ… Recovery completed with type: ${RECOVERY_TYPE}"
      
      - name: ðŸ©º Diagnostic Report
        if: always()
        run: |
          echo "ðŸ©º Generating v3.3.3 diagnostic report..."
          
          # Try diagnostic command with fallback
          if python factset_cli.py diagnose --v333-comprehensive 2>/dev/null; then
            echo "âœ… Advanced diagnostics completed successfully"
          else
            echo "ðŸ“‹ Creating manual diagnostic summary..."
            cat > diagnostic_report_v333.md << EOF
          ## ðŸ©º v3.3.3 Diagnostic Summary
          - **Recovery Time**: $(date)
          - **Recovery Type**: ${{ steps.recovery.outputs.status }}
          - **Validation**: ${{ needs.validate.result || 'not_run' }}
          - **Search Pipeline**: ${{ needs.search_pipeline.result || 'not_run' }} (Status: ${{ needs.search_pipeline.outputs.search_status || 'unknown' }})
          - **Process Pipeline**: ${{ needs.process_pipeline.result || 'not_run' }} (Status: ${{ needs.process_pipeline.outputs.process_status || 'unknown' }})
          
          ### ðŸ“Š Search Results:
          - Files Found: ${{ needs.search_pipeline.outputs.files_found || '0' }}
          - Companies Searched: ${{ needs.search_pipeline.outputs.companies_searched || '0' }}
          - Search Errors: ${{ needs.search_pipeline.outputs.search_errors || '0' }}
          
          ### ðŸ“ˆ Processing Results:
          - Companies Processed: ${{ needs.process_pipeline.outputs.companies_processed || '0' }}
          - Average Quality: ${{ needs.process_pipeline.outputs.quality_average || '0' }}/10
          - Files Processed: ${{ needs.process_pipeline.outputs.files_processed || '0' }}
          
          ### ðŸ’¡ Recommendations:
          1. Check individual job logs for specific error details
          2. Verify all required environment variables are set
          3. Consider running jobs individually for debugging
          4. Check data availability and API limits
          EOF
          fi
          
          echo "## ðŸ”„ v3.3.3 Recovery Report" >> $GITHUB_STEP_SUMMARY
          echo "- **Recovery Status**: ${{ steps.recovery.outputs.status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Actions Taken**: ${{ steps.recovery.outputs.actions }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Search Result**: ${{ needs.search_pipeline.result || 'not_run' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Process Result**: ${{ needs.process_pipeline.result || 'not_run' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Next Steps**: Review individual job logs and consider manual execution" >> $GITHUB_STEP_SUMMARY

  # ============================================================================
  # v3.3.3 NOTIFICATION JOB
  # ============================================================================
  notify:
    name: ðŸ“§ v3.3.3 Notification
    needs: [validate, search_pipeline, process_pipeline, quality_analysis]
    runs-on: ubuntu-latest
    timeout-minutes: 5
    if: always()
    
    steps:
      - name: ðŸ“§ Send Success Notification
        if: needs.process_pipeline.result == 'success'
        run: |
          echo "ðŸ“§ v3.3.3 Pipeline completed successfully!"
          echo "ðŸ” Search: ${{ needs.search_pipeline.outputs.search_status || 'not_run' }} (${{ needs.search_pipeline.outputs.files_found || '0' }} files)"
          echo "ðŸ“Š Process: ${{ needs.process_pipeline.outputs.process_status || 'failed' }} (${{ needs.process_pipeline.outputs.companies_processed || '0' }} companies)"
          echo "ðŸŽ¯ Quality: ${{ needs.process_pipeline.outputs.quality_average || '0' }}/10"
          echo "ðŸ“ Files: ${{ needs.process_pipeline.outputs.files_processed || '0' }}"
          echo "â±ï¸ Time: Search=${{ needs.search_pipeline.outputs.execution_time || '0' }}s, Process=${{ needs.process_pipeline.outputs.execution_time || '0' }}s"
          echo "ðŸ“ˆ Dashboard: ${{ needs.process_pipeline.outputs.dashboard_url }}"
      
      - name: âš ï¸ Send Partial Success Notification
        if: needs.process_pipeline.result == 'success' && (needs.search_pipeline.result == 'failure' || needs.search_pipeline.outputs.search_status == 'failed')
        run: |
          echo "âš ï¸ v3.3.3 Pipeline completed with partial success"
          echo "ðŸ” Search: FAILED (but processing succeeded with available data)"
          echo "ðŸ“Š Process: SUCCESS (${{ needs.process_pipeline.outputs.companies_processed || '0' }} companies)"
          echo "ðŸŽ¯ Quality: ${{ needs.process_pipeline.outputs.quality_average || '0' }}/10"
          echo "ðŸ’¡ Recommendation: Review search logs and consider manual search execution"
      
      - name: âŒ Send Failure Notification
        if: needs.process_pipeline.result == 'failure' || needs.validate.result == 'failure'
        run: |
          echo "âŒ v3.3.3 Pipeline encountered critical issues"
          echo "ðŸ” Validation: ${{ needs.validate.result || 'not_run' }}"
          echo "ðŸ” Search: ${{ needs.search_pipeline.result || 'not_run' }} (Status: ${{ needs.search_pipeline.outputs.search_status || 'unknown' }})"
          echo "ðŸ” Process: ${{ needs.process_pipeline.result || 'not_run' }} (Status: ${{ needs.process_pipeline.outputs.process_status || 'unknown' }})"
          echo "ðŸ”„ Recovery: ${{ needs.recovery.outputs.recovery_status || 'not_attempted' }}"
          echo "ðŸ“‹ Check individual job logs for detailed error information"
      
      - name: ðŸ“Š Final Status Summary
        if: always()
        run: |
          echo "## ðŸ“Š Final v3.3.3 Workflow Status (Separated Jobs)" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status | Details |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|---------|" >> $GITHUB_STEP_SUMMARY
          echo "| Validation | ${{ needs.validate.result }} | ${{ needs.validate.outputs.status }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Search Pipeline | ${{ needs.search_pipeline.result || 'not_run' }} | ${{ needs.search_pipeline.outputs.search_status || 'skipped' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Process Pipeline | ${{ needs.process_pipeline.result }} | ${{ needs.process_pipeline.outputs.process_status }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Quality Analysis | ${{ needs.quality_analysis.result || 'not_run' }} | ${{ needs.quality_analysis.outputs.quality_report || 'not_run' }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ” Search Metrics:" >> $GITHUB_STEP_SUMMARY
          echo "- **Files Found**: ${{ needs.search_pipeline.outputs.files_found || '0' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Companies Searched**: ${{ needs.search_pipeline.outputs.companies_searched || '0' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Search Errors**: ${{ needs.search_pipeline.outputs.search_errors || '0' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Search Time**: ${{ needs.search_pipeline.outputs.execution_time || '0' }}s" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ“Š Processing Metrics:" >> $GITHUB_STEP_SUMMARY
          echo "- **Companies Processed**: ${{ needs.process_pipeline.outputs.companies_processed || '0' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Files Processed**: ${{ needs.process_pipeline.outputs.files_processed || '0' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Average Quality**: ${{ needs.process_pipeline.outputs.quality_average || '0' }}/10" >> $GITHUB_STEP_SUMMARY
          echo "- **Processing Time**: ${{ needs.process_pipeline.outputs.execution_time || '0' }}s" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸŽ¯ Key Benefits of Separated Jobs:" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Resilient Processing**: Process job runs even if search fails" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Better Error Isolation**: Easier to identify which phase failed" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Flexible Execution**: Can skip search phase if needed" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Improved Debugging**: Separate logs and artifacts for each phase" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… **Partial Success Handling**: Process existing data even with search issues" >> $GITHUB_STEP_SUMMARY

# ============================================================================
# v3.3.3 SEPARATED JOBS WORKFLOW CONCLUSION
# ============================================================================