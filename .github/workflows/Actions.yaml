name: FactSet Pipeline v3.3.0 - Enhanced EPS Breakdown

on:
  push:
    branches:
      - main
  workflow_dispatch:
    inputs:
      execution_mode:
        description: 'Execution strategy'
        required: false
        default: 'conservative'
        type: choice
        options:
          - 'conservative'    # High priority only, with delays
          - 'process_only'    # Process existing data only
          - 'test_only'       # Test components without search
          - 'force_search'    # Force search (may hit rate limits)
      priority_focus:
        description: 'Search priority level'
        required: false
        default: 'high_only'
        type: choice
        options:
          - 'high_only'       # Top priority companies only
          - 'top_30'          # Top 30 companies
          - 'balanced'        # All companies (likely to hit rate limits)
      wait_for_rate_limits:
        description: 'Wait time before search (minutes)'
        required: false
        default: '30'
        type: string
  schedule:
    # Run daily at 2:10 AM UTC (when rate limits are most likely reset)
    - cron: "10 2 * * *"

env:
  PYTHONIOENCODING: utf-8
  FACTSET_PIPELINE_DEBUG: true

jobs:
  # Pre-flight checks to assess current status - v3.3.0
  preflight:
    runs-on: ubuntu-latest
    outputs:
      rate_limited: ${{ steps.check_status.outputs.rate_limited }}
      has_existing_data: ${{ steps.check_status.outputs.has_existing_data }}
      recommended_action: ${{ steps.check_status.outputs.recommended_action }}
      guideline_version: ${{ steps.check_status.outputs.guideline_version }}
    
    steps:
    - name: ðŸ“¥ Checkout Repository
      uses: actions/checkout@v4
    
    - name: ðŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: ðŸ“¦ Install Dependencies
      run: |
        pip install --upgrade pip
        pip install requests pandas python-dotenv
    
    - name: ðŸ” Check Current Status (v3.3.0)
      id: check_status
      env:
        GOOGLE_SEARCH_API_KEY: ${{ secrets.GOOGLE_SEARCH_API_KEY }}
        GOOGLE_SEARCH_CSE_ID: ${{ secrets.GOOGLE_SEARCH_CSE_ID }}
      run: |
        echo "ðŸ” Checking pipeline status (Guideline v3.3.0)..."
        
        # Set guideline version
        echo "guideline_version=3.3.0" >> $GITHUB_OUTPUT
        
        # Check for existing data
        existing_md_files=$(find data/md -name "*.md" 2>/dev/null | wc -l || echo "0")
        existing_csv_files=$(find data/csv -name "*.csv" 2>/dev/null | wc -l || echo "0")
        existing_processed=$(find data/processed -name "*.csv" -o -name "*.json" 2>/dev/null | wc -l || echo "0")
        
        echo "ðŸ“Š Found $existing_md_files MD files, $existing_csv_files CSV files, $existing_processed processed files"
        
        # Enhanced data quality check - v3.3.0
        if [ "$existing_md_files" -gt "10" ] && [ "$existing_processed" -gt "2" ]; then
          echo "has_existing_data=true" >> $GITHUB_OUTPUT
          echo "ðŸ“Š Quality existing data detected"
        elif [ "$existing_md_files" -gt "5" ]; then
          echo "has_existing_data=moderate" >> $GITHUB_OUTPUT
          echo "ðŸ“Š Moderate existing data detected"
        else
          echo "has_existing_data=false" >> $GITHUB_OUTPUT
          echo "ðŸ“Š Limited existing data"
        fi
        
        # Test rate limiting status with enhanced detection
        echo "ðŸ§ª Testing search access (enhanced rate limiting check v3.3.0)..."
        rate_limited="false"
        
        # Create enhanced test script for v3.3.0
        cat > test_search_v330.py << 'EOF'
        import requests
        import os
        import sys
        import time
        
        def test_search_api():
            api_key = os.environ.get('GOOGLE_SEARCH_API_KEY')
            cse_id = os.environ.get('GOOGLE_SEARCH_CSE_ID')
            
            if not api_key or not cse_id:
                print("API_CREDENTIALS_MISSING")
                return 1
            
            url = "https://www.googleapis.com/customsearch/v1"
            params = {
                'key': api_key,
                'cx': cse_id,
                'q': 'factset test search',
                'num': 1,
                'lr': 'lang_zh-TW',
                'safe': 'active'
            }
            
            try:
                print("Testing Google Search API v3.3.0...")
                response = requests.get(url, params=params, timeout=30)
                
                if response.status_code == 429:
                    print("RATE_LIMITED_429")
                    return 2
                elif response.status_code == 403:
                    print("RATE_LIMITED_403")
                    return 2
                elif 'rate limit' in response.text.lower():
                    print("RATE_LIMITED_CONTENT")
                    return 2
                elif 'sorry/index' in response.text.lower():
                    print("RATE_LIMITED_SORRY")
                    return 2
                elif response.status_code == 200:
                    data = response.json()
                    if 'items' in data:
                        print("SEARCH_AVAILABLE")
                        return 0
                    else:
                        print("SEARCH_NO_RESULTS")
                        return 3
                else:
                    print(f"SEARCH_ERROR_{response.status_code}")
                    return 4
                    
            except requests.exceptions.Timeout:
                print("SEARCH_TIMEOUT")
                return 5
            except requests.exceptions.ConnectionError:
                print("SEARCH_CONNECTION_ERROR")
                return 6
            except Exception as e:
                print(f"SEARCH_EXCEPTION: {str(e)}")
                return 7
        
        if __name__ == "__main__":
            exit_code = test_search_api()
            sys.exit(exit_code)
        EOF
        
        python test_search_v330.py
        search_status=$?
        
        # Enhanced status determination for v3.3.0
        case $search_status in
          0)
            echo "âœ… Search API accessible and functional"
            echo "rate_limited=false" >> $GITHUB_OUTPUT
            if [ "$existing_md_files" -gt "20" ]; then
              echo "recommended_action=conservative" >> $GITHUB_OUTPUT
            else
              echo "recommended_action=search" >> $GITHUB_OUTPUT
            fi
            ;;
          2)
            echo "ðŸš¨ Rate limiting detected (various indicators)"
            echo "rate_limited=true" >> $GITHUB_OUTPUT
            echo "recommended_action=process_existing" >> $GITHUB_OUTPUT
            ;;
          1)
            echo "âš ï¸ API credentials missing"
            echo "rate_limited=unknown" >> $GITHUB_OUTPUT
            echo "recommended_action=process_existing" >> $GITHUB_OUTPUT
            ;;
          *)
            echo "âš ï¸ Search API issues detected (code: $search_status)"
            echo "rate_limited=unknown" >> $GITHUB_OUTPUT
            if [ "$existing_md_files" -gt "5" ]; then
              echo "recommended_action=process_existing" >> $GITHUB_OUTPUT
            else
              echo "recommended_action=skip" >> $GITHUB_OUTPUT
            fi
            ;;
        esac

  # Main pipeline execution with v3.3.0 compliance
  pipeline:
    runs-on: ubuntu-latest
    needs: preflight
    
    steps:
    - name: ðŸ“¥ Checkout Repository
      uses: actions/checkout@v4
    
    - name: ðŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: ðŸ”§ Install System Dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y wkhtmltopdf || echo "wkhtmltopdf installation failed (optional)"
        echo "âœ… System dependencies installed"
    
    - name: ðŸ“¦ Install Python Dependencies (v3.3.0)
      run: |
        pip install --upgrade pip
        
        # Install core requirements for v3.3.0
        pip install requests pandas gspread google-auth python-dotenv
        pip install beautifulsoup4 markdownify
        
        # Optional search dependencies
        pip install googlesearch-python || echo "âš ï¸ googlesearch-python failed (optional)"
        pip install validators || echo "âš ï¸ validators failed (optional)"
        
        echo "âœ… Python dependencies installed for v3.3.0"
    
    - name: ðŸ”§ Setup Configuration (v3.3.0)
      run: |
        echo "ðŸ”§ Setting up v3.3.0 configuration..."
        
        # Create directories with v3.3.0 structure
        mkdir -p data/{csv,md,pdf,processed} logs
        
        # Download target companies with error handling
        python config.py --download-csv || echo "âš ï¸ CSV download failed, will use existing"
        
        # Verify guideline compliance
        echo "ðŸ“‹ Verifying v3.3.0 compliance..."
        python -c "
        import os
        import sys
        
        # Check for required files
        required_files = [
            'factset_pipeline.py',
            'factset_search.py', 
            'data_processor.py',
            'sheets_uploader.py',
            'config.py'
        ]
        
        missing = [f for f in required_files if not os.path.exists(f)]
        if missing:
            print(f'âŒ Missing required files: {missing}')
            sys.exit(1)
        
        print('âœ… All required v3.3.0 files present')
        "
        
        echo "âœ… Configuration setup completed (v3.3.0)"
    
    - name: ðŸ§ª Test Components (v3.3.0)
      env:
        GOOGLE_SEARCH_API_KEY: ${{ secrets.GOOGLE_SEARCH_API_KEY }}
        GOOGLE_SEARCH_CSE_ID: ${{ secrets.GOOGLE_SEARCH_CSE_ID }}
        GOOGLE_SHEETS_CREDENTIALS: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}
        GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
      run: |
        echo "ðŸ§ª Testing v3.3.0 components..."
        
        # Test configuration
        python config.py --show || echo "âš ï¸ Config test failed"
        
        # Test utilities
        python -c "import utils; print('âœ… Utils module available')" || echo "âš ï¸ Utils test failed"
        
        # Test data processor
        python -c "
        import data_processor
        if hasattr(data_processor, 'process_all_data'):
            print('âœ… Data processor v3.3.0 compliant')
        else:
            print('âŒ Data processor missing required functions')
        " || echo "âš ï¸ Data processor test failed"
        
        # Test search engine
        python -c "
        import factset_search
        if hasattr(factset_search, 'run_enhanced_search_suite'):
            print('âœ… Search engine v3.3.0 compliant')
        else:
            print('âŒ Search engine missing required functions')
        " || echo "âš ï¸ Search engine test failed"
        
        # Test sheets uploader  
        python sheets_uploader.py --test-connection || echo "âš ï¸ Sheets test failed"
        
        # Test main pipeline
        python factset_pipeline.py --status || echo "âš ï¸ Pipeline status test failed"
        
        echo "âœ… Component tests completed (v3.3.0)"
    
    - name: â° Wait for Rate Limiting (if specified)
      if: github.event.inputs.wait_for_rate_limits && github.event.inputs.wait_for_rate_limits != '0'
      run: |
        wait_minutes="${{ github.event.inputs.wait_for_rate_limits }}"
        echo "â° Waiting ${wait_minutes} minutes for rate limits to clear..."
        sleep $((wait_minutes * 60))
    
    - name: ðŸ“Š Process Existing Data (v3.3.0)
      if: needs.preflight.outputs.has_existing_data == 'true' || needs.preflight.outputs.has_existing_data == 'moderate' || needs.preflight.outputs.rate_limited == 'true'
      env:
        GOOGLE_SEARCH_API_KEY: ${{ secrets.GOOGLE_SEARCH_API_KEY }}
        GOOGLE_SEARCH_CSE_ID: ${{ secrets.GOOGLE_SEARCH_CSE_ID }}
        GOOGLE_SHEETS_CREDENTIALS: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}
        GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
      run: |
        echo "ðŸ“Š Processing existing data (v3.3.0)..."
        
        # Check existing data
        python data_processor.py --check-data || echo "Data check completed with warnings"
        
        # Process MD files if they exist
        if [ -d "data/md" ] && [ "$(ls -A data/md 2>/dev/null)" ]; then
          echo "ðŸ“„ Processing existing MD files with v3.3.0 EPS breakdown..."
          python data_processor.py --force --parse-md || echo "âš ï¸ MD processing had issues"
        fi
        
        # Process any existing data with v3.3.0 compliance
        python data_processor.py --force || echo "âš ï¸ Data processing had issues"
        
        echo "âœ… Existing data processing completed (v3.3.0)"
    
    - name: ðŸ” Conservative Search (v3.3.0)
      if: needs.preflight.outputs.rate_limited != 'true' && github.event.inputs.execution_mode != 'process_only'
      env:
        GOOGLE_SEARCH_API_KEY: ${{ secrets.GOOGLE_SEARCH_API_KEY }}
        GOOGLE_SEARCH_CSE_ID: ${{ secrets.GOOGLE_SEARCH_CSE_ID }}
        GOOGLE_SHEETS_CREDENTIALS: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}
        GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
      run: |
        echo "ðŸ” Attempting conservative search (v3.3.0)..."
        
        # Determine search parameters
        priority_focus="${{ github.event.inputs.priority_focus || 'high_only' }}"
        execution_mode="${{ github.event.inputs.execution_mode || 'conservative' }}"
        
        echo "ðŸ“Š Search parameters: mode=$execution_mode, focus=$priority_focus"
        
        if [ "$execution_mode" = "force_search" ]; then
          echo "âš ï¸ Force search mode - may encounter rate limiting"
          timeout 900 python factset_search.py --priority-focus "$priority_focus" || echo "ðŸš¨ Search failed due to rate limiting"
        elif [ "$execution_mode" = "conservative" ]; then
          echo "ðŸŽ¯ Conservative search mode (v3.3.0)"
          # Try search with v3.3.0 enhanced error handling
          timeout 600 python factset_search.py --priority-focus high_only || echo "ðŸš¨ Conservative search failed"
        else
          echo "â„¹ï¸ Skipping search in test-only mode"
        fi
        
        echo "âœ… Search phase completed (v3.3.0)"
    
    - name: ðŸ“Š Final Data Processing (v3.3.0)
      env:
        GOOGLE_SEARCH_API_KEY: ${{ secrets.GOOGLE_SEARCH_API_KEY }}
        GOOGLE_SEARCH_CSE_ID: ${{ secrets.GOOGLE_SEARCH_CSE_ID }}
        GOOGLE_SHEETS_CREDENTIALS: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}
        GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
      run: |
        echo "ðŸ“Š Final data processing (v3.3.0)..."
        
        # Process any new data with v3.3.0 compliance
        python data_processor.py --force --parse-md || echo "Data processing completed with warnings"
        
        echo "âœ… Final processing completed (v3.3.0)"
    
    - name: ðŸ’¾ Validate and Commit Results (v3.3.0)
      run: |
        echo "ðŸ’¾ Validating data before commit (v3.3.0)..."
        
        # Configure git for GitHub Actions bot
        git config --global user.name "github-actions[bot]"
        git config --global user.email "41898282+github-actions[bot]@users.noreply.github.com"
        
        # Enhanced validation for v3.3.0
        echo "ðŸ” Running comprehensive v3.3.0 data validation..."
        
        # Check files with enhanced quality requirements
        md_count=$(find data/md -name '*.md' -size +100c 2>/dev/null | wc -l)
        csv_count=$(find data/csv -name '*.csv' -size +50c 2>/dev/null | wc -l)
        processed_count=$(find data/processed -name '*' -type f -size +10c 2>/dev/null | wc -l)
        
        echo "ðŸ“Š File counts: ${md_count} MD files (>100 bytes), ${csv_count} CSV files (>50 bytes), ${processed_count} processed files"
        
        # Enhanced MD file validation for v3.3.0 - Focus on EPS data
        valid_md_count=0
        factset_md_count=0
        quality_md_count=0
        eps_breakdown_count=0
        
        if [ -d "data/md" ]; then
          for file in data/md/*.md; do
            [ -f "$file" ] || continue
            
            # Check if file contains actual financial content
            if grep -q -E "(factset|EPS|é ä¼°|ç›®æ¨™åƒ¹|åˆ†æžå¸«|è²¡å ±|ç‡Ÿæ”¶)" "$file" 2>/dev/null; then
              valid_md_count=$((valid_md_count + 1))
              
              # Extra check for FactSet specific content
              if grep -q -i "factset" "$file" 2>/dev/null; then
                factset_md_count=$((factset_md_count + 1))
              fi
              
              # Check for quality indicators (multi-year EPS, etc)
              if grep -q -E "(2025|2026|2027)" "$file" 2>/dev/null; then
                quality_md_count=$((quality_md_count + 1))
              fi
              
              # v3.3.0: Check for enhanced EPS breakdown data
              if grep -q -E "(EPS.*2025.*2026.*2027|æœ€é«˜å€¼.*æœ€ä½Žå€¼.*å¹³å‡å€¼)" "$file" 2>/dev/null; then
                eps_breakdown_count=$((eps_breakdown_count + 1))
              fi
            fi
          done
        fi
        
        # Enhanced CSV validation for v3.3.0
        valid_csv_count=0
        quality_csv_count=0
        
        if [ -d "data/csv" ]; then
          for file in data/csv/*.csv; do
            [ -f "$file" ] || continue
            
            # Check if CSV has proper headers and content
            if head -1 "$file" | grep -q -E "(Title|Link|å…¬å¸|è‚¡ç¥¨|åç¨±)" 2>/dev/null; then
              if [ "$(wc -l < "$file")" -gt "1" ]; then
                valid_csv_count=$((valid_csv_count + 1))
                
                # Check for quality content
                if grep -q -E "(factset|EPS|é ä¼°)" "$file" 2>/dev/null; then
                  quality_csv_count=$((quality_csv_count + 1))
                fi
              fi
            fi
          done
        fi
        
        # Enhanced processed file validation for v3.3.0
        valid_processed_count=0
        has_portfolio_summary=false
        has_detailed_data=false
        has_statistics=false
        has_v330_format=false
        
        if [ -f "data/processed/portfolio_summary.csv" ] && [ -s "data/processed/portfolio_summary.csv" ]; then
          has_portfolio_summary=true
          valid_processed_count=$((valid_processed_count + 1))
          
          # Validate v3.3.0 portfolio summary format - Enhanced EPS breakdown
          if head -1 "data/processed/portfolio_summary.csv" | grep -q "ä»£è™Ÿ.*åç¨±.*è‚¡ç¥¨ä»£è™Ÿ.*MDæœ€èˆŠæ—¥æœŸ.*MDæœ€æ–°æ—¥æœŸ.*MDè³‡æ–™ç­†æ•¸.*åˆ†æžå¸«æ•¸é‡.*ç›®æ¨™åƒ¹.*2025EPSå¹³å‡å€¼.*2026EPSå¹³å‡å€¼.*2027EPSå¹³å‡å€¼.*å“è³ªè©•åˆ†.*ç‹€æ…‹.*æ›´æ–°æ—¥æœŸ" 2>/dev/null; then
            echo "âœ… Portfolio Summary has v3.3.0 enhanced format (EPS breakdown)"
            has_v330_format=true
          fi
        fi
        
        if [ -f "data/processed/detailed_data.csv" ] && [ -s "data/processed/detailed_data.csv" ]; then
          has_detailed_data=true
          valid_processed_count=$((valid_processed_count + 1))
          
          # Validate v3.3.0 detailed data format - Full EPS breakdown with high/low/avg
          if head -1 "data/processed/detailed_data.csv" | grep -q "2025EPSæœ€é«˜å€¼.*2025EPSæœ€ä½Žå€¼.*2025EPSå¹³å‡å€¼.*2026EPSæœ€é«˜å€¼.*2026EPSæœ€ä½Žå€¼.*2026EPSå¹³å‡å€¼.*2027EPSæœ€é«˜å€¼.*2027EPSæœ€ä½Žå€¼.*2027EPSå¹³å‡å€¼" 2>/dev/null; then
            echo "âœ… Detailed Data has v3.3.0 enhanced EPS breakdown format"
          fi
        fi
        
        if [ -f "data/processed/statistics.json" ] && [ -s "data/processed/statistics.json" ]; then
          has_statistics=true
          valid_processed_count=$((valid_processed_count + 1))
        fi
        
        echo "âœ… Enhanced validation results (v3.3.0):"
        echo "   ðŸ“„ Valid MD files: ${valid_md_count} (${factset_md_count} with FactSet, ${quality_md_count} with quality indicators, ${eps_breakdown_count} with EPS breakdown)"
        echo "   ðŸ“Š Valid CSV files: ${valid_csv_count} (${quality_csv_count} with quality content)"
        echo "   ðŸŽ¯ Valid processed files: ${valid_processed_count}"
        echo "   ðŸ’¼ Has portfolio summary: ${has_portfolio_summary}"
        echo "   ðŸ“ˆ Has detailed data: ${has_detailed_data}"
        echo "   ðŸ“Š Has statistics: ${has_statistics}"
        echo "   ðŸ†• Has v3.3.0 format: ${has_v330_format}"
        
        # Enhanced decision logic for v3.3.0
        should_commit="false"
        commit_reason=""
        commit_priority="low"
        
        # Premium quality commit conditions (v3.3.0) - Enhanced EPS data requirements
        if [ "$valid_md_count" -gt "30" ] && [ "$factset_md_count" -gt "10" ] && [ "$eps_breakdown_count" -gt "10" ] && [ "$has_v330_format" = "true" ]; then
          should_commit="true"
          commit_priority="premium"
          commit_reason="Premium v3.3.0 data: ${valid_md_count} MD (${factset_md_count} FactSet, ${eps_breakdown_count} EPS breakdown) + v3.3.0 format"
        
        # High quality commit conditions
        elif [ "$valid_md_count" -gt "20" ] && [ "$factset_md_count" -gt "5" ] && [ "$valid_csv_count" -gt "2" ] && [ "$has_portfolio_summary" = "true" ]; then
          should_commit="true"
          commit_priority="high"
          commit_reason="High quality v3.3.0 data: ${valid_md_count} MD (${factset_md_count} FactSet, ${eps_breakdown_count} EPS) + ${valid_csv_count} CSV + portfolio"
        
        # Medium quality commit conditions  
        elif [ "$valid_md_count" -gt "10" ] && [ "$valid_csv_count" -gt "1" ] && [ "$has_portfolio_summary" = "true" ]; then
          should_commit="true"
          commit_priority="medium"
          commit_reason="Medium quality v3.3.0 data: ${valid_md_count} MD + ${valid_csv_count} CSV + portfolio"
        
        # Low quality but acceptable commit conditions
        elif [ "$valid_md_count" -gt "5" ] || ([ "$has_portfolio_summary" = "true" ] && [ "$has_statistics" = "true" ]); then
          should_commit="true"
          commit_priority="low"
          commit_reason="Acceptable v3.3.0 data: ${valid_md_count} MD files, portfolio=${has_portfolio_summary}, stats=${has_statistics}"
        
        # First-time data collection (more lenient)
        elif [ "${{ needs.preflight.outputs.has_existing_data }}" = "false" ] && [ "$valid_md_count" -gt "0" ]; then
          should_commit="true"
          commit_priority="initial"
          commit_reason="Initial v3.3.0 data collection: ${valid_md_count} MD files"
        
        # Skip commit if data quality is poor
        else
          should_commit="false"
          commit_reason="Insufficient v3.3.0 data quality (${valid_md_count} MD, ${valid_csv_count} CSV, processed=${valid_processed_count})"
        fi
        
        echo "ðŸŽ¯ v3.3.0 Decision: should_commit=${should_commit}, priority=${commit_priority}"
        echo "ðŸ“ Reason: ${commit_reason}"
        
        if [ "$should_commit" = "true" ]; then
          echo "âœ… Proceeding with validated v3.3.0 commit..."
          
          # Create enhanced .gitattributes for v3.3.0
          cat > .gitattributes << 'EOF'
        # Handle large files with Git LFS
        *.pdf filter=lfs diff=lfs merge=lfs -text
        *.zip filter=lfs diff=lfs merge=lfs -text
        *.gz filter=lfs diff=lfs merge=lfs -text
        
        # Ensure proper handling of text files
        *.md text eol=lf
        *.csv text eol=lf
        *.json text eol=lf
        *.txt text eol=lf
        *.py text eol=lf
        *.yml text eol=lf
        EOF
          
          # Enhanced cleanup for v3.3.0
          echo "ðŸ§¹ Enhanced cleanup for v3.3.0..."
          
          # Remove files larger than 50MB
          find data/ -size +50M -delete 2>/dev/null || true
          
          # Remove empty files
          find data/ -empty -delete 2>/dev/null || true
          
          # Remove obvious error files
          find data/ -name "*.md" -exec grep -l "404 Not Found\|403 Forbidden\|500 Internal Server Error\|Rate limit exceeded" {} \; -delete 2>/dev/null || true
          
          # Enhanced selective file addition for v3.3.0
          echo "ðŸ“ Adding validated v3.3.0 files to git..."
          
          # Add validated MD files
          if [ "$valid_md_count" -gt "0" ]; then
            find data/md -name '*.md' -size +100c -exec sh -c '
              if grep -q -E "(factset|EPS|é ä¼°|ç›®æ¨™åƒ¹|åˆ†æžå¸«|è²¡å ±|ç‡Ÿæ”¶)" "$1" 2>/dev/null; then
                git add "$1"
              fi
            ' _ {} \;
            echo "âœ… Added ${valid_md_count} validated MD files"
          fi
          
          # Add validated CSV files
          if [ "$valid_csv_count" -gt "0" ]; then
            find data/csv -name '*.csv' -size +50c -exec sh -c '
              if head -1 "$1" | grep -q -E "(Title|Link|å…¬å¸|è‚¡ç¥¨|åç¨±)" 2>/dev/null && [ "$(wc -l < "$1")" -gt "1" ]; then
                git add "$1"
              fi
            ' _ {} \;
            echo "âœ… Added ${valid_csv_count} validated CSV files"
          fi
          
          # Add v3.3.0 processed data if valid
          if [ "$valid_processed_count" -gt "0" ]; then
            if [ "$has_portfolio_summary" = "true" ]; then
              git add data/processed/portfolio_summary.csv
              echo "âœ… Added portfolio_summary.csv (v3.3.0 format)"
            fi
            if [ "$has_detailed_data" = "true" ]; then
              git add data/processed/detailed_data.csv
              echo "âœ… Added detailed_data.csv (v3.3.0 enhanced EPS format)"
            fi
            if [ "$has_statistics" = "true" ]; then
              git add data/processed/statistics.json
              echo "âœ… Added statistics.json (v3.3.0 format)"
            fi
          fi
          
          # Add supporting files
          git add logs/ 2>/dev/null || true
          git add "è§€å¯Ÿåå–®.csv" 2>/dev/null || true
          git add .gitattributes 2>/dev/null || true
          
          # Create comprehensive v3.3.0 commit message
          execution_mode="${{ github.event.inputs.execution_mode || 'conservative' }}"
          priority_focus="${{ github.event.inputs.priority_focus || 'high_only' }}"
          
          # Add quality emoji based on priority
          quality_emoji="ðŸ“Š"
          case "$commit_priority" in
            "premium") quality_emoji="ðŸ†" ;;
            "high") quality_emoji="ðŸŽ‰" ;;
            "medium") quality_emoji="âœ…" ;;
            "low") quality_emoji="ðŸ“Š" ;;
            "initial") quality_emoji="ðŸ†•" ;;
          esac
          
          commit_message="${quality_emoji} FactSet Pipeline v3.3.0: ${commit_reason}

        ðŸ“Š Data Quality Summary:
        - MD Files: ${valid_md_count} total (${factset_md_count} FactSet, ${quality_md_count} quality indicators, ${eps_breakdown_count} EPS breakdown)
        - CSV Files: ${valid_csv_count} total (${quality_csv_count} quality content)
        - Processed: ${valid_processed_count} files (Portfolio=${has_portfolio_summary}, Details=${has_detailed_data}, Stats=${has_statistics})
        - v3.3.0 Format: ${has_v330_format}

        ðŸŽ¯ Execution Details:
        - Mode: ${execution_mode}
        - Focus: ${priority_focus}
        - Guideline: v3.3.0
        - Quality: ${commit_priority}
        - Rate Limited: ${{ needs.preflight.outputs.rate_limited }}

        ðŸ†• v3.3.0 Features:
        - Enhanced EPS breakdown (2025/2026/2027 with æœ€é«˜å€¼/æœ€ä½Žå€¼/å¹³å‡å€¼)
        - Quality scoring system with emoji status (ðŸŸ¢ðŸŸ¡ðŸŸ ðŸ”´)
        - Improved duplicate detection for MD files
        - Immediate rate limiting protection
        - è§€å¯Ÿåå–®.csv integration (116+ companies)
        - Advanced financial data extraction patterns"
          
          # Commit and push with comprehensive error handling
          if git diff --staged --quiet; then
            echo "â„¹ï¸ No validated changes to commit (files may have been filtered out)"
          else
            echo "âœ… Committing validated v3.3.0 data..."
            if git commit -m "$commit_message"; then
              echo "ðŸ“¤ Pushing to repository..."
              if git push; then
                echo "ðŸŽ‰ Validated v3.3.0 results committed and pushed successfully!"
                echo "ðŸ“ˆ Quality level: ${commit_priority}"
                echo "ðŸ“Š Data summary: ${valid_md_count} MD + ${valid_csv_count} CSV + ${valid_processed_count} processed files"
                echo "ðŸ†• v3.3.0 enhanced EPS breakdown: ${eps_breakdown_count} files"
              else
                echo "âš ï¸ Commit succeeded but push failed - changes are saved locally"
              fi
            else
              echo "âš ï¸ Commit failed - no changes were made to repository"
            fi
          fi
          
        else
          echo "âš ï¸ Skipping commit due to v3.3.0 data quality requirements"
          echo "ðŸ“ Reason: ${commit_reason}"
          echo "ðŸ’¡ Previous good data preserved in repository"
          
          # Create execution log for v3.3.0
          mkdir -p logs
          echo "$(date '+%Y-%m-%d %H:%M:%S'): SKIPPED COMMIT v3.3.0 - ${commit_reason}" >> logs/pipeline_execution.log
          echo "  - MD files: ${md_count} total, ${valid_md_count} valid (${factset_md_count} FactSet, ${quality_md_count} quality, ${eps_breakdown_count} EPS breakdown)" >> logs/pipeline_execution.log
          echo "  - CSV files: ${csv_count} total, ${valid_csv_count} valid (${quality_csv_count} quality)" >> logs/pipeline_execution.log
          echo "  - Processed files: ${processed_count} total, ${valid_processed_count} valid" >> logs/pipeline_execution.log
          echo "  - v3.3.0 format compliance: ${has_v330_format}" >> logs/pipeline_execution.log
          echo "  - Execution mode: ${{ github.event.inputs.execution_mode || 'conservative' }}" >> logs/pipeline_execution.log
          echo "  - Priority focus: ${{ github.event.inputs.priority_focus || 'high_only' }}" >> logs/pipeline_execution.log
          echo "  - Rate limited: ${{ needs.preflight.outputs.rate_limited }}" >> logs/pipeline_execution.log
          echo "  - Guideline version: 3.3.0" >> logs/pipeline_execution.log
          
          # Commit just the execution log
          git add logs/pipeline_execution.log 2>/dev/null || true
          git commit -m "ðŸ“‹ v3.3.0 Pipeline execution log: ${commit_reason}" 2>/dev/null || true
          git push 2>/dev/null || true
          
          echo "ðŸ“‹ v3.3.0 execution logged for debugging purposes"
        fi
    
    - name: ðŸ“ˆ Upload to Google Sheets (v3.3.0)
      if: env.GOOGLE_SHEET_ID != ''
      env:
        GOOGLE_SEARCH_API_KEY: ${{ secrets.GOOGLE_SEARCH_API_KEY }}
        GOOGLE_SEARCH_CSE_ID: ${{ secrets.GOOGLE_SEARCH_CSE_ID }}
        GOOGLE_SHEETS_CREDENTIALS: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}
        GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
      run: |
        echo "ðŸ“ˆ Uploading to Google Sheets (v3.3.0)..."
        
        # Check if we have v3.3.0 compliant data to upload
        if [ -f "data/processed/portfolio_summary.csv" ] || [ -f "data/processed/detailed_data.csv" ]; then
          echo "ðŸ“Š Found v3.3.0 processed data for upload"
          python sheets_uploader.py || echo "âš ï¸ Sheets upload had issues"
        else
          echo "â„¹ï¸ No v3.3.0 processed data found for upload"
        fi
        
        echo "âœ… Upload attempt completed (v3.3.0)"
    
    - name: ðŸ“Š Generate Status Report (v3.3.0)
      if: always()
      run: |
        echo "ðŸ“Š Generating v3.3.0 status report..."
        
        # Try to get pipeline status
        python factset_pipeline.py --status || echo "Status command not available"
        
        # Enhanced manual status check for v3.3.0
        echo "=== v3.3.0 Manual Status Check ==="
        echo "MD files: $(find data/md -name '*.md' 2>/dev/null | wc -l)"
        echo "CSV files: $(find data/csv -name '*.csv' 2>/dev/null | wc -l)"
        echo "Processed files: $(find data/processed -name '*' -type f 2>/dev/null | wc -l)"
        
        # Check for v3.3.0 format compliance
        if [ -f "data/processed/portfolio_summary.csv" ]; then
          echo "Portfolio Summary: âœ… Present"
          if head -1 "data/processed/portfolio_summary.csv" | grep -q "ä»£è™Ÿ.*åç¨±.*è‚¡ç¥¨ä»£è™Ÿ.*MDæœ€èˆŠæ—¥æœŸ.*MDæœ€æ–°æ—¥æœŸ.*MDè³‡æ–™ç­†æ•¸.*åˆ†æžå¸«æ•¸é‡.*ç›®æ¨™åƒ¹.*2025EPSå¹³å‡å€¼.*2026EPSå¹³å‡å€¼.*2027EPSå¹³å‡å€¼.*å“è³ªè©•åˆ†.*ç‹€æ…‹.*æ›´æ–°æ—¥æœŸ" 2>/dev/null; then
            echo "Portfolio Format: âœ… v3.3.0 compliant (Enhanced EPS breakdown)"
          else
            echo "Portfolio Format: âš ï¸ Format check failed"
          fi
        else
          echo "Portfolio Summary: âŒ Missing"
        fi
        
        # Check detailed data for v3.3.0 EPS breakdown
        if [ -f "data/processed/detailed_data.csv" ]; then
          echo "Detailed Data: âœ… Present"
          if head -1 "data/processed/detailed_data.csv" | grep -q "2025EPSæœ€é«˜å€¼.*2025EPSæœ€ä½Žå€¼.*2025EPSå¹³å‡å€¼" 2>/dev/null; then
            echo "Detailed Format: âœ… v3.3.0 compliant (Full EPS breakdown)"
          else
            echo "Detailed Format: âš ï¸ Missing v3.3.0 EPS breakdown"
          fi
        else
          echo "Detailed Data: âŒ Missing"
        fi
        
        # Check for rate limiting indicators in logs
        if grep -r "429\|rate limit\|too many requests" logs/ 2>/dev/null; then
          echo "ðŸš¨ Rate limiting detected in logs"
        else
          echo "âœ… No rate limiting indicators found"
        fi
        
        echo "âœ… v3.3.0 status report completed"
    
    - name: ðŸ“„ Collect Artifacts (v3.3.0)
      if: always()
      run: |
        echo "ðŸ“„ Collecting v3.3.0 artifacts..."
        
        # Create artifacts directory
        mkdir -p pipeline_artifacts_v330
        
        # Copy data with error handling
        [ -d "data" ] && cp -r data pipeline_artifacts_v330/ 2>/dev/null || echo "No data directory"
        [ -d "logs" ] && cp -r logs pipeline_artifacts_v330/ 2>/dev/null || echo "No logs directory"
        
        # Copy specific files
        [ -f "è§€å¯Ÿåå–®.csv" ] && cp "è§€å¯Ÿåå–®.csv" pipeline_artifacts_v330/ || echo "No watchlist file"
        
        # Create comprehensive v3.3.0 summary
        cat > pipeline_artifacts_v330/EXECUTION_SUMMARY_v330.md << EOF
        # FactSet Pipeline v3.3.0 Execution Summary
        
        **Date**: $(date)
        **Guideline Version**: 3.3.0
        **Execution Mode**: ${{ github.event.inputs.execution_mode || 'conservative' }}
        **Priority Focus**: ${{ github.event.inputs.priority_focus || 'high_only' }}
        **Trigger**: ${{ github.event_name }}
        **Rate Limited**: ${{ needs.preflight.outputs.rate_limited }}
        **Had Existing Data**: ${{ needs.preflight.outputs.has_existing_data }}
        
        ## v3.3.0 Enhanced Features
        - Enhanced EPS breakdown (2025/2026/2027 with æœ€é«˜å€¼/æœ€ä½Žå€¼/å¹³å‡å€¼)
        - Quality scoring system (1-4 scale)
        - Status emoji indicators (ðŸŸ¢ å®Œæ•´, ðŸŸ¡ è‰¯å¥½, ðŸŸ  éƒ¨åˆ†, ðŸ”´ ä¸è¶³)
        - Immediate rate limiting protection
        - è§€å¯Ÿåå–®.csv integration (116+ companies)
        - Advanced duplicate detection for MD files with same data
        - Improved financial data extraction patterns
        
        ## File Counts
        - MD files: $(find data/md -name '*.md' 2>/dev/null | wc -l)
        - CSV files: $(find data/csv -name '*.csv' 2>/dev/null | wc -l)
        - Processed files: $(find data/processed -name '*' -type f 2>/dev/null | wc -l)
        
        ## Data Quality (v3.3.0)
        $(find data/md -name '*.md' -exec grep -l "factset\|EPS\|é ä¼°\|ç›®æ¨™åƒ¹" {} \; 2>/dev/null | wc -l) MD files with financial content
        $(find data/md -name '*.md' -exec grep -l "2025\|2026\|2027" {} \; 2>/dev/null | wc -l) MD files with multi-year data
        $(find data/md -name '*.md' -exec grep -l "EPS.*2025.*2026.*2027\|æœ€é«˜å€¼.*æœ€ä½Žå€¼.*å¹³å‡å€¼" {} \; 2>/dev/null | wc -l) MD files with enhanced EPS breakdown
        
        ## Format Compliance (v3.3.0)
        $([ -f "data/processed/portfolio_summary.csv" ] && echo "âœ… Portfolio Summary present" || echo "âŒ Portfolio Summary missing")
        $([ -f "data/processed/detailed_data.csv" ] && echo "âœ… Detailed Data present" || echo "âŒ Detailed Data missing")
        $([ -f "data/processed/statistics.json" ] && echo "âœ… Statistics present" || echo "âŒ Statistics missing")
        
        ## EPS Breakdown Validation (v3.3.0)
        $([ -f "data/processed/portfolio_summary.csv" ] && head -1 "data/processed/portfolio_summary.csv" | grep -q "2025EPSå¹³å‡å€¼.*2026EPSå¹³å‡å€¼.*2027EPSå¹³å‡å€¼" && echo "âœ… Portfolio has v3.3.0 EPS format" || echo "âŒ Portfolio missing v3.3.0 EPS format")
        $([ -f "data/processed/detailed_data.csv" ] && head -1 "data/processed/detailed_data.csv" | grep -q "2025EPSæœ€é«˜å€¼.*2025EPSæœ€ä½Žå€¼.*2025EPSå¹³å‡å€¼" && echo "âœ… Detailed data has v3.3.0 EPS breakdown" || echo "âŒ Detailed data missing v3.3.0 EPS breakdown")
        
        ## Execution Status
        $(python factset_pipeline.py --status 2>/dev/null || echo "Pipeline status not available")
        
        ## Rate Limiting Check
        $(grep -r "429\|Too Many Requests\|rate limit" logs/ 2>/dev/null || echo "No rate limiting detected in logs")
        EOF
        
        echo "âœ… v3.3.0 artifacts collected"
    
    - name: ðŸ“¦ Upload Artifacts (v3.3.0)
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: factset-pipeline-v330-${{ github.run_number }}-${{ github.event.inputs.execution_mode || 'conservative' }}
        path: pipeline_artifacts_v330/
        retention-days: 14
    
    - name: ðŸ“Š Summary Report (v3.3.0)
      if: always()
      run: |
        echo "## ðŸ“Š FactSet Pipeline v3.3.0 Execution Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Guideline Version**: 3.3.0 (Enhanced EPS Breakdown)" >> $GITHUB_STEP_SUMMARY
        echo "**Execution Mode**: ${{ github.event.inputs.execution_mode || 'conservative' }}" >> $GITHUB_STEP_SUMMARY
        echo "**Priority Focus**: ${{ github.event.inputs.priority_focus || 'high_only' }}" >> $GITHUB_STEP_SUMMARY
        echo "**Rate Limited**: ${{ needs.preflight.outputs.rate_limited }}" >> $GITHUB_STEP_SUMMARY
        echo "**Recommended Action**: ${{ needs.preflight.outputs.recommended_action }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Add dashboard link if available
        if [ -n "${{ secrets.GOOGLE_SHEET_ID }}" ]; then
          echo "ðŸ“ˆ **Dashboard**: [View v3.3.0 Results](https://docs.google.com/spreadsheets/d/${{ secrets.GOOGLE_SHEET_ID }}/edit)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Enhanced file statistics for v3.3.0
        echo "### ðŸ“ Generated Files (v3.3.0)" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸ“ MD files: $(find data/md -name '*.md' 2>/dev/null | wc -l)" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸ“„ CSV files: $(find data/csv -name '*.csv' 2>/dev/null | wc -l)" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸ“Š Processed files: $(find data/processed -name '*' -type f 2>/dev/null | wc -l)" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸ’° Files with financial content: $(find data/md -name '*.md' -exec grep -l "factset\|EPS\|é ä¼°\|ç›®æ¨™åƒ¹" {} \; 2>/dev/null | wc -l)" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸ“ˆ Files with multi-year data: $(find data/md -name '*.md' -exec grep -l "2025\|2026\|2027" {} \; 2>/dev/null | wc -l)" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸ†• Files with v3.3.0 EPS breakdown: $(find data/md -name '*.md' -exec grep -l "EPS.*2025.*2026.*2027\|æœ€é«˜å€¼.*æœ€ä½Žå€¼.*å¹³å‡å€¼" {} \; 2>/dev/null | wc -l)" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # v3.3.0 format compliance
        echo "### ðŸŽ¯ v3.3.0 Format Compliance" >> $GITHUB_STEP_SUMMARY
        if [ -f "data/processed/portfolio_summary.csv" ]; then
          if head -1 "data/processed/portfolio_summary.csv" | grep -q "2025EPSå¹³å‡å€¼.*2026EPSå¹³å‡å€¼.*2027EPSå¹³å‡å€¼" 2>/dev/null; then
            echo "- âœ… **Portfolio Summary**: v3.3.0 Enhanced EPS format (14-column with 3-year breakdown)" >> $GITHUB_STEP_SUMMARY
          else
            echo "- âš ï¸ **Portfolio Summary**: Present but missing v3.3.0 EPS format" >> $GITHUB_STEP_SUMMARY
          fi
        else
          echo "- âŒ **Portfolio Summary**: Missing" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ -f "data/processed/detailed_data.csv" ]; then
          if head -1 "data/processed/detailed_data.csv" | grep -q "2025EPSæœ€é«˜å€¼.*2025EPSæœ€ä½Žå€¼.*2025EPSå¹³å‡å€¼" 2>/dev/null; then
            echo "- âœ… **Detailed Data**: v3.3.0 Full EPS breakdown (21-column with high/low/avg)" >> $GITHUB_STEP_SUMMARY
          else
            echo "- âš ï¸ **Detailed Data**: Present but missing v3.3.0 EPS breakdown" >> $GITHUB_STEP_SUMMARY
          fi
        else
          echo "- âŒ **Detailed Data**: Missing" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ -f "data/processed/statistics.json" ]; then
          echo "- âœ… **Statistics**: Present (v3.3.0 metrics)" >> $GITHUB_STEP_SUMMARY
        else
          echo "- âŒ **Statistics**: Missing" >> $GITHUB_STEP_SUMMARY
        fi
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Rate limiting status
        if grep -r "429\|Too Many Requests\|rate limit" logs/ 2>/dev/null >/dev/null; then
          echo "ðŸš¨ **Rate Limiting Detected**: Search requests were blocked" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ’¡ **Recommendation**: Wait 4-12 hours before next search attempt" >> $GITHUB_STEP_SUMMARY
        else
          echo "âœ… **Search Status**: No rate limiting detected" >> $GITHUB_STEP_SUMMARY
        fi
        
        # v3.3.0 features summary
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ðŸš€ v3.3.0 Enhanced Features Delivered" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸ“Š **Portfolio Summary**: 14-column format with 2025/2026/2027 EPSå¹³å‡å€¼" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸ“ˆ **Detailed Data**: 21-column format with full EPS breakdown (æœ€é«˜å€¼/æœ€ä½Žå€¼/å¹³å‡å€¼)" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸŽ¯ **Quality Scoring**: 1-4 scale with emoji status indicators (ðŸŸ¢ðŸŸ¡ðŸŸ ðŸ”´)" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸ›‘ **Rate Protection**: Immediate stop on 429 errors" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸ“‹ **Company Integration**: è§€å¯Ÿåå–®.csv (116+ companies)" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸ” **Duplicate Detection**: Advanced MD file deduplication for same data" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸ’° **Enhanced Extraction**: Improved financial data extraction patterns" >> $GITHUB_STEP_SUMMARY

  # Enhanced recovery job for v3.3.0
  recovery:
    runs-on: ubuntu-latest
    needs: [preflight, pipeline]
    if: failure() || needs.preflight.outputs.rate_limited == 'true'
    
    steps:
    - name: ðŸ“¥ Checkout Repository
      uses: actions/checkout@v4
    
    - name: ðŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: ðŸ“¦ Install Dependencies
      run: |
        pip install --upgrade pip
        pip install requests pandas gspread google-auth python-dotenv markdownify || echo "Some dependencies failed"
    
    - name: ðŸ”„ Recovery Strategy (v3.3.0)
      env:
        GOOGLE_SHEETS_CREDENTIALS: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}
        GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
      run: |
        echo "ðŸ”„ Implementing v3.3.0 recovery strategy..."
        
        # Try to salvage any existing data
        if [ -d "data/md" ] && [ "$(ls -A data/md 2>/dev/null)" ]; then
          echo "ðŸ“„ Found existing MD files, attempting v3.3.0 processing with enhanced EPS breakdown..."
          python data_processor.py --check-data || echo "Data check completed"
          python data_processor.py --force --parse-md || echo "MD processing attempted"
        fi
        
        # Try v3.3.0 compliant data processing
        python data_processor.py --force || echo "Basic processing attempted"
        
        # Try sheets upload if v3.3.0 data exists
        if [ -f "data/processed/portfolio_summary.csv" ]; then
          echo "ðŸ“ˆ Attempting v3.3.0 sheets upload with existing data..."
          python sheets_uploader.py || echo "Sheets upload attempted"
        fi
        
        echo "âœ… v3.3.0 recovery attempts completed"
    
    - name: ðŸ“‹ Recovery Report (v3.3.0)
      if: always()
      run: |
        echo "## ðŸ”„ v3.3.0 Recovery Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "The main v3.3.0 pipeline encountered issues and recovery was attempted." >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ needs.preflight.outputs.rate_limited }}" = "true" ]; then
          echo "ðŸš¨ **Issue**: Google Search rate limiting detected" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ’¡ **Action**: Processed existing data with v3.3.0 enhanced EPS compliance" >> $GITHUB_STEP_SUMMARY
          echo "â° **Recommendation**: Wait 4-12 hours before attempting new searches" >> $GITHUB_STEP_SUMMARY
        else
          echo "âš ï¸ **Issue**: v3.3.0 pipeline execution failed" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ”§ **Action**: Attempted data recovery and v3.3.0 processing" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### v3.3.0 Recovery Results" >> $GITHUB_STEP_SUMMARY
        echo "- MD files processed: $(find data/md -name '*.md' 2>/dev/null | wc -l)" >> $GITHUB_STEP_SUMMARY
        echo "- Data files generated: $(find data/processed -name '*' -type f 2>/dev/null | wc -l)" >> $GITHUB_STEP_SUMMARY
        if [ -f "data/processed/portfolio_summary.csv" ]; then
          if head -1 "data/processed/portfolio_summary.csv" | grep -q "2025EPSå¹³å‡å€¼.*2026EPSå¹³å‡å€¼.*2027EPSå¹³å‡å€¼" 2>/dev/null; then
            echo "- v3.3.0 compliance: âœ… Enhanced EPS format" >> $GITHUB_STEP_SUMMARY
          else
            echo "- v3.3.0 compliance: âš ï¸ Basic format only" >> $GITHUB_STEP_SUMMARY
          fi
        else
          echo "- v3.3.0 compliance: âŒ No portfolio data" >> $GITHUB_STEP_SUMMARY
        fi
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Next Steps" >> $GITHUB_STEP_SUMMARY
        echo "1. Review v3.3.0 execution logs in artifacts" >> $GITHUB_STEP_SUMMARY
        echo "2. Check for rate limiting indicators" >> $GITHUB_STEP_SUMMARY
        echo "3. Consider manual execution with v3.3.0 conservative settings" >> $GITHUB_STEP_SUMMARY
        echo "4. Verify API credentials if search consistently fails" >> $GITHUB_STEP_SUMMARY
        echo "5. Ensure all modules are v3.3.0 compliant with enhanced EPS breakdown" >> $GITHUB_STEP_SUMMARY
        echo "6. Validate duplicate detection for MD files with same data" >> $GITHUB_STEP_SUMMARY