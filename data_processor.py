"""
data_processor.py - Data Processing Module (Enhanced with Company Mapping Fix)

Version: 3.1.0
Date: 2025-06-21
Author: Google Search FactSet Pipeline - Modular Architecture
License: MIT

ENHANCEMENTS:
- âœ… Fixed company name extraction from search results
- âœ… Resolved dtype compatibility warnings
- âœ… Improved FactSet data parsing from MD files
- âœ… Enhanced company mapping with watchlist integration
- âœ… Better data validation and quality checks
- âœ… Comprehensive error handling and logging

Description:
    Data processing module for FactSet pipeline:
    - Parses markdown files for financial data
    - Consolidates CSV data from multiple sources
    - Extracts company information from search results
    - Maps generic names to real company names
    - Generates portfolio summaries and statistics
    - Validates data quality and completeness

Dependencies:
    - pandas
    - numpy
    - re (regex)
    - json
    - pathlib
"""

import os
import re
import json
import warnings
import traceback
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any

# Suppress specific pandas warnings
warnings.filterwarnings('ignore', category=FutureWarning, module='pandas')

# Import local modules
try:
    import utils
    import config as config_module
except ImportError as e:
    print(f"âš ï¸ Could not import local modules: {e}")

# Version Information
__version__ = "3.1.0"
__date__ = "2025-06-21"
__author__ = "Google Search FactSet Pipeline - Enhanced"

# ============================================================================
# CONFIGURATION AND CONSTANTS
# ============================================================================

# Financial data extraction patterns
FACTSET_PATTERNS = {
    'eps_current': [
        r'EPS[ï¼š:\s]*([0-9]+\.?[0-9]*)',
        r'æ¯è‚¡ç›ˆé¤˜[ï¼š:\s]*([0-9]+\.?[0-9]*)',
        r'é ä¼°.*?EPS[ï¼š:\s]*([0-9]+\.?[0-9]*)',
        r'Current.*?EPS[ï¼š:\s]*([0-9]+\.?[0-9]*)',
    ],
    'eps_previous': [
        r'å…ˆå‰.*?EPS[ï¼š:\s]*([0-9]+\.?[0-9]*)',
        r'Previous.*?EPS[ï¼š:\s]*([0-9]+\.?[0-9]*)',
        r'ä¸Šæ¬¡.*?é ä¼°[ï¼š:\s]*([0-9]+\.?[0-9]*)',
    ],
    'target_price': [
        r'ç›®æ¨™åƒ¹[ï¼š:\s]*([0-9]+\.?[0-9]*)',
        r'Target.*?Price[ï¼š:\s]*([0-9]+\.?[0-9]*)',
        r'price.*?target[ï¼š:\s]*([0-9]+\.?[0-9]*)',
        r'ç›®æ¨™[ï¼š:\s]*([0-9]+\.?[0-9]*)',
    ],
    'analyst_count': [
        r'åˆ†æå¸«[ï¼š:\s]*([0-9]+)',
        r'Analyst[s]?[ï¼š:\s]*([0-9]+)',
        r'([0-9]+).*?åˆ†æå¸«',
        r'([0-9]+).*?analyst',
    ],
    'eps_2025_high': [
        r'2025.*?æœ€é«˜[ï¼š:\s]*([0-9]+\.?[0-9]*)',
        r'2025.*?High[ï¼š:\s]*([0-9]+\.?[0-9]*)',
        r'2025.*?EPS.*?æœ€é«˜[ï¼š:\s]*([0-9]+\.?[0-9]*)',
    ],
    'eps_2025_low': [
        r'2025.*?æœ€ä½[ï¼š:\s]*([0-9]+\.?[0-9]*)',
        r'2025.*?Low[ï¼š:\s]*([0-9]+\.?[0-9]*)',
        r'2025.*?EPS.*?æœ€ä½[ï¼š:\s]*([0-9]+\.?[0-9]*)',
    ],
    'eps_2025_avg': [
        r'2025.*?å¹³å‡[ï¼š:\s]*([0-9]+\.?[0-9]*)',
        r'2025.*?Average[ï¼š:\s]*([0-9]+\.?[0-9]*)',
        r'2025.*?EPS.*?å¹³å‡[ï¼š:\s]*([0-9]+\.?[0-9]*)',
    ],
    'eps_2025_median': [
        r'2025.*?ä¸­ä½æ•¸[ï¼š:\s]*([0-9]+\.?[0-9]*)',
        r'2025.*?Median[ï¼š:\s]*([0-9]+\.?[0-9]*)',
        r'2025.*?EPS.*?ä¸­ä½æ•¸[ï¼š:\s]*([0-9]+\.?[0-9]*)',
    ],
}

# Numeric columns that should be converted to float
NUMERIC_COLUMNS = [
    'ç•¶å‰EPSé ä¼°', 'å…ˆå‰EPSé ä¼°', 'ç›®æ¨™åƒ¹', 'åˆ†æå¸«æ•¸é‡',
    '2025EPSæœ€é«˜å€¼', '2025EPSæœ€ä½å€¼', '2025EPSå¹³å‡å€¼', '2025EPSä¸­ä½æ•¸',
    '2026EPSæœ€é«˜å€¼', '2026EPSæœ€ä½å€¼', '2026EPSå¹³å‡å€¼', '2026EPSä¸­ä½æ•¸',
    '2027EPSæœ€é«˜å€¼', '2027EPSæœ€ä½å€¼', '2027EPSå¹³å‡å€¼', '2027EPSä¸­ä½æ•¸',
    '2025ç‡Ÿæ”¶æœ€é«˜å€¼', '2025ç‡Ÿæ”¶æœ€ä½å€¼', '2025ç‡Ÿæ”¶å¹³å‡å€¼', '2025ç‡Ÿæ”¶ä¸­ä½æ•¸',
    '2026ç‡Ÿæ”¶æœ€é«˜å€¼', '2026ç‡Ÿæ”¶æœ€ä½å€¼', '2026ç‡Ÿæ”¶å¹³å‡å€¼', '2026ç‡Ÿæ”¶ä¸­ä½æ•¸',
    '2027ç‡Ÿæ”¶æœ€é«˜å€¼', '2027ç‡Ÿæ”¶æœ€ä½å€¼', '2027ç‡Ÿæ”¶å¹³å‡å€¼', '2027ç‡Ÿæ”¶ä¸­ä½æ•¸',
]

# ============================================================================
# COMPANY NAME EXTRACTION AND MAPPING
# ============================================================================

def load_watchlist(watchlist_path: str = 'è§€å¯Ÿåå–®.csv') -> Optional[pd.DataFrame]:
    """
    Load the watchlist CSV file
    """
    try:
        if os.path.exists(watchlist_path):
            df = pd.read_csv(watchlist_path, encoding='utf-8')
            print(f"âœ… Loaded watchlist: {len(df)} companies")
            return df
        else:
            print(f"âš ï¸ Watchlist file not found: {watchlist_path}")
            return None
    except Exception as e:
        print(f"âŒ Error loading watchlist: {e}")
        return None

def extract_company_info_from_title(title: str, watchlist_df: Optional[pd.DataFrame] = None) -> Tuple[Optional[str], Optional[str]]:
    """
    Extract company name and stock code from search result title
    """
    if not title:
        return None, None
    
    company_name = None
    stock_code = None
    
    # Extract stock code patterns
    stock_patterns = [
        r'\((\d{4})\)',      # (2454)
        r'(\d{4})\.TW',      # 2454.TW
        r'(\d{4})-TW',       # 2454-TW
        r'(\d{4})\.TPE',     # 2454.TPE
    ]
    
    for pattern in stock_patterns:
        match = re.search(pattern, title)
        if match:
            stock_code = match.group(1)
            break
    
    # Extract known company names (extended list)
    company_patterns = [
        r'(å°ç©é›»|å°ç£ç©é«”é›»è·¯)',
        r'(è¯ç™¼ç§‘|MediaTek)',
        r'(å¯Œé‚¦é‡‘|å¯Œé‚¦é‡‘æ§|Fubon)',
        r'(é´»æµ·|Hon Hai|Foxconn)',
        r'(å°é”é›»|Delta)',
        r'(å…‰å¯¶ç§‘|Lite-On)',
        r'(è¯é›»|UMC)',
        r'(å»£é”|Quanta)',
        r'(è¯ç¢©|ASUS)',
        r'(å®ç¢|Acer)',
        r'(ç·¯å‰µ|Wistron)',
        r'(ä»å¯¶|Compal)',
        r'(å’Œç¢©|Pegatron)',
        r'(æ—¥æœˆå…‰|ASE)',
        r'(çŸ½å“|SPIL)',
        r'(æ¬£èˆˆ|Unimicron)',
        r'(å—äºç§‘|Nanya)',
        r'(ç¾¤è¯|Phison)',
        r'(ç‘æ˜±|Realtek)',
        r'(å‰èŒ‚)',
        r'(ä¸­è¯é›»|ä¸­è¯é›»ä¿¡)',
        r'(å°å¡‘|Formosa)',
        r'(åœ‹æ³°é‡‘|åœ‹æ³°é‡‘æ§)',
        r'(ç‰å±±é‡‘|ç‰å±±é‡‘æ§)',
        r'(çµ±ä¸€|çµ±ä¸€ä¼æ¥­)',
        r'(é•·æ¦®|Evergreen)',
        r'(é™½æ˜|Yang Ming)',
        r'(ä¸­é‹¼|China Steel)',
        r'(å°åŒ–|å°ç£åŒ–çº–)',
        r'(å—äº|Nan Ya)',
    ]
    
    for pattern in company_patterns:
        match = re.search(pattern, title, re.IGNORECASE)
        if match:
            # Take the first (usually Chinese) name
            company_name = match.group(1).split('|')[0]
            break
    
    # If we have stock code, try to match with watchlist
    if stock_code and watchlist_df is not None:
        try:
            # Convert stock code to string for matching
            watchlist_match = watchlist_df[watchlist_df['ä»£è™Ÿ'].astype(str) == str(stock_code)]
            if not watchlist_match.empty:
                company_name = watchlist_match.iloc[0]['åç¨±']
        except Exception as e:
            pass  # Continue with existing company_name
    
    # If we have company name but no stock code, try reverse lookup
    if company_name and not stock_code and watchlist_df is not None:
        try:
            watchlist_match = watchlist_df[watchlist_df['åç¨±'] == company_name]
            if not watchlist_match.empty:
                stock_code = str(watchlist_match.iloc[0]['ä»£è™Ÿ'])
        except Exception as e:
            pass  # Continue with existing stock_code
    
    # Fallback: manual mapping for common companies
    if stock_code and not company_name:
        stock_to_company = {
            '2330': 'å°ç©é›»',
            '2454': 'è¯ç™¼ç§‘',
            '2881': 'å¯Œé‚¦é‡‘',
            '2317': 'é´»æµ·',
            '2308': 'å°é”é›»',
            '2301': 'å…‰å¯¶ç§‘',
            '2303': 'è¯é›»',
            '2382': 'å»£é”',
            '2357': 'è¯ç¢©',
            '2353': 'å®ç¢',
            '3231': 'ç·¯å‰µ',
            '2324': 'ä»å¯¶',
            '4938': 'å’Œç¢©',
            '2311': 'æ—¥æœˆå…‰',
            '2325': 'çŸ½å“',
            '3037': 'æ¬£èˆˆ',
            '2408': 'å—äºç§‘',
            '8299': 'ç¾¤è¯',
            '2379': 'ç‘æ˜±',
            '1587': 'å‰èŒ‚',
            '2412': 'ä¸­è¯é›»',
            '6505': 'å°å¡‘åŒ–',
            '2882': 'åœ‹æ³°é‡‘',
            '2884': 'ç‰å±±é‡‘',
            '1216': 'çµ±ä¸€',
            '2603': 'é•·æ¦®',
            '2609': 'é™½æ˜',
            '2002': 'ä¸­é‹¼',
        }
        company_name = stock_to_company.get(stock_code, company_name)
    
    return company_name, stock_code

def fix_company_mapping(df: pd.DataFrame, watchlist_path: str = 'è§€å¯Ÿåå–®.csv') -> pd.DataFrame:
    """
    Fix the missing company names and stock codes in consolidated data
    """
    print("ğŸ”§ Fixing company name mapping...")
    
    # Load watchlist
    watchlist_df = load_watchlist(watchlist_path)
    
    # Track improvements
    fixed_names = 0
    fixed_codes = 0
    
    # Process each row
    for idx, row in df.iterrows():
        needs_fix = (pd.isna(row.get('å…¬å¸åç¨±')) or 
                    row.get('å…¬å¸åç¨±') == '' or 
                    row.get('å…¬å¸åç¨±') == 'null' or
                    str(row.get('å…¬å¸åç¨±')).startswith('Company_'))
        
        if needs_fix:
            title = row.get('Title', '')
            if title:
                # Extract company info from title
                company_name, stock_code = extract_company_info_from_title(title, watchlist_df)
                
                if company_name and company_name != row.get('å…¬å¸åç¨±'):
                    df.at[idx, 'å…¬å¸åç¨±'] = company_name
                    fixed_names += 1
                
                if stock_code and stock_code != row.get('è‚¡ç¥¨ä»£è™Ÿ'):
                    df.at[idx, 'è‚¡ç¥¨ä»£è™Ÿ'] = stock_code
                    fixed_codes += 1
    
    print(f"âœ… Company mapping fixed:")
    print(f"   ğŸ“ Company names: {fixed_names} fixed")
    print(f"   ğŸ”¢ Stock codes: {fixed_codes} fixed")
    
    return df

# ============================================================================
# MARKDOWN FILE PARSING
# ============================================================================

def extract_factset_data_from_content(content: str) -> Dict[str, Any]:
    """
    Extract FactSet financial data from markdown content
    """
    if not content:
        return {}
    
    extracted_data = {}
    
    # Clean content for better matching
    content = re.sub(r'\s+', ' ', content)  # Normalize whitespace
    content = content.replace('ï¼Œ', ',').replace('ï¼š', ':')  # Normalize punctuation
    
    # Extract data using patterns
    for data_type, patterns in FACTSET_PATTERNS.items():
        for pattern in patterns:
            try:
                matches = re.findall(pattern, content, re.IGNORECASE)
                if matches:
                    # Take the first valid match
                    value = matches[0]
                    try:
                        # Convert to float if possible
                        numeric_value = float(value)
                        extracted_data[data_type] = numeric_value
                        break  # Stop after first successful match
                    except ValueError:
                        continue  # Try next pattern
            except Exception as e:
                continue  # Skip problematic patterns
    
    # Map extracted data to column names
    column_mapping = {
        'eps_current': 'ç•¶å‰EPSé ä¼°',
        'eps_previous': 'å…ˆå‰EPSé ä¼°',
        'target_price': 'ç›®æ¨™åƒ¹',
        'analyst_count': 'åˆ†æå¸«æ•¸é‡',
        'eps_2025_high': '2025EPSæœ€é«˜å€¼',
        'eps_2025_low': '2025EPSæœ€ä½å€¼',
        'eps_2025_avg': '2025EPSå¹³å‡å€¼',
        'eps_2025_median': '2025EPSä¸­ä½æ•¸',
    }
    
    # Convert to final format
    final_data = {}
    for key, value in extracted_data.items():
        if key in column_mapping:
            final_data[column_mapping[key]] = value
    
    return final_data

def parse_markdown_files(csv_file: str, config: Dict) -> List[Dict]:
    """
    Parse markdown files associated with a CSV file
    """
    md_dir = config['output']['md_dir']
    csv_filename = os.path.splitext(os.path.basename(csv_file))[0]
    
    # Find corresponding MD files
    md_files = []
    if os.path.exists(md_dir):
        for md_file in os.listdir(md_dir):
            if md_file.endswith('.md'):
                # Check if this MD file might belong to this batch
                # Look for batch number or similar naming patterns
                if csv_filename.replace('FactSet_Batch_', '') in md_file or 'webpage' in md_file:
                    md_files.append(os.path.join(md_dir, md_file))
    
    if not md_files:
        print(f"ğŸ“„ No MD files to process for {os.path.basename(csv_file)}")
        return []
    
    print(f"ğŸ“ Processing {len(md_files)} MD files for {os.path.basename(csv_file)}")
    
    parsed_data = []
    errors = 0
    
    for md_file in md_files:
        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Extract financial data
            financial_data = extract_factset_data_from_content(content)
            
            if financial_data:
                # Add metadata
                financial_data.update({
                    'MD_File': os.path.basename(md_file),
                    'Source_File': os.path.basename(csv_file),
                    'Processed_Time': datetime.now().isoformat()
                })
                parsed_data.append(financial_data)
        
        except Exception as e:
            errors += 1
            if utils.is_debug_mode():
                print(f"   âŒ Error parsing {md_file}: {e}")
    
    print(f"ğŸ“Š Parsing complete: {len(parsed_data)} records parsed, {errors} errors")
    return parsed_data

def apply_md_data_to_csv(df: pd.DataFrame, parsed_md_data: List[Dict]) -> pd.DataFrame:
    """
    Apply parsed markdown data to CSV dataframe
    """
    if not parsed_md_data:
        return df
    
    updates_applied = 0
    
    for md_data in parsed_md_data:
        md_filename = md_data.get('MD_File', '')
        
        # Find corresponding rows in CSV by MD file name
        matching_rows = df[df['MD File'] == md_filename]
        
        if matching_rows.empty:
            # Try to match by partial filename or other criteria
            base_name = md_filename.replace('.md', '')
            matching_rows = df[df['MD File'].str.contains(base_name, na=False)]
        
        # Apply data to matching rows
        for idx in matching_rows.index:
            for column, value in md_data.items():
                if column in df.columns and column not in ['MD_File', 'Source_File', 'Processed_Time']:
                    if pd.isna(df.at[idx, column]) or df.at[idx, column] == '':
                        # Safe assignment with proper dtype handling
                        try:
                            if column in NUMERIC_COLUMNS:
                                df.at[idx, column] = pd.to_numeric(value, errors='coerce')
                            else:
                                df.at[idx, column] = str(value)
                            updates_applied += 1
                        except Exception as e:
                            if utils.is_debug_mode():
                                print(f"   âš ï¸ Error updating {column}: {e}")
    
    if updates_applied > 0:
        print(f"   âœ… Applied {updates_applied} updates from MD files")
    
    return df

# ============================================================================
# CSV PROCESSING AND CONSOLIDATION
# ============================================================================

def safe_numeric_conversion(value: Any, column_name: str) -> Any:
    """
    Safely convert values to appropriate types with proper error handling
    """
    if pd.isna(value) or value == '' or value == 'null':
        return np.nan
    
    if column_name in NUMERIC_COLUMNS:
        try:
            # Clean numeric string
            if isinstance(value, str):
                # Remove common non-numeric characters
                cleaned = re.sub(r'[^\d\.-]', '', value)
                if cleaned:
                    return float(cleaned)
                else:
                    return np.nan
            else:
                return float(value)
        except (ValueError, TypeError):
            return np.nan
    else:
        return str(value) if value is not None else ''

def process_single_csv(csv_file: str, config: Dict, parse_md: bool = True) -> pd.DataFrame:
    """
    Process a single CSV file with optional MD parsing
    """
    try:
        # Read CSV with proper encoding handling
        try:
            df = pd.read_csv(csv_file, encoding='utf-8')
        except UnicodeDecodeError:
            df = pd.read_csv(csv_file, encoding='utf-8-sig')
        except:
            df = pd.read_csv(csv_file, encoding='cp1252')
        
        if df.empty:
            return df
        
        # Parse markdown files if requested
        parsed_md_data = []
        if parse_md:
            parsed_md_data = parse_markdown_files(csv_file, config)
        
        # Apply MD data to CSV
        if parsed_md_data:
            df = apply_md_data_to_csv(df, parsed_md_data)
        
        # Fix company mapping
        df = fix_company_mapping(df)
        
        # Safe data type conversion
        for column in df.columns:
            if column in NUMERIC_COLUMNS:
                df[column] = df[column].apply(lambda x: safe_numeric_conversion(x, column))
            else:
                df[column] = df[column].astype(str).replace('nan', '')
        
        # Add processing metadata
        df['Source_File'] = os.path.basename(csv_file)
        df['Processed_Time'] = datetime.now().isoformat()
        
        return df
    
    except Exception as e:
        print(f"âŒ Error processing {csv_file}: {e}")
        if utils.is_debug_mode():
            traceback.print_exc()
        return pd.DataFrame()

def consolidate_csv_files(config: Dict, parse_md: bool = True) -> pd.DataFrame:
    """
    Consolidate all CSV files into a single dataframe
    """
    print("ğŸ“Š Consolidating CSV files...")
    
    csv_dir = config['output']['csv_dir']
    if not os.path.exists(csv_dir):
        print(f"âŒ CSV directory not found: {csv_dir}")
        return pd.DataFrame()
    
    # Find all CSV files
    csv_files = [f for f in os.listdir(csv_dir) if f.endswith('.csv')]
    if not csv_files:
        print(f"âŒ No CSV files found in {csv_dir}")
        return pd.DataFrame()
    
    print(f"ğŸ“„ Found {len(csv_files)} CSV files to process")
    
    # Process each CSV file
    all_dataframes = []
    for csv_file in sorted(csv_files):
        csv_path = os.path.join(csv_dir, csv_file)
        df = process_single_csv(csv_path, config, parse_md)
        
        if not df.empty:
            print(f"âœ… {csv_file}: {len(df)} records")
            all_dataframes.append(df)
        else:
            print(f"âš ï¸ {csv_file}: No data")
    
    if not all_dataframes:
        print("âŒ No data found in any CSV files")
        return pd.DataFrame()
    
    # Combine all dataframes
    consolidated_df = pd.concat(all_dataframes, ignore_index=True, sort=False)
    
    # Save consolidated data
    output_file = config['output']['consolidated_csv']
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    try:
        consolidated_df.to_csv(output_file, index=False, encoding='utf-8')
        print(f"âœ… Consolidated data saved: {output_file}")
        print(f"ğŸ“Š Total records: {len(consolidated_df)}")
    except Exception as e:
        print(f"âŒ Error saving consolidated data: {e}")
    
    return consolidated_df

# ============================================================================
# PORTFOLIO SUMMARY GENERATION
# ============================================================================

def generate_portfolio_summary(consolidated_df: pd.DataFrame, config: Dict) -> pd.DataFrame:
    """
    Generate portfolio summary from consolidated data
    """
    print("ğŸ“‹ Generating portfolio summary...")
    
    if consolidated_df.empty:
        print("âŒ No consolidated data available for summary")
        return pd.DataFrame()
    
    # Group by company and aggregate data
    summary_columns = [
        'å…¬å¸åç¨±', 'è‚¡ç¥¨ä»£è™Ÿ', 'ç•¶å‰EPSé ä¼°', 'ç›®æ¨™åƒ¹', 'åˆ†æå¸«æ•¸é‡', 
        'è³‡æ–™ä¾†æº', 'æ›´æ–°æ™‚é–“'
    ]
    
    # Prepare summary data
    summary_data = []
    
    # Get unique companies
    companies = consolidated_df['å…¬å¸åç¨±'].dropna().unique()
    companies = [c for c in companies if c != '' and c != 'null' and not str(c).startswith('Company_')]
    
    if not companies:
        print("âš ï¸ No valid company names found, using all records")
        # Fallback: use all records even if company names are missing
        for idx, row in consolidated_df.iterrows():
            summary_row = {}
            for col in summary_columns:
                if col in row:
                    summary_row[col] = row[col]
                else:
                    summary_row[col] = ''
            
            # Use source file as data source if missing
            if not summary_row.get('è³‡æ–™ä¾†æº'):
                summary_row['è³‡æ–™ä¾†æº'] = row.get('Source_File', '')
            
            summary_data.append(summary_row)
    else:
        print(f"ğŸ“Š Processing {len(companies)} unique companies")
        
        for company in companies:
            company_data = consolidated_df[consolidated_df['å…¬å¸åç¨±'] == company]
            
            if company_data.empty:
                continue
            
            # Aggregate company data
            summary_row = {
                'å…¬å¸åç¨±': company,
                'è‚¡ç¥¨ä»£è™Ÿ': '',
                'ç•¶å‰EPSé ä¼°': np.nan,
                'ç›®æ¨™åƒ¹': '',
                'åˆ†æå¸«æ•¸é‡': '',
                'è³‡æ–™ä¾†æº': '',
                'æ›´æ–°æ™‚é–“': datetime.now().isoformat()
            }
            
            # Get stock code (take first non-empty value)
            stock_codes = company_data['è‚¡ç¥¨ä»£è™Ÿ'].dropna()
            stock_codes = stock_codes[stock_codes != '']
            if not stock_codes.empty:
                summary_row['è‚¡ç¥¨ä»£è™Ÿ'] = stock_codes.iloc[0]
            
            # Get current EPS (take most recent or highest confidence value)
            eps_values = company_data['ç•¶å‰EPSé ä¼°'].dropna()
            if not eps_values.empty:
                # Take the most recent non-zero value
                non_zero_eps = eps_values[eps_values != 0]
                if not non_zero_eps.empty:
                    summary_row['ç•¶å‰EPSé ä¼°'] = non_zero_eps.iloc[-1]
                else:
                    summary_row['ç•¶å‰EPSé ä¼°'] = eps_values.iloc[-1]
            
            # Get target price
            target_prices = company_data['ç›®æ¨™åƒ¹'].dropna()
            target_prices = target_prices[target_prices != '']
            if not target_prices.empty:
                summary_row['ç›®æ¨™åƒ¹'] = target_prices.iloc[-1]
            
            # Get analyst count
            analyst_counts = company_data['åˆ†æå¸«æ•¸é‡'].dropna()
            analyst_counts = analyst_counts[analyst_counts != '']
            if not analyst_counts.empty:
                summary_row['åˆ†æå¸«æ•¸é‡'] = analyst_counts.iloc[-1]
            
            # Get data source
            sources = company_data['Source_File'].dropna()
            if not sources.empty:
                summary_row['è³‡æ–™ä¾†æº'] = sources.iloc[-1]
            
            summary_data.append(summary_row)
    
    # Create summary dataframe
    summary_df = pd.DataFrame(summary_data)
    
    # Clean and validate summary data
    for column in summary_df.columns:
        if column in NUMERIC_COLUMNS:
            summary_df[column] = summary_df[column].apply(
                lambda x: safe_numeric_conversion(x, column)
            )
        else:
            summary_df[column] = summary_df[column].astype(str).replace('nan', '')
    
    # Save portfolio summary
    output_file = config['output']['summary_csv']
    try:
        summary_df.to_csv(output_file, index=False, encoding='utf-8')
        print(f"âœ… Portfolio summary saved: {output_file}")
        print(f"ğŸ“Š Companies in summary: {len(summary_df)}")
        
        # Show data quality stats
        companies_with_names = len(summary_df[summary_df['å…¬å¸åç¨±'] != ''])
        companies_with_codes = len(summary_df[summary_df['è‚¡ç¥¨ä»£è™Ÿ'] != ''])
        companies_with_eps = len(summary_df[summary_df['ç•¶å‰EPSé ä¼°'].notna()])
        
        print(f"   ğŸ“ Companies with names: {companies_with_names}/{len(summary_df)}")
        print(f"   ğŸ”¢ Companies with codes: {companies_with_codes}/{len(summary_df)}")
        print(f"   ğŸ’° Companies with EPS: {companies_with_eps}/{len(summary_df)}")
        
    except Exception as e:
        print(f"âŒ Error saving portfolio summary: {e}")
        if utils.is_debug_mode():
            traceback.print_exc()
    
    return summary_df

# ============================================================================
# STATISTICS AND VALIDATION
# ============================================================================

def generate_statistics(consolidated_df: pd.DataFrame, summary_df: pd.DataFrame, 
                       config: Dict) -> Dict:
    """
    Generate comprehensive statistics about the data quality
    """
    stats = {
        'generated_at': datetime.now().isoformat(),
        'total_records': len(consolidated_df),
        'total_companies': len(summary_df),
        'data_quality': {},
        'search_results': {},
        'financial_data': {},
        'source_breakdown': {}
    }
    
    # Data quality statistics
    if not consolidated_df.empty:
        stats['data_quality'] = {
            'records_with_titles': len(consolidated_df[consolidated_df['Title'].notna()]),
            'records_with_links': len(consolidated_df[consolidated_df['Link'].notna()]),
            'records_with_company_names': len(consolidated_df[
                (consolidated_df['å…¬å¸åç¨±'].notna()) & 
                (consolidated_df['å…¬å¸åç¨±'] != '') &
                (~consolidated_df['å…¬å¸åç¨±'].str.startswith('Company_', na=False))
            ]),
            'records_with_stock_codes': len(consolidated_df[
                (consolidated_df['è‚¡ç¥¨ä»£è™Ÿ'].notna()) & 
                (consolidated_df['è‚¡ç¥¨ä»£è™Ÿ'] != '')
            ]),
        }
    
    # Financial data statistics
    if not summary_df.empty:
        stats['financial_data'] = {
            'companies_with_eps': len(summary_df[summary_df['ç•¶å‰EPSé ä¼°'].notna()]),
            'companies_with_target_price': len(summary_df[
                (summary_df['ç›®æ¨™åƒ¹'].notna()) & (summary_df['ç›®æ¨™åƒ¹'] != '')
            ]),
            'companies_with_analyst_count': len(summary_df[
                (summary_df['åˆ†æå¸«æ•¸é‡'].notna()) & (summary_df['åˆ†æå¸«æ•¸é‡'] != '')
            ]),
        }
    
    # Source breakdown
    if not consolidated_df.empty and 'Source_File' in consolidated_df.columns:
        source_counts = consolidated_df['Source_File'].value_counts().to_dict()
        stats['source_breakdown'] = source_counts
    
    # Calculate success rates
    if stats['total_records'] > 0:
        stats['success_rates'] = {
            'company_mapping_rate': (stats['data_quality'].get('records_with_company_names', 0) / 
                                   stats['total_records']) * 100,
            'financial_data_rate': (stats['financial_data'].get('companies_with_eps', 0) / 
                                  max(stats['total_companies'], 1)) * 100,
        }
    
    # Save statistics
    output_file = config['output']['stats_json']
    try:
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        print(f"âœ… Statistics saved: {output_file}")
    except Exception as e:
        print(f"âŒ Error saving statistics: {e}")
    
    return stats

def validate_data_quality(consolidated_df: pd.DataFrame, summary_df: pd.DataFrame) -> Dict:
    """
    Validate data quality and identify issues
    """
    issues = []
    warnings = []
    
    # Check for empty dataframes
    if consolidated_df.empty:
        issues.append("No consolidated data found")
    
    if summary_df.empty:
        issues.append("No portfolio summary data found")
        return {'issues': issues, 'warnings': warnings}
    
    # Check company name mapping
    companies_with_real_names = len(summary_df[
        (summary_df['å…¬å¸åç¨±'].notna()) & 
        (summary_df['å…¬å¸åç¨±'] != '') &
        (~summary_df['å…¬å¸åç¨±'].str.startswith('Company_', na=False))
    ])
    
    if companies_with_real_names == 0:
        issues.append("No companies found with real names (all showing as Company_X or null)")
    elif companies_with_real_names < len(summary_df) * 0.5:
        warnings.append(f"Only {companies_with_real_names}/{len(summary_df)} companies have real names")
    
    # Check financial data availability
    companies_with_eps = len(summary_df[summary_df['ç•¶å‰EPSé ä¼°'].notna()])
    if companies_with_eps == 0:
        issues.append("No companies found with EPS data")
    elif companies_with_eps < len(summary_df) * 0.2:
        warnings.append(f"Only {companies_with_eps}/{len(summary_df)} companies have EPS data")
    
    # Check stock codes
    companies_with_codes = len(summary_df[
        (summary_df['è‚¡ç¥¨ä»£è™Ÿ'].notna()) & (summary_df['è‚¡ç¥¨ä»£è™Ÿ'] != '')
    ])
    if companies_with_codes < len(summary_df) * 0.3:
        warnings.append(f"Only {companies_with_codes}/{len(summary_df)} companies have stock codes")
    
    return {'issues': issues, 'warnings': warnings}

# ============================================================================
# MAIN PROCESSING PIPELINE
# ============================================================================

def process_all_data(config_file: Optional[str] = None, force: bool = False, 
                    parse_md: bool = True) -> bool:
    """
    Process all data through the complete pipeline
    """
    print(f"ğŸ”§ Starting data processing...")
    
    # Load configuration
    if config_file:
        config = config_module.load_config(config_file)
    else:
        config = config_module.load_config()
    
    if not config:
        print("âŒ Could not load configuration")
        return False
    
    success_count = 0
    total_steps = 3
    
    try:
        # Step 1: Parse markdown files and consolidate CSV data
        print("\nğŸ§© Parsing markdown files...")
        consolidated_df = consolidate_csv_files(config, parse_md)
        
        if consolidated_df.empty:
            print("âŒ No data found to process")
            return False
        
        success_count += 1
        print("âœ… MD file parsing completed")
        
        # Step 2: Generate portfolio summary
        print("\nğŸ“‹ Generating portfolio summary...")
        summary_df = generate_portfolio_summary(consolidated_df, config)
        
        if summary_df.empty:
            print("âŒ Failed to generate portfolio summary")
            return False
        
        success_count += 1
        print("âœ… Portfolio summary completed")
        
        # Step 3: Generate statistics and validate
        print("\nğŸ“Š Generating statistics...")
        stats = generate_statistics(consolidated_df, summary_df, config)
        
        # Validate data quality
        validation_result = validate_data_quality(consolidated_df, summary_df)
        
        if validation_result['issues']:
            print("âš ï¸ Data quality issues found:")
            for issue in validation_result['issues']:
                print(f"   - {issue}")
        
        if validation_result['warnings']:
            print("âš ï¸ Data quality warnings:")
            for warning in validation_result['warnings']:
                print(f"   - {warning}")
        
        success_count += 1
        print("âœ… Statistics generation completed")
        
        # Final summary
        print(f"\n{'='*50}")
        print("ğŸ“Š DATA PROCESSING SUMMARY")
        print("="*50)
        print(f"âœ… Data processing complete: {success_count}/{total_steps} steps successful")
        print(f"ğŸ“„ Total records processed: {len(consolidated_df)}")
        print(f"ğŸ¢ Companies in portfolio: {len(summary_df)}")
        
        if stats.get('success_rates'):
            rates = stats['success_rates']
            print(f"ğŸ“ˆ Company mapping rate: {rates.get('company_mapping_rate', 0):.1f}%")
            print(f"ğŸ’° Financial data rate: {rates.get('financial_data_rate', 0):.1f}%")
        
        return True
        
    except Exception as e:
        print(f"âŒ Data processing failed: {e}")
        if utils.is_debug_mode():
            traceback.print_exc()
        return False

# ============================================================================
# MAIN ENTRY POINT
# ============================================================================

def main():
    """
    Main entry point for testing
    """
    print(f"ğŸ“Š Data Processor v{__version__}")
    
    success = process_all_data(force=True, parse_md=True)
    
    if success:
        print("âœ… Data processing completed successfully")
    else:
        print("âŒ Data processing failed")
    
    return success

if __name__ == "__main__":
    main()