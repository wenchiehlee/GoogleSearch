"""
GoogleSearch.py - Multi-Query PDF Search and Conversion Tool

Version: 2.0.5
Date: 2025-06-19
Author: Generated by ChatGPT based on instructions
License: MIT

Description:
    A comprehensive tool for searching, downloading, and converting PDF documents
    using Google Custom Search API and Microsoft Markitdown conversion.
    
    v2.0.5 updated FactSet data extraction with Traditional Chinese column headers.

Version History:
    v1.0.0 (Initial) - Basic Google search with single query support
    v1.1.0 - Added PDF download functionality
    v1.2.0 - Added Markitdown conversion support
    v2.0.0 - Multi-query support, enhanced error handling, improved structure
    v2.0.1 - Updated FactSet search to match 觀察名單, changed query to "FactSet"
    v2.0.2 - Fixed FactSet search to process all content types, not skip non-PDFs
    v2.0.3 - Added web page download and conversion to PDF/MD for FactSet matches, cross-platform compatibility
    v2.0.4 - Added content parsing for FactSet data extraction into structured CSV columns
    v2.0.5 - Updated FactSet data extraction with Traditional Chinese column headers

Dependencies:
    - requests
    - python-dotenv
    - markitdown (external tool)
    - wkhtmltopdf (optional, for web page to PDF conversion)
"""

import requests
import os
import csv
import re
from urllib.parse import urlparse, unquote, parse_qs
from dotenv import load_dotenv
import subprocess
import platform

# Version Information
__version__ = "2.0.5"
__date__ = "2025-06-19"
__author__ = "Generated by ChatGPT"

# Load environment variables
load_dotenv()

GOOGLE_SEARCH_API_KEY = os.getenv("GOOGLE_SEARCH_API_KEY")
GOOGLE_SEARCH_CSE_ID = os.getenv("GOOGLE_SEARCH_CSE_ID")
WATCH_LIST_URL = "https://raw.githubusercontent.com/wenchiehlee/GoPublic/refs/heads/main/%E8%A7%80%E5%AF%9F%E5%90%8D%E5%96%AE.csv"
os.environ["PYTHONIOENCODING"] = "utf-8"

def check_tool_availability():
    """Check availability of required tools on the current platform."""
    tools_status = {
        'markitdown': False,
        'wkhtmltopdf': False,
        'platform': platform.system()
    }
    
    # Check markitdown
    try:
        result = subprocess.run(['markitdown', '--help'], 
                              capture_output=True, text=True, timeout=10)
        tools_status['markitdown'] = result.returncode == 0
    except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError):
        tools_status['markitdown'] = False
    
    # Check wkhtmltopdf
    try:
        result = subprocess.run(['wkhtmltopdf', '--version'], 
                              capture_output=True, text=True, timeout=10)
        tools_status['wkhtmltopdf'] = result.returncode == 0
    except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError):
        tools_status['wkhtmltopdf'] = False
    
    return tools_status

def download_watch_list():
    """Download and parse the watch list CSV file."""
    try:
        print("Downloading 觀察名單 (watch list)...")
        response = requests.get(WATCH_LIST_URL)
        if response.status_code == 200:
            lines = response.text.strip().split('\n')
            watch_list = []
            
            for i, line in enumerate(lines):
                if i == 0 and ('公司' in line or 'Company' in line or '名稱' in line):
                    continue
                
                items = [item.strip().strip('"') for item in line.split(',')]
                watch_list.extend([item for item in items if item])
            
            print(f"Successfully loaded {len(watch_list)} items from watch list")
            return watch_list
        else:
            print(f"Failed to download watch list: {response.status_code}")
            return []
    except Exception as e:
        print(f"Error downloading watch list: {e}")
        return []

def filter_results_by_watch_list(results, watch_list):
    """Filter search results based on the watch list."""
    if not watch_list:
        return results
    
    filtered_results = []
    
    for result in results:
        title = result.get('title', '').lower()
        snippet = result.get('snippet', '').lower()
        link = result.get('link', '').lower()
        
        for watch_item in watch_list:
            watch_item_lower = watch_item.lower()
            if (watch_item_lower in title or 
                watch_item_lower in snippet or 
                watch_item_lower in link):
                filtered_results.append(result)
                break
    
    return filtered_results

def parse_factset_content(md_content):
    """Parse FactSet content from markdown text and extract structured data."""
    data = {
        '公司名稱': '',
        '股票代號': '',
        '分析師數量': '',
        '當前EPS預估': '',
        '先前EPS預估': '',
        '目標價': '',
        '2025EPS最高值': '',
        '2025EPS最低值': '',
        '2025EPS平均值': '',
        '2025EPS中位數': '',
        '2026EPS最高值': '',
        '2026EPS最低值': '',
        '2026EPS平均值': '',
        '2026EPS中位數': '',
        '2027EPS最高值': '',
        '2027EPS最低值': '',
        '2027EPS平均值': '',
        '2027EPS中位數': '',
        '2025營收最高值': '',
        '2025營收最低值': '',
        '2025營收平均值': '',
        '2025營收中位數': '',
        '2026營收最高值': '',
        '2026營收最低值': '',
        '2026營收平均值': '',
        '2026營收中位數': '',
        '2027營收最高值': '',
        '2027營收最低值': '',
        '2027營收平均值': '',
        '2027營收中位數': ''
    }
    
    try:
        # Extract company name and stock symbol with improved patterns
        
        # Pattern 1: Look for "對廣達([2382-TW])" format
        pattern1 = re.search(r'對([^(（]+)\([^)]*?([0-9]+-[A-Z]+)[^)]*?\)', md_content)
        if pattern1:
            company_name = pattern1.group(1).strip()
            if len(company_name) <= 4 and not re.search(r'[a-zA-Z0-9]', company_name):
                data['公司名稱'] = company_name
                data['股票代號'] = pattern1.group(2).strip()
                print(f"Pattern 1 matched: '{company_name}' ({pattern1.group(2)})")
        
        # Pattern 2: Look for company before stock code (FIXED - exclude punctuation)
        if not data['公司名稱']:
            # Updated regex to exclude punctuation marks like colons, commas, etc.
            pattern2 = re.search(r'[：:,，]([^\s\n\(\)（）：:,，]{2,4})\(\[?([0-9]+-[A-Z]+)\]?\)', md_content)
            if pattern2:
                company_name = pattern2.group(1).strip()
                if len(company_name) <= 4 and not re.search(r'[a-zA-Z0-9]', company_name):
                    data['公司名稱'] = company_name
                    data['股票代號'] = pattern2.group(2).strip()
                    print(f"Pattern 2 matched: '{company_name}' ({pattern2.group(2)})")
        
        # Pattern 3: Alternative pattern without punctuation prefix
        if not data['公司名稱']:
            pattern3 = re.search(r'([^\s\n\(\)（）：:,，]{2,4})\(\[?([0-9]+-[A-Z]+)\]?\)', md_content)
            if pattern3:
                company_name = pattern3.group(1).strip()
                # Additional filter to exclude common non-company words
                excluded_words = ['factset', 'Factset', 'FactSet', '最新', '調查', '速報']
                if (len(company_name) <= 4 and 
                    not re.search(r'[a-zA-Z0-9]', company_name) and 
                    company_name not in excluded_words):
                    data['公司名稱'] = company_name
                    data['股票代號'] = pattern3.group(2).strip()
                    print(f"Pattern 3 matched: '{company_name}' ({pattern3.group(2)})")
        
        # Pattern 4: Extract from markdown link format
        if not data['公司名稱']:
            pattern4 = re.search(r'([^\[\]]+)\(\[([0-9]+-[A-Z]+)-[A-Z]+\]', md_content)
            if pattern4:
                company_name = pattern4.group(1).strip()
                if len(company_name) <= 4 and not re.search(r'[a-zA-Z0-9]', company_name):
                    data['公司名稱'] = company_name
                    data['股票代號'] = pattern4.group(2) + '-TW'
                    print(f"Pattern 4 matched: '{company_name}' ({data['股票代號']})")
        
        # Extract analyst count
        analyst_pattern = re.search(r'共(\d+)位分析師', md_content)
        if analyst_pattern:
            data['分析師數量'] = analyst_pattern.group(1)
        
        # Extract EPS estimates
        eps_update_pattern = re.search(r'中位數由([\d.]+)元(?:上修|下修)至([\d.]+)元', md_content)
        if eps_update_pattern:
            data['先前EPS預估'] = eps_update_pattern.group(1)
            data['當前EPS預估'] = eps_update_pattern.group(2)
        else:
            eps_alt_pattern = re.search(r'(?:中位數為|EPS預估[：:]?)([\d.]+)元', md_content)
            if eps_alt_pattern:
                data['當前EPS預估'] = eps_alt_pattern.group(1)
        
        # Extract target price
        target_patterns = [
            r'預估目標價為([\d.]+)元',
            r'目標價為([\d.]+)元',
            r'目標價[：:]?([\d.]+)元'
        ]
        
        for pattern in target_patterns:
            target_match = re.search(pattern, md_content)
            if target_match:
                data['目標價'] = target_match.group(1)
                break
        
        # Extract EPS table data
        eps_section = re.search(r'\*市場預估EPS\*(.*?)(?:\*市場預估營收\*|\*歷史獲利表現\*|$)', md_content, re.DOTALL)
        if eps_section:
            eps_table = eps_section.group(1)
            lines = eps_table.split('\n')
            for line in lines:
                line = line.strip()
                if '最高值' in line:
                    values = re.findall(r'([\d.]+)', line)
                    if len(values) >= 1:
                        data['2025EPS最高值'] = values[0]
                        if len(values) >= 2:
                            data['2026EPS最高值'] = values[1]
                        if len(values) >= 3:
                            data['2027EPS最高值'] = values[2]
                elif '最低值' in line:
                    values = re.findall(r'([\d.]+)', line)
                    if len(values) >= 1:
                        data['2025EPS最低值'] = values[0]
                        if len(values) >= 2:
                            data['2026EPS最低值'] = values[1]
                        if len(values) >= 3:
                            data['2027EPS最低值'] = values[2]
                elif '平均值' in line:
                    values = re.findall(r'([\d.]+)', line)
                    if len(values) >= 1:
                        data['2025EPS平均值'] = values[0]
                        if len(values) >= 2:
                            data['2026EPS平均值'] = values[1]
                        if len(values) >= 3:
                            data['2027EPS平均值'] = values[2]
                elif '中位數' in line:
                    values = re.findall(r'([\d.]+)', line)
                    if len(values) >= 1:
                        data['2025EPS中位數'] = values[0]
                        if len(values) >= 2:
                            data['2026EPS中位數'] = values[1]
                        if len(values) >= 3:
                            data['2027EPS中位數'] = values[2]
        
        # Extract Revenue table data
        revenue_section = re.search(r'\*市場預估營收\*(.*?)(?:\*歷史獲利表現\*|$)', md_content, re.DOTALL)
        if revenue_section:
            revenue_table = revenue_section.group(1)
            lines = revenue_table.split('\n')
            for line in lines:
                line = line.strip()
                if '最高值' in line:
                    values = re.findall(r'([\d,]+)', line)
                    cleaned_values = [v.replace(',', '') for v in values if v.replace(',', '').isdigit()]
                    if len(cleaned_values) >= 1:
                        data['2025營收最高值'] = cleaned_values[0]
                        if len(cleaned_values) >= 2:
                            data['2026營收最高值'] = cleaned_values[1]
                        if len(cleaned_values) >= 3:
                            data['2027營收最高值'] = cleaned_values[2]
                elif '最低值' in line:
                    values = re.findall(r'([\d,]+)', line)
                    cleaned_values = [v.replace(',', '') for v in values if v.replace(',', '').isdigit()]
                    if len(cleaned_values) >= 1:
                        data['2025營收最低值'] = cleaned_values[0]
                        if len(cleaned_values) >= 2:
                            data['2026營收最低值'] = cleaned_values[1]
                        if len(cleaned_values) >= 3:
                            data['2027營收最低值'] = cleaned_values[2]
                elif '平均值' in line:
                    values = re.findall(r'([\d,]+)', line)
                    cleaned_values = [v.replace(',', '') for v in values if v.replace(',', '').isdigit()]
                    if len(cleaned_values) >= 1:
                        data['2025營收平均值'] = cleaned_values[0]
                        if len(cleaned_values) >= 2:
                            data['2026營收平均值'] = cleaned_values[1]
                        if len(cleaned_values) >= 3:
                            data['2027營收平均值'] = cleaned_values[2]
                elif '中位數' in line:
                    values = re.findall(r'([\d,]+)', line)
                    cleaned_values = [v.replace(',', '') for v in values if v.replace(',', '').isdigit()]
                    if len(cleaned_values) >= 1:
                        data['2025營收中位數'] = cleaned_values[0]
                        if len(cleaned_values) >= 2:
                            data['2026營收中位數'] = cleaned_values[1]
                        if len(cleaned_values) >= 3:
                            data['2027營收中位數'] = cleaned_values[2]
    
    except Exception as e:
        print(f"Error parsing FactSet content: {e}")
    
    # Debug output
    if data['公司名稱']:
        print(f"Successfully parsed: {data['公司名稱']} ({data['股票代號']})")
    else:
        print(f"Warning: Could not extract company name from content")
        print(f"Content preview: {md_content[:200]}...")
    
    return data

def is_markitdown_valid():
    """Check if the Markitdown command is available on the system."""
    tools_status = check_tool_availability()
    return tools_status['markitdown']

def google_search(api_key, cse_id, query, num_results=10, max_results=500, last_days=180, language="lang_zh-TW", country="countryTW"):
    """Perform a Google Custom Search using the Custom Search JSON API."""
    url = "https://www.googleapis.com/customsearch/v1"
    results = []
    date_limit = f"d{last_days}"

    for start in range(1, max_results + 1, num_results):
        params = {
            "key": api_key,
            "cx": cse_id,
            "q": query,
            "num": num_results,
            "start": start,
            "dateRestrict": date_limit,
            "sort": "date",
            "lr": language,
            "cr": country
        }
        response = requests.get(url, params=params)
        if response.status_code == 200:
            data = response.json()
            if "items" in data:
                results.extend(data["items"])
            else:
                print("No more results.")
                break
        else:
            print(f"Error: {response.status_code}, {response.text}")
            break
        if len(results) >= max_results:
            break
    return results[:max_results]

def save_results_to_csv(results, filename, include_parsing_columns=False):
    """Save search results to a CSV file."""
    base_fieldnames = ["Title", "Link", "Snippet", "File", "MD File"]
    
    parsing_fieldnames = [
        '公司名稱', '股票代號', '分析師數量', '當前EPS預估', '先前EPS預估', '目標價',
        '2025EPS最高值', '2025EPS最低值', '2025EPS平均值', '2025EPS中位數',
        '2026EPS最高值', '2026EPS最低值', '2026EPS平均值', '2026EPS中位數',
        '2027EPS最高值', '2027EPS最低值', '2027EPS平均值', '2027EPS中位數',
        '2025營收最高值', '2025營收最低值', '2025營收平均值', '2025營收中位數',
        '2026營收最高值', '2026營收最低值', '2026營收平均值', '2026營收中位數',
        '2027營收最高值', '2027營收最低值', '2027營收平均值', '2027營收中位數'
    ] if include_parsing_columns else []
    
    fieldnames = base_fieldnames + parsing_fieldnames
    
    with open(filename, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.DictWriter(file, fieldnames=fieldnames)
        writer.writeheader()
        for item in results:
            row_data = {
                "Title": item.get("title"),
                "Link": item.get("link"),
                "Snippet": item.get("snippet"),
                "File": "",
                "MD File": ""
            }
            
            if include_parsing_columns:
                for field in parsing_fieldnames:
                    row_data[field] = ""
            
            writer.writerow(row_data)

def download_web_page_as_pdf_and_md(url, counter, tools_status):
    """Download a web page and convert it to both PDF and Markdown formats."""
    try:
        os.makedirs("PDF", exist_ok=True)
        os.makedirs("MD", exist_ok=True)
        
        base_filename = f"webpage{counter}"
        pdf_filepath = os.path.join("PDF", f"{base_filename}.pdf")
        md_filepath = os.path.join("MD", f"{base_filename}.md")
        
        # Markdown conversion
        md_success = False
        
        if tools_status['markitdown']:
            try:
                print(f"Converting web page to Markdown using markitdown: {url}")
                with open(md_filepath, "w", encoding="utf-8") as md_file:
                    result = subprocess.run([
                        "markitdown", url
                    ], stdout=md_file, stderr=subprocess.PIPE, check=True, text=True, timeout=60)
                print(f"Successfully converted web page to Markdown: {md_filepath}")
                md_success = True
            except (subprocess.CalledProcessError, subprocess.TimeoutExpired) as e:
                print(f"Markitdown URL conversion failed, trying HTML download method")
                md_success = False
        
        if not md_success:
            try:
                print(f"Downloading HTML content for conversion: {url}")
                response = requests.get(url, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }, timeout=30)
                
                if response.status_code == 200:
                    temp_html = f"temp_webpage{counter}.html"
                    with open(temp_html, "w", encoding="utf-8") as f:
                        f.write(response.text)
                    
                    if tools_status['markitdown']:
                        try:
                            with open(md_filepath, "w", encoding="utf-8") as md_file:
                                subprocess.run([
                                    "markitdown", temp_html
                                ], stdout=md_file, check=True, timeout=30)
                            print(f"Successfully converted HTML to Markdown: {md_filepath}")
                            md_success = True
                        except (subprocess.CalledProcessError, subprocess.TimeoutExpired):
                            print("HTML to Markdown conversion failed, using fallback")
                            md_success = False
                    
                    if not md_success:
                        with open(md_filepath, "w", encoding="utf-8") as md_file:
                            md_file.write(f"# Web Page Content\n\nSource: {url}\n")
                            md_file.write(f"Retrieved: {counter}\n\n")
                            md_file.write("## HTML Content\n\n")
                            clean_content = response.text.replace('<', '\n<').replace('>', '>\n')
                            md_file.write(clean_content)
                        print(f"Saved HTML content as structured text: {md_filepath}")
                        md_success = True
                    
                    try:
                        os.remove(temp_html)
                    except:
                        pass
                else:
                    print(f"Failed to download web page content: {response.status_code}")
                    return "", ""
            except Exception as e:
                print(f"Error during HTML download: {e}")
                return "", ""
        
        # PDF conversion
        pdf_success = False
        
        if tools_status['wkhtmltopdf']:
            try:
                print(f"Converting web page to PDF using wkhtmltopdf: {url}")
                cmd = [
                    "wkhtmltopdf", 
                    "--page-size", "A4", 
                    "--margin-top", "0.75in", 
                    "--margin-right", "0.75in", 
                    "--margin-bottom", "0.75in", 
                    "--margin-left", "0.75in",
                    "--enable-local-file-access",
                    url, 
                    pdf_filepath
                ]
                
                result = subprocess.run(cmd, check=True, capture_output=True, timeout=60)
                print(f"Successfully converted web page to PDF: {pdf_filepath}")
                pdf_success = True
                
            except (subprocess.CalledProcessError, subprocess.TimeoutExpired, FileNotFoundError) as e:
                print(f"wkhtmltopdf conversion failed: {e}")
                pdf_success = False
        else:
            print("wkhtmltopdf not available for PDF conversion")
        
        if not pdf_success:
            try:
                fallback_filepath = pdf_filepath.replace('.pdf', '.txt')
                with open(fallback_filepath, "w", encoding="utf-8") as txt_file:
                    txt_file.write(f"Web Page Content\n")
                    txt_file.write(f"================\n\n")
                    txt_file.write(f"Source URL: {url}\n")
                    txt_file.write(f"Retrieved: Page {counter}\n")
                    txt_file.write(f"Platform: {tools_status['platform']}\n")
                    txt_file.write(f"wkhtmltopdf available: {tools_status['wkhtmltopdf']}\n\n")
                    txt_file.write("Note: PDF conversion not available. See corresponding MD file for full content.\n")
                    if md_success:
                        txt_file.write(f"Markdown file: {md_filepath}\n")
                
                pdf_filepath = fallback_filepath
                print(f"Created text placeholder: {pdf_filepath}")
                pdf_success = True
            except Exception as e:
                print(f"Failed to create fallback file: {e}")
                pdf_filepath = ""
        
        if md_success and pdf_success:
            return pdf_filepath, md_filepath
        elif md_success:
            return pdf_filepath, md_filepath
        else:
            return "", ""
        
    except Exception as e:
        print(f"Error downloading web page {url}: {e}")
        return "", ""

def download_files_from_links_and_convert(csv_filename, pdf_only=True, include_parsing=False):
    """Download files from links in the CSV and convert them."""
    tools_status = check_tool_availability()
    
    print(f"Platform: {tools_status['platform']}")
    print(f"Markitdown available: {tools_status['markitdown']}")
    print(f"wkhtmltopdf available: {tools_status['wkhtmltopdf']}")
    
    if not tools_status['markitdown']:
        print("Warning: Markitdown is not available. Some conversions may be limited.")
    
    if not pdf_only and not tools_status['wkhtmltopdf']:
        print("Warning: wkhtmltopdf is not available. Web pages will be saved as text files instead of PDFs.")

    with open(csv_filename, mode="r", newline="", encoding="utf-8") as file:
        reader = csv.DictReader(file)
        rows = list(reader)
        fieldnames = reader.fieldnames

    total_rows = len(rows)
    print(f"Total rows to process for {csv_filename}: {total_rows}")

    pdf_counter = 1
    web_counter = 1

    for index, row in enumerate(rows):
        link = row["Link"]
        if link:
            try:
                decoded_link = unquote(link)
                
                is_pdf_candidate = (decoded_link.lower().endswith(".pdf") or 
                                  "fileredirect.aspx" in decoded_link or
                                  "pdf" in decoded_link.lower())
                
                if is_pdf_candidate:
                    response = requests.get(link, stream=True, allow_redirects=True, timeout=30)
                    if response.status_code == 200:
                        content_type = response.headers.get('content-type', '').lower()
                        if 'pdf' in content_type or decoded_link.lower().endswith('.pdf'):
                            
                            final_url = response.url
                            parsed_url = urlparse(final_url)
                            if "fileredirect.aspx" in parsed_url.path:
                                query_params = parse_qs(parsed_url.query)
                                if "Path" in query_params:
                                    clean_path = unquote(query_params["Path"][0])
                                    filename = os.path.basename(clean_path)
                                    filepath = os.path.join("PDF", filename)
                                    os.makedirs("PDF", exist_ok=True)
                                else:
                                    filepath = os.path.join("PDF", f"pdf{pdf_counter}.pdf")
                                    pdf_counter += 1
                            else:
                                filename = os.path.basename(parsed_url.path) or f"pdf{pdf_counter}.pdf"
                                filepath = os.path.join("PDF", filename)
                                os.makedirs("PDF", exist_ok=True)
                                pdf_counter += 1

                            with open(filepath, "wb") as f:
                                for chunk in response.iter_content(chunk_size=8192):
                                    f.write(chunk)
                            row["File"] = filepath

                            md_filepath = os.path.join("MD", os.path.basename(filepath).replace(".pdf", ".md"))
                            os.makedirs("MD", exist_ok=True)
                            
                            if tools_status['markitdown']:
                                try:
                                    with open(md_filepath, "w", encoding="utf-8") as md_file:
                                        subprocess.run([
                                            "markitdown", filepath
                                        ], stdout=md_file, check=True, timeout=30)
                                    row["MD File"] = md_filepath
                                except (subprocess.CalledProcessError, subprocess.TimeoutExpired) as e:
                                    print(f"Error converting {filepath} to Markdown: {e}")
                                    row["MD File"] = ""
                            else:
                                print(f"Markitdown not available, skipping MD conversion for {filepath}")
                                row["MD File"] = ""
                        else:
                            if pdf_only:
                                print(f"Response is not a PDF file: {link}")
                                row["File"] = ""
                                row["MD File"] = ""
                            else:
                                print(f"Processing web page (PDF candidate that's not PDF): {link}")
                                web_pdf_path, web_md_path = download_web_page_as_pdf_and_md(link, web_counter, tools_status)
                                row["File"] = web_pdf_path
                                row["MD File"] = web_md_path
                                web_counter += 1
                    else:
                        print(f"Failed to download {link}: {response.status_code}")
                        row["File"] = ""
                        row["MD File"] = ""
                else:
                    if pdf_only:
                        print(f"Skipped non-PDF link: {link}")
                        row["File"] = ""
                        row["MD File"] = ""
                    else:
                        print(f"Processing web page: {link}")
                        web_pdf_path, web_md_path = download_web_page_as_pdf_and_md(link, web_counter, tools_status)
                        row["File"] = web_pdf_path
                        row["MD File"] = web_md_path
                        web_counter += 1
                        
                        if include_parsing and web_md_path and os.path.exists(web_md_path):
                            print(f"Parsing FactSet content from: {web_md_path}")
                            try:
                                with open(web_md_path, 'r', encoding='utf-8') as md_file:
                                    md_content = md_file.read()
                                    parsed_data = parse_factset_content(md_content)
                                    
                                    for key, value in parsed_data.items():
                                        if key in row:
                                            row[key] = value
                                        
                                    print(f"Extracted data: Company={parsed_data.get('公司名稱', 'N/A')}, "
                                          f"Symbol={parsed_data.get('股票代號', 'N/A')}, "
                                          f"EPS={parsed_data.get('當前EPS預估', 'N/A')}")
                            except Exception as e:
                                print(f"Error parsing FactSet content: {e}")
                                
            except Exception as e:
                print(f"Error processing {link}: {e}")
                row["File"] = ""
                row["MD File"] = ""

        print(f"Processed row {index + 1} of {total_rows} for {csv_filename}")

    parsing_fieldnames = [
        '公司名稱', '股票代號', '分析師數量', '當前EPS預估', '先前EPS預估', '目標價',
        '2025EPS最高值', '2025EPS最低值', '2025EPS平均值', '2025EPS中位數',
        '2026EPS最高值', '2026EPS最低值', '2026EPS平均值', '2026EPS中位數',
        '2027EPS最高值', '2027EPS最低值', '2027EPS平均值', '2027EPS中位數',
        '2025營收最高值', '2025營收最低值', '2025營收平均值', '2025營收中位數',
        '2026營收最高值', '2026營收最低值', '2026營收平均值', '2026營收中位數',
        '2027營收最高值', '2027營收最低值', '2027營收平均值', '2027營收中位數'
    ] if include_parsing else []
    
    final_fieldnames = list(fieldnames) + [field for field in parsing_fieldnames if field not in fieldnames]

    with open(csv_filename, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.DictWriter(file, fieldnames=final_fieldnames)
        writer.writeheader()
        writer.writerows(rows)
        
    if include_parsing:
        print(f"Added FactSet parsing columns to {csv_filename}")

def process_search_query(query, output_filename, description, use_watch_list=False, pdf_only=True, include_parsing=False):
    """Process a single search query."""
    print(f"\n{'='*60}")
    print(f"Starting {description}")
    print(f"Query: {query}")
    print(f"Output file: {output_filename}")
    if use_watch_list:
        print(f"Watch list filtering: ENABLED")
    if not pdf_only:
        print(f"Content type: ALL (PDFs and web pages)")
    else:
        print(f"Content type: PDF only")
    if include_parsing:
        print(f"FactSet content parsing: ENABLED")
    print(f"{'='*60}")
    
    search_results = google_search(GOOGLE_SEARCH_API_KEY, GOOGLE_SEARCH_CSE_ID, query, max_results=500, last_days=360)
    
    if search_results:
        print(f"Found {len(search_results)} initial results for {description}")
        
        if use_watch_list:
            watch_list = download_watch_list()
            if watch_list:
                search_results = filter_results_by_watch_list(search_results, watch_list)
                print(f"After watch list filtering: {len(search_results)} results remain")
        
        if search_results:
            save_results_to_csv(search_results, output_filename, include_parsing_columns=include_parsing)
            print(f"Saved search results to {output_filename}")
            download_files_from_links_and_convert(output_filename, pdf_only=pdf_only, include_parsing=include_parsing)
            print(f"Completed processing for {description}")
        else:
            print(f"No results remain after filtering for {description}")
    else:
        print(f"No search results found for {description}")

def main():
    """Main function to execute all search queries and processing."""
    print(f"GoogleSearch.py v{__version__} - Multi-Query PDF Search and Conversion Tool")
    print(f"Date: {__date__} | Author: {__author__}")
    print(f"Platform: {platform.system()}")
    print("=" * 80)
    
    tools_status = check_tool_availability()
    print("Tool Availability Check:")
    print(f"- Markitdown: {'✓ Available' if tools_status['markitdown'] else '✗ Not Available'}")
    print(f"- wkhtmltopdf: {'✓ Available' if tools_status['wkhtmltopdf'] else '✗ Not Available'}")
    print()
    
    if not tools_status['markitdown']:
        print("⚠️  Warning: Markitdown not found. Install with: pip install markitdown")
    
    if not tools_status['wkhtmltopdf']:
        print("⚠️  Warning: wkhtmltopdf not found.")
        if platform.system() == "Windows":
            print("   Download from: https://wkhtmltopdf.org/downloads.html")
        else:
            print("   Install with: sudo apt-get install wkhtmltopdf")
        print("   Web pages will be saved as text files instead of PDFs.")
    
    print()
    
    if not GOOGLE_SEARCH_API_KEY or not GOOGLE_SEARCH_CSE_ID:
        print("Error: Missing Google Search API credentials.")
        print("Please ensure GOOGLE_SEARCH_API_KEY and GOOGLE_SEARCH_CSE_ID are set in your .env file.")
        return
    
    search_queries = [
        {
            "query": "得標統計表 T004 -公司債 filetype:pdf",
            "filename": "GoogleResults.csv",
            "description": "Financial Documents Search (得標統計表 T004)",
            "use_watch_list": False,
            "pdf_only": True,
            "include_parsing": False
        },
        {
            "query": "FactSet 分析師",
            "filename": "FactSetResults.csv", 
            "description": "FactSet Documents Search",
            "use_watch_list": True,
            "pdf_only": False,
            "include_parsing": True
        }
    ]
    
    for search_config in search_queries:
        try:
            process_search_query(
                search_config["query"],
                search_config["filename"],
                search_config["description"],
                search_config.get("use_watch_list", False),
                search_config.get("pdf_only", True),
                search_config.get("include_parsing", False)
            )
        except Exception as e:
            print(f"Error processing {search_config['description']}: {e}")
            continue
    
    print(f"\n{'='*60}")
    print("All search queries completed!")
    print("Generated files:")
    for search_config in search_queries:
        if os.path.exists(search_config["filename"]):
            print(f"- {search_config['filename']}")
            if search_config.get("include_parsing", False):
                print(f"  └─ Includes parsed FactSet data columns (Traditional Chinese)")
    print("- PDF/ (downloaded PDF files and web pages)")
    print("- MD/ (converted Markdown files)")
    print(f"{'='*60}")
    print(f"GoogleSearch.py v{__version__} - Process completed successfully!")

if __name__ == "__main__":
    main()