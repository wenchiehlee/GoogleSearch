"""
GoogleSearch.py - Multi-Query PDF Search and Conversion Tool

Version: 2.0.8
Date: 2025-06-19
Author: Generated by ChatGPT based on instructions
License: MIT

Description:
    A comprehensive tool for searching, downloading, and converting PDF documents
    using Google Custom Search API and Microsoft Markitdown conversion.
    
    v2.0.8 adjusts FactSet search parameters for better results coverage.

Version History:
    v1.0.0 (Initial) - Basic Google search with single query support
    v1.1.0 - Added PDF download functionality
    v1.2.0 - Added Markitdown conversion support
    v2.0.0 - Multi-query support, enhanced error handling, improved structure
    v2.0.1 - Updated FactSet search to match è§€å¯Ÿåå–®, changed query to "FactSet"
    v2.0.2 - Fixed FactSet search to process all content types, not skip non-PDFs
    v2.0.3 - Added web page download and conversion to PDF/MD for FactSet matches, cross-platform compatibility
    v2.0.4 - Added content parsing for FactSet data extraction into structured CSV columns
    v2.0.5 - Updated FactSet data extraction with Traditional Chinese column headers, fixed company name extraction regex
    v2.0.6 - Added configurable search parameters and individual search enable/disable options for better control
    v2.0.7 - Added release date extraction (æ—¥æœŸ) for FactSet content with multiple date format support
    v2.0.8 - Adjusted FactSet search parameters (360 days, Traditional Chinese) for better results coverage

Dependencies:
    - requests
    - python-dotenv
    - markitdown (external tool)
    - wkhtmltopdf (optional, for web page to PDF conversion)
"""

import requests
import os
import csv
import re
from urllib.parse import urlparse, unquote, parse_qs
from dotenv import load_dotenv
import subprocess
import platform

# Version Information
__version__ = "2.0.8"
__date__ = "2025-06-19"
__author__ = "Generated by ChatGPT"

# Load environment variables
load_dotenv()

GOOGLE_SEARCH_API_KEY = os.getenv("GOOGLE_SEARCH_API_KEY")
GOOGLE_SEARCH_CSE_ID = os.getenv("GOOGLE_SEARCH_CSE_ID")
WATCH_LIST_URL = "https://raw.githubusercontent.com/wenchiehlee/GoPublic/refs/heads/main/%E8%A7%80%E5%AF%9F%E5%90%8D%E5%96%AE.csv"
os.environ["PYTHONIOENCODING"] = "utf-8"

def check_tool_availability():
    """Check availability of required tools on the current platform."""
    tools_status = {
        'markitdown': False,
        'wkhtmltopdf': False,
        'platform': platform.system()
    }
    
    # Check markitdown
    try:
        result = subprocess.run(['markitdown', '--help'], 
                              capture_output=True, text=True, timeout=10)
        tools_status['markitdown'] = result.returncode == 0
    except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError):
        tools_status['markitdown'] = False
    
    # Check wkhtmltopdf
    try:
        result = subprocess.run(['wkhtmltopdf', '--version'], 
                              capture_output=True, text=True, timeout=10)
        tools_status['wkhtmltopdf'] = result.returncode == 0
    except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError):
        tools_status['wkhtmltopdf'] = False
    
    return tools_status

def download_watch_list():
    """Download and parse the watch list CSV file."""
    try:
        print("Downloading è§€å¯Ÿåå–® (watch list)...")
        response = requests.get(WATCH_LIST_URL)
        if response.status_code == 200:
            lines = response.text.strip().split('\n')
            watch_list = []
            
            for i, line in enumerate(lines):
                if i == 0 and ('å…¬å¸' in line or 'Company' in line or 'åç¨±' in line):
                    continue
                
                items = [item.strip().strip('"') for item in line.split(',')]
                watch_list.extend([item for item in items if item])
            
            print(f"Successfully loaded {len(watch_list)} items from watch list")
            return watch_list
        else:
            print(f"Failed to download watch list: {response.status_code}")
            return []
    except Exception as e:
        print(f"Error downloading watch list: {e}")
        return []

def filter_results_by_watch_list(results, watch_list):
    """Filter search results based on the watch list."""
    if not watch_list:
        return results
    
    filtered_results = []
    
    for result in results:
        title = result.get('title', '').lower()
        snippet = result.get('snippet', '').lower()
        link = result.get('link', '').lower()
        
        for watch_item in watch_list:
            watch_item_lower = watch_item.lower()
            if (watch_item_lower in title or 
                watch_item_lower in snippet or 
                watch_item_lower in link):
                filtered_results.append(result)
                break
    
    return filtered_results

def parse_factset_content(md_content):
    """Parse FactSet content from markdown text and extract structured data."""
    data = {
        'å…¬å¸åç¨±': '',
        'è‚¡ç¥¨ä»£è™Ÿ': '',
        'æ—¥æœŸ': '',
        'åˆ†æå¸«æ•¸é‡': '',
        'ç•¶å‰EPSé ä¼°': '',
        'å…ˆå‰EPSé ä¼°': '',
        'ç›®æ¨™åƒ¹': '',
        '2025EPSæœ€é«˜å€¼': '',
        '2025EPSæœ€ä½å€¼': '',
        '2025EPSå¹³å‡å€¼': '',
        '2025EPSä¸­ä½æ•¸': '',
        '2026EPSæœ€é«˜å€¼': '',
        '2026EPSæœ€ä½å€¼': '',
        '2026EPSå¹³å‡å€¼': '',
        '2026EPSä¸­ä½æ•¸': '',
        '2027EPSæœ€é«˜å€¼': '',
        '2027EPSæœ€ä½å€¼': '',
        '2027EPSå¹³å‡å€¼': '',
        '2027EPSä¸­ä½æ•¸': '',
        '2025ç‡Ÿæ”¶æœ€é«˜å€¼': '',
        '2025ç‡Ÿæ”¶æœ€ä½å€¼': '',
        '2025ç‡Ÿæ”¶å¹³å‡å€¼': '',
        '2025ç‡Ÿæ”¶ä¸­ä½æ•¸': '',
        '2026ç‡Ÿæ”¶æœ€é«˜å€¼': '',
        '2026ç‡Ÿæ”¶æœ€ä½å€¼': '',
        '2026ç‡Ÿæ”¶å¹³å‡å€¼': '',
        '2026ç‡Ÿæ”¶ä¸­ä½æ•¸': '',
        '2027ç‡Ÿæ”¶æœ€é«˜å€¼': '',
        '2027ç‡Ÿæ”¶æœ€ä½å€¼': '',
        '2027ç‡Ÿæ”¶å¹³å‡å€¼': '',
        '2027ç‡Ÿæ”¶ä¸­ä½æ•¸': ''
    }
    
    try:
        # Extract company name and stock symbol with improved patterns
        
        # Pattern 1: Look for "å°å»£é”([2382-TW])" format
        pattern1 = re.search(r'å°([^(ï¼ˆ]+)\([^)]*?([0-9]+-[A-Z]+)[^)]*?\)', md_content)
        if pattern1:
            company_name = pattern1.group(1).strip()
            if len(company_name) <= 4 and not re.search(r'[a-zA-Z0-9]', company_name):
                data['å…¬å¸åç¨±'] = company_name
                data['è‚¡ç¥¨ä»£è™Ÿ'] = pattern1.group(2).strip()
                print(f"Pattern 1 matched: '{company_name}' ({pattern1.group(2)})")
        
        # Pattern 2: Look for company before stock code (FIXED - exclude punctuation)
        if not data['å…¬å¸åç¨±']:
            # Updated regex to exclude punctuation marks like colons, commas, etc.
            pattern2 = re.search(r'[ï¼š:,ï¼Œ]([^\s\n\(\)ï¼ˆï¼‰ï¼š:,ï¼Œ]{2,4})\(\[?([0-9]+-[A-Z]+)\]?\)', md_content)
            if pattern2:
                company_name = pattern2.group(1).strip()
                if len(company_name) <= 4 and not re.search(r'[a-zA-Z0-9]', company_name):
                    data['å…¬å¸åç¨±'] = company_name
                    data['è‚¡ç¥¨ä»£è™Ÿ'] = pattern2.group(2).strip()
                    print(f"Pattern 2 matched: '{company_name}' ({pattern2.group(2)})")
        
        # Pattern 3: Alternative pattern without punctuation prefix
        if not data['å…¬å¸åç¨±']:
            pattern3 = re.search(r'([^\s\n\(\)ï¼ˆï¼‰ï¼š:,ï¼Œ]{2,4})\(\[?([0-9]+-[A-Z]+)\]?\)', md_content)
            if pattern3:
                company_name = pattern3.group(1).strip()
                # Additional filter to exclude common non-company words
                excluded_words = ['factset', 'Factset', 'FactSet', 'æœ€æ–°', 'èª¿æŸ¥', 'é€Ÿå ±']
                if (len(company_name) <= 4 and 
                    not re.search(r'[a-zA-Z0-9]', company_name) and 
                    company_name not in excluded_words):
                    data['å…¬å¸åç¨±'] = company_name
                    data['è‚¡ç¥¨ä»£è™Ÿ'] = pattern3.group(2).strip()
                    print(f"Pattern 3 matched: '{company_name}' ({pattern3.group(2)})")
        
        # Pattern 4: Extract from markdown link format
        if not data['å…¬å¸åç¨±']:
            pattern4 = re.search(r'([^\[\]]+)\(\[([0-9]+-[A-Z]+)-[A-Z]+\]', md_content)
            if pattern4:
                company_name = pattern4.group(1).strip()
                if len(company_name) <= 4 and not re.search(r'[a-zA-Z0-9]', company_name):
                    data['å…¬å¸åç¨±'] = company_name
                    data['è‚¡ç¥¨ä»£è™Ÿ'] = pattern4.group(2) + '-TW'
                    print(f"Pattern 4 matched: '{company_name}' ({data['è‚¡ç¥¨ä»£è™Ÿ']})")
        
        # Extract analyst count
        analyst_pattern = re.search(r'å…±(\d+)ä½åˆ†æå¸«', md_content)
        if analyst_pattern:
            data['åˆ†æå¸«æ•¸é‡'] = analyst_pattern.group(1)
        
        # Extract release date
        date_patterns = [
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',  # 2025å¹´6æœˆ18æ—¥ format
            r'(\d{4})-(\d{1,2})-(\d{1,2})',     # 2025-06-18 format
            r'(\d{4})/(\d{1,2})/(\d{1,2})',     # 2025/06/18 format
            r'(\d{1,2})/(\d{1,2})/(\d{4})',     # 06/18/2025 format
            r'(\d{4})\.(\d{1,2})\.(\d{1,2})',   # 2025.06.18 format
        ]
        
        for pattern in date_patterns:
            date_match = re.search(pattern, md_content)
            if date_match:
                if 'å¹´' in pattern:  # Chinese format: 2025å¹´6æœˆ18æ—¥
                    year, month, day = date_match.groups()
                    data['æ—¥æœŸ'] = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                elif pattern.endswith(r'(\d{4})'):  # MM/DD/YYYY format
                    month, day, year = date_match.groups()
                    data['æ—¥æœŸ'] = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                else:  # YYYY-MM-DD, YYYY/MM/DD, YYYY.MM.DD formats
                    year, month, day = date_match.groups()
                    data['æ—¥æœŸ'] = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                print(f"Date extracted: {data['æ—¥æœŸ']} using pattern: {pattern}")
                break
        
        # If no date found in content, try to extract from common article date patterns
        if not data['æ—¥æœŸ']:
            # Try to find date near common keywords
            date_context_patterns = [
                r'(?:ç™¼å¸ƒ|æ›´æ–°|æ™‚é–“|æ—¥æœŸ)[ï¼š:\s]*(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
                r'(?:ç™¼å¸ƒ|æ›´æ–°|æ™‚é–“|æ—¥æœŸ)[ï¼š:\s]*(\d{4})-(\d{1,2})-(\d{1,2})',
                r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥.*?(?:ä¸Šåˆ|ä¸‹åˆ|å‡Œæ™¨)',
                r'(\d{1,2})æœˆ(\d{1,2})æ—¥.*?(\d{4})',  # 6æœˆ18æ—¥ 2025 format
            ]
            
            for pattern in date_context_patterns:
                date_match = re.search(pattern, md_content)
                if date_match:
                    if pattern.endswith(r'(\d{4})'):  # MMæœˆDDæ—¥ YYYY format
                        month, day, year = date_match.groups()
                        data['æ—¥æœŸ'] = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                    else:  # YYYYå¹´MMæœˆDDæ—¥ format
                        year, month, day = date_match.groups()
                        data['æ—¥æœŸ'] = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                    print(f"Date extracted from context: {data['æ—¥æœŸ']}")
                    break
        
        # Extract EPS estimates
        eps_update_pattern = re.search(r'ä¸­ä½æ•¸ç”±([\d.]+)å…ƒ(?:ä¸Šä¿®|ä¸‹ä¿®)è‡³([\d.]+)å…ƒ', md_content)
        if eps_update_pattern:
            data['å…ˆå‰EPSé ä¼°'] = eps_update_pattern.group(1)
            data['ç•¶å‰EPSé ä¼°'] = eps_update_pattern.group(2)
        else:
            eps_alt_pattern = re.search(r'(?:ä¸­ä½æ•¸ç‚º|EPSé ä¼°[ï¼š:]?)([\d.]+)å…ƒ', md_content)
            if eps_alt_pattern:
                data['ç•¶å‰EPSé ä¼°'] = eps_alt_pattern.group(1)
        
        # Extract target price
        target_patterns = [
            r'é ä¼°ç›®æ¨™åƒ¹ç‚º([\d.]+)å…ƒ',
            r'ç›®æ¨™åƒ¹ç‚º([\d.]+)å…ƒ',
            r'ç›®æ¨™åƒ¹[ï¼š:]?([\d.]+)å…ƒ'
        ]
        
        for pattern in target_patterns:
            target_match = re.search(pattern, md_content)
            if target_match:
                data['ç›®æ¨™åƒ¹'] = target_match.group(1)
                break
        
        # Extract EPS table data
        eps_section = re.search(r'\*å¸‚å ´é ä¼°EPS\*(.*?)(?:\*å¸‚å ´é ä¼°ç‡Ÿæ”¶\*|\*æ­·å²ç²åˆ©è¡¨ç¾\*|$)', md_content, re.DOTALL)
        if eps_section:
            eps_table = eps_section.group(1)
            lines = eps_table.split('\n')
            for line in lines:
                line = line.strip()
                if 'æœ€é«˜å€¼' in line:
                    values = re.findall(r'([\d.]+)', line)
                    if len(values) >= 1:
                        data['2025EPSæœ€é«˜å€¼'] = values[0]
                        if len(values) >= 2:
                            data['2026EPSæœ€é«˜å€¼'] = values[1]
                        if len(values) >= 3:
                            data['2027EPSæœ€é«˜å€¼'] = values[2]
                elif 'æœ€ä½å€¼' in line:
                    values = re.findall(r'([\d.]+)', line)
                    if len(values) >= 1:
                        data['2025EPSæœ€ä½å€¼'] = values[0]
                        if len(values) >= 2:
                            data['2026EPSæœ€ä½å€¼'] = values[1]
                        if len(values) >= 3:
                            data['2027EPSæœ€ä½å€¼'] = values[2]
                elif 'å¹³å‡å€¼' in line:
                    values = re.findall(r'([\d.]+)', line)
                    if len(values) >= 1:
                        data['2025EPSå¹³å‡å€¼'] = values[0]
                        if len(values) >= 2:
                            data['2026EPSå¹³å‡å€¼'] = values[1]
                        if len(values) >= 3:
                            data['2027EPSå¹³å‡å€¼'] = values[2]
                elif 'ä¸­ä½æ•¸' in line:
                    values = re.findall(r'([\d.]+)', line)
                    if len(values) >= 1:
                        data['2025EPSä¸­ä½æ•¸'] = values[0]
                        if len(values) >= 2:
                            data['2026EPSä¸­ä½æ•¸'] = values[1]
                        if len(values) >= 3:
                            data['2027EPSä¸­ä½æ•¸'] = values[2]
        
        # Extract Revenue table data
        revenue_section = re.search(r'\*å¸‚å ´é ä¼°ç‡Ÿæ”¶\*(.*?)(?:\*æ­·å²ç²åˆ©è¡¨ç¾\*|$)', md_content, re.DOTALL)
        if revenue_section:
            revenue_table = revenue_section.group(1)
            lines = revenue_table.split('\n')
            for line in lines:
                line = line.strip()
                if 'æœ€é«˜å€¼' in line:
                    values = re.findall(r'([\d,]+)', line)
                    cleaned_values = [v.replace(',', '') for v in values if v.replace(',', '').isdigit()]
                    if len(cleaned_values) >= 1:
                        data['2025ç‡Ÿæ”¶æœ€é«˜å€¼'] = cleaned_values[0]
                        if len(cleaned_values) >= 2:
                            data['2026ç‡Ÿæ”¶æœ€é«˜å€¼'] = cleaned_values[1]
                        if len(cleaned_values) >= 3:
                            data['2027ç‡Ÿæ”¶æœ€é«˜å€¼'] = cleaned_values[2]
                elif 'æœ€ä½å€¼' in line:
                    values = re.findall(r'([\d,]+)', line)
                    cleaned_values = [v.replace(',', '') for v in values if v.replace(',', '').isdigit()]
                    if len(cleaned_values) >= 1:
                        data['2025ç‡Ÿæ”¶æœ€ä½å€¼'] = cleaned_values[0]
                        if len(cleaned_values) >= 2:
                            data['2026ç‡Ÿæ”¶æœ€ä½å€¼'] = cleaned_values[1]
                        if len(cleaned_values) >= 3:
                            data['2027ç‡Ÿæ”¶æœ€ä½å€¼'] = cleaned_values[2]
                elif 'å¹³å‡å€¼' in line:
                    values = re.findall(r'([\d,]+)', line)
                    cleaned_values = [v.replace(',', '') for v in values if v.replace(',', '').isdigit()]
                    if len(cleaned_values) >= 1:
                        data['2025ç‡Ÿæ”¶å¹³å‡å€¼'] = cleaned_values[0]
                        if len(cleaned_values) >= 2:
                            data['2026ç‡Ÿæ”¶å¹³å‡å€¼'] = cleaned_values[1]
                        if len(cleaned_values) >= 3:
                            data['2027ç‡Ÿæ”¶å¹³å‡å€¼'] = cleaned_values[2]
                elif 'ä¸­ä½æ•¸' in line:
                    values = re.findall(r'([\d,]+)', line)
                    cleaned_values = [v.replace(',', '') for v in values if v.replace(',', '').isdigit()]
                    if len(cleaned_values) >= 1:
                        data['2025ç‡Ÿæ”¶ä¸­ä½æ•¸'] = cleaned_values[0]
                        if len(cleaned_values) >= 2:
                            data['2026ç‡Ÿæ”¶ä¸­ä½æ•¸'] = cleaned_values[1]
                        if len(cleaned_values) >= 3:
                            data['2027ç‡Ÿæ”¶ä¸­ä½æ•¸'] = cleaned_values[2]
    
    except Exception as e:
        print(f"Error parsing FactSet content: {e}")
    
    # Debug output
    if data['å…¬å¸åç¨±']:
        print(f"Successfully parsed: {data['å…¬å¸åç¨±']} ({data['è‚¡ç¥¨ä»£è™Ÿ']})")
    else:
        print(f"Warning: Could not extract company name from content")
        print(f"Content preview: {md_content[:200]}...")
    
    return data

def is_markitdown_valid():
    """Check if the Markitdown command is available on the system."""
    tools_status = check_tool_availability()
    return tools_status['markitdown']

def google_search(api_key, cse_id, query, num_results=10, max_results=500, last_days=None, language=None, country=None):
    """Perform a Google Custom Search using the Custom Search JSON API."""
    url = "https://www.googleapis.com/customsearch/v1"
    results = []

    print(f"ğŸ” Search parameters: days={last_days}, lang={language}, country={country}")

    for start in range(1, max_results + 1, num_results):
        params = {
            "key": api_key,
            "cx": cse_id,
            "q": query,
            "num": num_results,
            "start": start,
        }
        
        # Only add restrictions if specified
        if last_days:
            params["dateRestrict"] = f"d{last_days}"
            params["sort"] = "date"
        else:
            params["sort"] = "relevance"
        
        if language:
            params["lr"] = language
            
        if country:
            params["cr"] = country
            
        response = requests.get(url, params=params)
        if response.status_code == 200:
            data = response.json()
            if "items" in data:
                results.extend(data["items"])
                print(f"Retrieved {len(data['items'])} results (total: {len(results)})")
            else:
                print("No more results.")
                break
        else:
            print(f"Error: {response.status_code}, {response.text}")
            break
        if len(results) >= max_results:
            break
    return results[:max_results]

def save_results_to_csv(results, filename, include_parsing_columns=False):
    """Save search results to a CSV file."""
    base_fieldnames = ["Title", "Link", "Snippet", "File", "MD File"]
    
    parsing_fieldnames = [
        'å…¬å¸åç¨±', 'è‚¡ç¥¨ä»£è™Ÿ', 'æ—¥æœŸ', 'åˆ†æå¸«æ•¸é‡', 'ç•¶å‰EPSé ä¼°', 'å…ˆå‰EPSé ä¼°', 'ç›®æ¨™åƒ¹',
        '2025EPSæœ€é«˜å€¼', '2025EPSæœ€ä½å€¼', '2025EPSå¹³å‡å€¼', '2025EPSä¸­ä½æ•¸',
        '2026EPSæœ€é«˜å€¼', '2026EPSæœ€ä½å€¼', '2026EPSå¹³å‡å€¼', '2026EPSä¸­ä½æ•¸',
        '2027EPSæœ€é«˜å€¼', '2027EPSæœ€ä½å€¼', '2027EPSå¹³å‡å€¼', '2027EPSä¸­ä½æ•¸',
        '2025ç‡Ÿæ”¶æœ€é«˜å€¼', '2025ç‡Ÿæ”¶æœ€ä½å€¼', '2025ç‡Ÿæ”¶å¹³å‡å€¼', '2025ç‡Ÿæ”¶ä¸­ä½æ•¸',
        '2026ç‡Ÿæ”¶æœ€é«˜å€¼', '2026ç‡Ÿæ”¶æœ€ä½å€¼', '2026ç‡Ÿæ”¶å¹³å‡å€¼', '2026ç‡Ÿæ”¶ä¸­ä½æ•¸',
        '2027ç‡Ÿæ”¶æœ€é«˜å€¼', '2027ç‡Ÿæ”¶æœ€ä½å€¼', '2027ç‡Ÿæ”¶å¹³å‡å€¼', '2027ç‡Ÿæ”¶ä¸­ä½æ•¸'
    ] if include_parsing_columns else []
    
    fieldnames = base_fieldnames + parsing_fieldnames
    
    with open(filename, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.DictWriter(file, fieldnames=fieldnames)
        writer.writeheader()
        for item in results:
            row_data = {
                "Title": item.get("title"),
                "Link": item.get("link"),
                "Snippet": item.get("snippet"),
                "File": "",
                "MD File": ""
            }
            
            if include_parsing_columns:
                for field in parsing_fieldnames:
                    row_data[field] = ""
            
            writer.writerow(row_data)

def download_web_page_as_pdf_and_md(url, counter, tools_status):
    """Download a web page and convert it to both PDF and Markdown formats."""
    try:
        os.makedirs("PDF", exist_ok=True)
        os.makedirs("MD", exist_ok=True)
        
        base_filename = f"webpage{counter}"
        pdf_filepath = os.path.join("PDF", f"{base_filename}.pdf")
        md_filepath = os.path.join("MD", f"{base_filename}.md")
        
        # Markdown conversion
        md_success = False
        
        if tools_status['markitdown']:
            try:
                print(f"Converting web page to Markdown using markitdown: {url}")
                with open(md_filepath, "w", encoding="utf-8") as md_file:
                    result = subprocess.run([
                        "markitdown", url
                    ], stdout=md_file, stderr=subprocess.PIPE, check=True, text=True, timeout=60)
                print(f"Successfully converted web page to Markdown: {md_filepath}")
                md_success = True
            except (subprocess.CalledProcessError, subprocess.TimeoutExpired) as e:
                print(f"Markitdown URL conversion failed, trying HTML download method")
                md_success = False
        
        if not md_success:
            try:
                print(f"Downloading HTML content for conversion: {url}")
                response = requests.get(url, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }, timeout=30)
                
                if response.status_code == 200:
                    temp_html = f"temp_webpage{counter}.html"
                    with open(temp_html, "w", encoding="utf-8") as f:
                        f.write(response.text)
                    
                    if tools_status['markitdown']:
                        try:
                            with open(md_filepath, "w", encoding="utf-8") as md_file:
                                subprocess.run([
                                    "markitdown", temp_html
                                ], stdout=md_file, check=True, timeout=30)
                            print(f"Successfully converted HTML to Markdown: {md_filepath}")
                            md_success = True
                        except (subprocess.CalledProcessError, subprocess.TimeoutExpired):
                            print("HTML to Markdown conversion failed, using fallback")
                            md_success = False
                    
                    if not md_success:
                        with open(md_filepath, "w", encoding="utf-8") as md_file:
                            md_file.write(f"# Web Page Content\n\nSource: {url}\n")
                            md_file.write(f"Retrieved: {counter}\n\n")
                            md_file.write("## HTML Content\n\n")
                            clean_content = response.text.replace('<', '\n<').replace('>', '>\n')
                            md_file.write(clean_content)
                        print(f"Saved HTML content as structured text: {md_filepath}")
                        md_success = True
                    
                    try:
                        os.remove(temp_html)
                    except:
                        pass
                else:
                    print(f"Failed to download web page content: {response.status_code}")
                    return "", ""
            except Exception as e:
                print(f"Error during HTML download: {e}")
                return "", ""
        
        # PDF conversion
        pdf_success = False
        
        if tools_status['wkhtmltopdf']:
            try:
                print(f"Converting web page to PDF using wkhtmltopdf: {url}")
                cmd = [
                    "wkhtmltopdf", 
                    "--page-size", "A4", 
                    "--margin-top", "0.75in", 
                    "--margin-right", "0.75in", 
                    "--margin-bottom", "0.75in", 
                    "--margin-left", "0.75in",
                    "--enable-local-file-access",
                    url, 
                    pdf_filepath
                ]
                
                result = subprocess.run(cmd, check=True, capture_output=True, timeout=60)
                print(f"Successfully converted web page to PDF: {pdf_filepath}")
                pdf_success = True
                
            except (subprocess.CalledProcessError, subprocess.TimeoutExpired, FileNotFoundError) as e:
                print(f"wkhtmltopdf conversion failed: {e}")
                pdf_success = False
        else:
            print("wkhtmltopdf not available for PDF conversion")
        
        if not pdf_success:
            try:
                fallback_filepath = pdf_filepath.replace('.pdf', '.txt')
                with open(fallback_filepath, "w", encoding="utf-8") as txt_file:
                    txt_file.write(f"Web Page Content\n")
                    txt_file.write(f"================\n\n")
                    txt_file.write(f"Source URL: {url}\n")
                    txt_file.write(f"Retrieved: Page {counter}\n")
                    txt_file.write(f"Platform: {tools_status['platform']}\n")
                    txt_file.write(f"wkhtmltopdf available: {tools_status['wkhtmltopdf']}\n\n")
                    txt_file.write("Note: PDF conversion not available. See corresponding MD file for full content.\n")
                    if md_success:
                        txt_file.write(f"Markdown file: {md_filepath}\n")
                
                pdf_filepath = fallback_filepath
                print(f"Created text placeholder: {pdf_filepath}")
                pdf_success = True
            except Exception as e:
                print(f"Failed to create fallback file: {e}")
                pdf_filepath = ""
        
        if md_success and pdf_success:
            return pdf_filepath, md_filepath
        elif md_success:
            return pdf_filepath, md_filepath
        else:
            return "", ""
        
    except Exception as e:
        print(f"Error downloading web page {url}: {e}")
        return "", ""

def download_files_from_links_and_convert(csv_filename, pdf_only=True, include_parsing=False):
    """Download files from links in the CSV and convert them."""
    tools_status = check_tool_availability()
    
    print(f"Platform: {tools_status['platform']}")
    print(f"Markitdown available: {tools_status['markitdown']}")
    print(f"wkhtmltopdf available: {tools_status['wkhtmltopdf']}")
    
    if not tools_status['markitdown']:
        print("Warning: Markitdown is not available. Some conversions may be limited.")
    
    if not pdf_only and not tools_status['wkhtmltopdf']:
        print("Warning: wkhtmltopdf is not available. Web pages will be saved as text files instead of PDFs.")

    with open(csv_filename, mode="r", newline="", encoding="utf-8") as file:
        reader = csv.DictReader(file)
        rows = list(reader)
        fieldnames = reader.fieldnames

    total_rows = len(rows)
    print(f"Total rows to process for {csv_filename}: {total_rows}")

    pdf_counter = 1
    web_counter = 1

    for index, row in enumerate(rows):
        link = row["Link"]
        if link:
            try:
                decoded_link = unquote(link)
                
                is_pdf_candidate = (decoded_link.lower().endswith(".pdf") or 
                                  "fileredirect.aspx" in decoded_link or
                                  "pdf" in decoded_link.lower())
                
                if is_pdf_candidate:
                    response = requests.get(link, stream=True, allow_redirects=True, timeout=30)
                    if response.status_code == 200:
                        content_type = response.headers.get('content-type', '').lower()
                        if 'pdf' in content_type or decoded_link.lower().endswith('.pdf'):
                            
                            final_url = response.url
                            parsed_url = urlparse(final_url)
                            if "fileredirect.aspx" in parsed_url.path:
                                query_params = parse_qs(parsed_url.query)
                                if "Path" in query_params:
                                    clean_path = unquote(query_params["Path"][0])
                                    filename = os.path.basename(clean_path)
                                    filepath = os.path.join("PDF", filename)
                                    os.makedirs("PDF", exist_ok=True)
                                else:
                                    filepath = os.path.join("PDF", f"pdf{pdf_counter}.pdf")
                                    pdf_counter += 1
                            else:
                                filename = os.path.basename(parsed_url.path) or f"pdf{pdf_counter}.pdf"
                                filepath = os.path.join("PDF", filename)
                                os.makedirs("PDF", exist_ok=True)
                                pdf_counter += 1

                            with open(filepath, "wb") as f:
                                for chunk in response.iter_content(chunk_size=8192):
                                    f.write(chunk)
                            row["File"] = filepath

                            md_filepath = os.path.join("MD", os.path.basename(filepath).replace(".pdf", ".md"))
                            os.makedirs("MD", exist_ok=True)
                            
                            if tools_status['markitdown']:
                                try:
                                    with open(md_filepath, "w", encoding="utf-8") as md_file:
                                        subprocess.run([
                                            "markitdown", filepath
                                        ], stdout=md_file, check=True, timeout=30)
                                    row["MD File"] = md_filepath
                                except (subprocess.CalledProcessError, subprocess.TimeoutExpired) as e:
                                    print(f"Error converting {filepath} to Markdown: {e}")
                                    row["MD File"] = ""
                            else:
                                print(f"Markitdown not available, skipping MD conversion for {filepath}")
                                row["MD File"] = ""
                        else:
                            if pdf_only:
                                print(f"Response is not a PDF file: {link}")
                                row["File"] = ""
                                row["MD File"] = ""
                            else:
                                print(f"Processing web page (PDF candidate that's not PDF): {link}")
                                web_pdf_path, web_md_path = download_web_page_as_pdf_and_md(link, web_counter, tools_status)
                                row["File"] = web_pdf_path
                                row["MD File"] = web_md_path
                                web_counter += 1
                    else:
                        print(f"Failed to download {link}: {response.status_code}")
                        row["File"] = ""
                        row["MD File"] = ""
                else:
                    if pdf_only:
                        print(f"Skipped non-PDF link: {link}")
                        row["File"] = ""
                        row["MD File"] = ""
                    else:
                        print(f"Processing web page: {link}")
                        web_pdf_path, web_md_path = download_web_page_as_pdf_and_md(link, web_counter, tools_status)
                        row["File"] = web_pdf_path
                        row["MD File"] = web_md_path
                        web_counter += 1
                        
                        if include_parsing and web_md_path and os.path.exists(web_md_path):
                            print(f"Parsing FactSet content from: {web_md_path}")
                            try:
                                with open(web_md_path, 'r', encoding='utf-8') as md_file:
                                    md_content = md_file.read()
                                    parsed_data = parse_factset_content(md_content)
                                    
                                    for key, value in parsed_data.items():
                                        if key in row:
                                            row[key] = value
                                        
                                    print(f"Extracted data: Company={parsed_data.get('å…¬å¸åç¨±', 'N/A')}, "
                                          f"Symbol={parsed_data.get('è‚¡ç¥¨ä»£è™Ÿ', 'N/A')}, "
                                          f"Date={parsed_data.get('æ—¥æœŸ', 'N/A')}, "
                                          f"EPS={parsed_data.get('ç•¶å‰EPSé ä¼°', 'N/A')}")
                            except Exception as e:
                                print(f"Error parsing FactSet content: {e}")
                                
            except Exception as e:
                print(f"Error processing {link}: {e}")
                row["File"] = ""
                row["MD File"] = ""

        print(f"Processed row {index + 1} of {total_rows} for {csv_filename}")

    parsing_fieldnames = [
        'å…¬å¸åç¨±', 'è‚¡ç¥¨ä»£è™Ÿ', 'æ—¥æœŸ', 'åˆ†æå¸«æ•¸é‡', 'ç•¶å‰EPSé ä¼°', 'å…ˆå‰EPSé ä¼°', 'ç›®æ¨™åƒ¹',
        '2025EPSæœ€é«˜å€¼', '2025EPSæœ€ä½å€¼', '2025EPSå¹³å‡å€¼', '2025EPSä¸­ä½æ•¸',
        '2026EPSæœ€é«˜å€¼', '2026EPSæœ€ä½å€¼', '2026EPSå¹³å‡å€¼', '2026EPSä¸­ä½æ•¸',
        '2027EPSæœ€é«˜å€¼', '2027EPSæœ€ä½å€¼', '2027EPSå¹³å‡å€¼', '2027EPSä¸­ä½æ•¸',
        '2025ç‡Ÿæ”¶æœ€é«˜å€¼', '2025ç‡Ÿæ”¶æœ€ä½å€¼', '2025ç‡Ÿæ”¶å¹³å‡å€¼', '2025ç‡Ÿæ”¶ä¸­ä½æ•¸',
        '2026ç‡Ÿæ”¶æœ€é«˜å€¼', '2026ç‡Ÿæ”¶æœ€ä½å€¼', '2026ç‡Ÿæ”¶å¹³å‡å€¼', '2026ç‡Ÿæ”¶ä¸­ä½æ•¸',
        '2027ç‡Ÿæ”¶æœ€é«˜å€¼', '2027ç‡Ÿæ”¶æœ€ä½å€¼', '2027ç‡Ÿæ”¶å¹³å‡å€¼', '2027ç‡Ÿæ”¶ä¸­ä½æ•¸'
    ] if include_parsing else []
    
    final_fieldnames = list(fieldnames) + [field for field in parsing_fieldnames if field not in fieldnames]

    with open(csv_filename, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.DictWriter(file, fieldnames=final_fieldnames)
        writer.writeheader()
        writer.writerows(rows)
        
    if include_parsing:
        print(f"Added FactSet parsing columns to {csv_filename}")

def process_search_query(query, output_filename, description, use_watch_list=False, pdf_only=True, include_parsing=False, search_params=None):
    """Process a single search query."""
    print(f"\n{'='*60}")
    print(f"Starting {description}")
    print(f"Query: {query}")
    print(f"Output file: {output_filename}")
    if use_watch_list:
        print(f"Watch list filtering: ENABLED")
    if not pdf_only:
        print(f"Content type: ALL (PDFs and web pages)")
    else:
        print(f"Content type: PDF only")
    if include_parsing:
        print(f"FactSet content parsing: ENABLED")
    if search_params:
        print(f"Search restrictions: {search_params}")
    print(f"{'='*60}")
    
    # Set default search parameters
    default_params = {
        "max_results": 500,
        "last_days": 360,
        "language": "lang_zh-TW",
        "country": "countryTW"
    }
    
    # Override with custom search_params if provided
    if search_params:
        default_params.update(search_params)
    
    search_results = google_search(
        GOOGLE_SEARCH_API_KEY, 
        GOOGLE_SEARCH_CSE_ID, 
        query, 
        **default_params
    )
    
    if search_results:
        print(f"Found {len(search_results)} initial results for {description}")
        
        if use_watch_list:
            watch_list = download_watch_list()
            if watch_list:
                search_results = filter_results_by_watch_list(search_results, watch_list)
                print(f"After watch list filtering: {len(search_results)} results remain")
        
        if search_results:
            save_results_to_csv(search_results, output_filename, include_parsing_columns=include_parsing)
            print(f"Saved search results to {output_filename}")
            download_files_from_links_and_convert(output_filename, pdf_only=pdf_only, include_parsing=include_parsing)
            print(f"Completed processing for {description}")
        else:
            print(f"No results remain after filtering for {description}")
    else:
        print(f"No search results found for {description}")

def main():
    """Main function to execute all search queries and processing."""
    print(f"GoogleSearch.py v{__version__} - Multi-Query PDF Search and Conversion Tool")
    print(f"Date: {__date__} | Author: {__author__}")
    print(f"Platform: {platform.system()}")
    print("=" * 80)
    
    # ============================================================================
    # QUICK CONFIGURATION - Edit these settings to enable/disable searches
    # ============================================================================
    ENABLE_FINANCIAL_SEARCH = False    # Set to True to enable å¾—æ¨™çµ±è¨ˆè¡¨ search
    ENABLE_FACTSET_SEARCH = True       # Set to False to disable FactSet search
    # ============================================================================
    
    tools_status = check_tool_availability()
    print("Tool Availability Check:")
    print(f"- Markitdown: {'âœ“ Available' if tools_status['markitdown'] else 'âœ— Not Available'}")
    print(f"- wkhtmltopdf: {'âœ“ Available' if tools_status['wkhtmltopdf'] else 'âœ— Not Available'}")
    print()
    
    if not tools_status['markitdown']:
        print("âš ï¸  Warning: Markitdown not found. Install with: pip install markitdown")
    
    if not tools_status['wkhtmltopdf']:
        print("âš ï¸  Warning: wkhtmltopdf not found.")
        if platform.system() == "Windows":
            print("   Download from: https://wkhtmltopdf.org/downloads.html")
        else:
            print("   Install with: sudo apt-get install wkhtmltopdf")
        print("   Web pages will be saved as text files instead of PDFs.")
    
    print()
    
    if not GOOGLE_SEARCH_API_KEY or not GOOGLE_SEARCH_CSE_ID:
        print("Error: Missing Google Search API credentials.")
        print("Please ensure GOOGLE_SEARCH_API_KEY and GOOGLE_SEARCH_CSE_ID are set in your .env file.")
        return
    
    search_queries = [
        {
            "query": "å¾—æ¨™çµ±è¨ˆè¡¨ T004 -å…¬å¸å‚µ filetype:pdf",
            "filename": "GoogleResults.csv",
            "description": "Financial Documents Search (å¾—æ¨™çµ±è¨ˆè¡¨ T004)",
            "use_watch_list": False,
            "pdf_only": True,
            "include_parsing": False,
            "enabled": ENABLE_FINANCIAL_SEARCH,
            "search_params": {
                "last_days": 360,
                "language": "lang_zh-TW",
                "country": "countryTW"
            }
        },
        {
            "query": "FactSet",
            "filename": "FactSetResults.csv", 
            "description": "FactSet Documents Search",
            "use_watch_list": False,
            "pdf_only": False,
            "include_parsing": True,
            "enabled": ENABLE_FACTSET_SEARCH,
            "search_params": {
                "last_days": 360,           # Keep date restriction for better results
                "language": "lang_zh-TW",   # Keep language restriction for Chinese content
                "country": None             # Remove country restriction only
            }
        }
    ]
    
    # Filter enabled searches only
    enabled_searches = [search for search in search_queries if search.get("enabled", True)]
    
    if not enabled_searches:
        print("âŒ No searches are enabled! Please set at least one search to 'enabled': True")
        print("ğŸ’¡ Edit the ENABLE_* variables at the top of main() function")
        return
    
    print(f"ğŸ¯ Running {len(enabled_searches)} of {len(search_queries)} configured searches:")
    for search in enabled_searches:
        print(f"   âœ… {search['description']}")
    
    for disabled_search in [s for s in search_queries if not s.get("enabled", True)]:
        print(f"   â¸ï¸  {disabled_search['description']} (disabled)")
    
    print()
    
    for search_config in enabled_searches:
        try:
            process_search_query(
                search_config["query"],
                search_config["filename"],
                search_config["description"],
                search_config.get("use_watch_list", False),
                search_config.get("pdf_only", True),
                search_config.get("include_parsing", False),
                search_config.get("search_params", None)
            )
        except Exception as e:
            print(f"Error processing {search_config['description']}: {e}")
            continue
    
    print(f"\n{'='*60}")
    print("All enabled search queries completed!")
    print("Generated files:")
    for search_config in enabled_searches:
        if os.path.exists(search_config["filename"]):
            print(f"- {search_config['filename']}")
            if search_config.get("include_parsing", False):
                print(f"  â””â”€ Includes parsed FactSet data columns (Traditional Chinese)")
    print("- PDF/ (downloaded PDF files and web pages)")
    print("- MD/ (converted Markdown files)")
    print(f"{'='*60}")
    print(f"GoogleSearch.py v{__version__} - Process completed successfully!")

if __name__ == "__main__":
    main()