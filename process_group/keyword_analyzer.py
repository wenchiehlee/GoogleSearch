# æ¸¬è©¦åŠŸèƒ½
if __name__ == "__main__":
    analyzer = KeywordAnalyzer()
    
    print("=== å¢å¼·ç‰ˆæŸ¥è©¢æ¨¡å¼åˆ†æå™¨æ¸¬è©¦ v3.6.1 ===")
    
    # ğŸ†• å°ˆé–€æ¸¬è©¦ç‰¹æ®Šå¾Œç¶´è™•ç†
    print("\nğŸ§ª æ¸¬è©¦ç‰¹æ®Šå¾Œç¶´åç¨±è®Šé«”ç”Ÿæˆ:")
    test_names = ['å¥•åŠ›-KY', 'å‰µæ„-DR', 'æ™ºæ“ KY', 'å°æ–° DR']
    for name in test_names:
        variations = analyzer._get_company_name_variations_enhanced(name)
"""
Keyword Analyzer - FactSet Pipeline v3.6.1 (ENHANCED - Better Name Normalization)
å°ˆæ³¨æ–¼æ¨™æº–åŒ–æŸ¥è©¢æ¨¡å¼åˆ†æï¼Œå¢å¼·å…¬å¸åç¨±æ¨™æº–åŒ–
ä¿®æ­£ï¼šå®Œå…¨éæ¿¾ç„¡æ•ˆæ¨¡å¼ + å¢å¼·å…¬å¸åç¨±è®Šé«”è™•ç†ï¼ˆå¦‚ å¥•åŠ›-KYï¼‰
"""

import os
import re
import yaml
import pandas as pd
from datetime import datetime
from typing import Dict, Any, List, Optional, Set, Tuple
from collections import Counter
import statistics

class KeywordAnalyzer:
    """æŸ¥è©¢æ¨¡å¼åˆ†æå™¨ - v3.6.1 å¢å¼·åç¨±æ¨™æº–åŒ–ç‰ˆ"""
    
    def __init__(self):
        """åˆå§‹åŒ–æŸ¥è©¢æ¨¡å¼åˆ†æå™¨"""
        
        # è¼‰å…¥è§€å¯Ÿåå–®é€²è¡Œé©—è­‰å’Œæ¨™æº–åŒ–
        self.watchlist_mapping = self._load_watchlist_for_normalization()
        
        # ğŸ†• REFINED_SEARCH_PATTERNS å°æ‡‰çš„æ¨¡å¼åˆ†é¡
        self.refined_search_patterns = {
            'factset_direct': [
                'factset {symbol}', 'factset {name}', '{symbol} factset', '{name} factset',
                'factset {symbol} eps', 'factset {name} é ä¼°', '"{symbol}" factset åˆ†æå¸«', 
                '"{name}" factset ç›®æ¨™åƒ¹', '{name} {symbol} factset', '{symbol} {name} factset',
                '{name} factset eps', '{symbol} factset åˆ†æå¸«', '{name} factset åˆ†æå¸«',
                '{symbol} factset ç›®æ¨™åƒ¹', '{name} {symbol} factset åˆ†æå¸«', 
                '{name} {symbol} factset eps', '{name} {symbol} factset é ä¼°',
                '{name} {symbol} factset ç›®æ¨™åƒ¹', '{name} factset ç›®æ¨™åƒ¹'
            ],
            'cnyes_factset': [
                'site:cnyes.com factset {symbol}', 'site:cnyes.com {symbol} factset',
                'site:cnyes.com {symbol} eps é ä¼°', 'site:cnyes.com {name} factset',
                'site:cnyes.com {symbol} åˆ†æå¸«', 'site:cnyes.com factset {name}',
                'site:cnyes.com {symbol} å°è‚¡é ä¼°', 'site:cnyes.com {name} {symbol}',
                'site:cnyes.com {symbol} factset eps', 'site:cnyes.com {name} factset eps'
            ],
            'eps_forecast': [
                '{symbol} eps é ä¼°', '{name} eps é ä¼°', '{symbol} eps 2025',
                '{name} eps 2025', '{symbol} æ¯è‚¡ç›ˆé¤˜ é ä¼°', '{name} æ¯è‚¡ç›ˆé¤˜ é ä¼°',
                '{symbol} eps forecast', '{name} earnings estimates',
                '{name} {symbol} eps', '{name} {symbol} eps é ä¼°',
                '{name} {symbol} 2025 eps', '{name} {symbol} earnings'
            ],
            'analyst_consensus': [
                '{symbol} åˆ†æå¸« é ä¼°', '{name} åˆ†æå¸« é ä¼°', '{symbol} åˆ†æå¸« ç›®æ¨™åƒ¹',
                '{name} åˆ†æå¸« ç›®æ¨™åƒ¹', '{symbol} consensus estimate', '{name} analyst forecast',
                '{symbol} å…±è­˜é ä¼°', '{name} æŠ•è³‡è©•ç­‰', '{name} {symbol} åˆ†æå¸«',
                '{name} {symbol} åˆ†æå¸« é ä¼°', '{name} {symbol} analyst',
                '{name} {symbol} consensus'
            ],
            'taiwan_financial_simple': [
                'site:cnyes.com {symbol}', 'site:statementdog.com {symbol}',
                'site:wantgoo.com {symbol}', 'site:goodinfo.tw {symbol}',
                'site:uanalyze.com.tw {symbol}', 'site:findbillion.com {symbol}',
                'site:moneydj.com {symbol}', 'site:yahoo.com {symbol} è‚¡ç¥¨',
                '{name} {symbol} è‚¡ç¥¨', '{name} {symbol} è‚¡åƒ¹'
            ]
        }
        
        # metadata æ¨¡å¼ - ç”¨æ–¼æå– search_query
        self.metadata_patterns = {
            'search_query': r'search_query:\s*(.+?)(?:\n|$)',
            'keywords': r'keywords:\s*(.+?)(?:\n|$)',
            'search_terms': r'search_terms:\s*(.+?)(?:\n|$)'
        }
        
        # ğŸ”§ å¢å¼·çš„ç„¡æ•ˆæ¨¡å¼éæ¿¾å™¨ - å®Œå…¨éæ¿¾ result ç›¸é—œæ¨¡å¼
        self.invalid_pattern_filters = [
            # Result ç›¸é—œæ¨¡å¼ - å®Œå…¨éæ¿¾
            r'^result[_\s]*\d*[?\w]*$',     # result_1, result_11, result_x, result_xx, result_?
            r'^result[_\s]*[a-z]*$',        # result_x, result_xx, result_abc
            r'^result[_\s]*[?]*$',          # result_?, result_??
            r'^result$',                    # å–®ç´”çš„ result
            
            # å…¶ä»–ç„¡æ•ˆæ¨¡å¼
            r'^\d+$',                       # ç´”æ•¸å­—
            r'^[a-zA-Z]$',                  # å–®ä¸€å­—æ¯
            r'^.{1,2}$',                    # å¤ªçŸ­çš„æ¨¡å¼
            r'^test',                       # æ¸¬è©¦æ¨¡å¼
            r'^debug',                      # èª¿è©¦æ¨¡å¼
            r'^temp',                       # è‡¨æ™‚æ¨¡å¼
            r'^none$',                      # none
            r'^null$',                      # null
            r'^undefined$',                 # undefined
        ]

    def _load_watchlist_for_normalization(self) -> Dict[str, str]:
        """ğŸ”§ ä¿®æ­£ç‰ˆï¼šè¼‰å…¥è§€å¯Ÿåå–®ç”¨æ–¼æ¨™æº–åŒ–"""
        mapping = {}
        
        possible_paths = [
            'StockID_TWSE_TPEX.csv',
            '../StockID_TWSE_TPEX.csv', 
            '../../StockID_TWSE_TPEX.csv',
            'data/StockID_TWSE_TPEX.csv',
            '../data/StockID_TWSE_TPEX.csv'
        ]
        
        for csv_path in possible_paths:
            if os.path.exists(csv_path):
                try:
                    # ä½¿ç”¨å¤šç¨®ç·¨ç¢¼å˜—è©¦è®€å–
                    encodings = ['utf-8', 'utf-8-sig', 'big5', 'gbk', 'cp950']
                    df = None
                    
                    for encoding in encodings:
                        try:
                            df = pd.read_csv(csv_path, header=None, names=['code', 'name'], encoding=encoding)
                            break
                        except UnicodeDecodeError:
                            continue
                    
                    if df is not None:
                        for idx, row in df.iterrows():
                            try:
                                code = str(row['code']).strip()
                                name = str(row['name']).strip()
                                
                                if (code.isdigit() and len(code) == 4 and 
                                    name not in ['nan', 'NaN', 'null', 'None', '']):
                                    mapping[code] = name
                                    # ğŸ”§ åŒæ™‚å»ºç«‹åç¨±åˆ°ä»£è™Ÿçš„åå‘æ˜ å°„
                                    mapping[name] = code
                            except:
                                continue
                        
                        print(f"âœ… è§€å¯Ÿåå–®è¼‰å…¥: {len(mapping)//2} å®¶å…¬å¸ (ç”¨æ–¼æ¨™æº–åŒ–)")
                        return mapping
                        
                except Exception as e:
                    print(f"âš ï¸ è¼‰å…¥è§€å¯Ÿåå–®å¤±æ•—: {e}")
                    continue
        
        print("âš ï¸ è§€å¯Ÿåå–®æœªè¼‰å…¥ï¼Œæ¨™æº–åŒ–å¯èƒ½ä¸å®Œæ•´")
        return {}

    def analyze_query_patterns(self, processed_companies: List[Dict]) -> Dict[str, Any]:
        """ğŸ”§ ä¿®æ­£ç‰ˆï¼šåˆ†ææ¨™æº–åŒ–æŸ¥è©¢æ¨¡å¼"""
        try:
            print("ğŸ” é–‹å§‹æ¨™æº–åŒ–æŸ¥è©¢æ¨¡å¼åˆ†æ (å¢å¼·åç¨±æ¨™æº–åŒ–ç‰ˆ)...")
            
            # æå–æ‰€æœ‰æŸ¥è©¢æ¨¡å¼ä¸¦æ¨™æº–åŒ–
            all_query_patterns = []
            pattern_quality_mapping = {}
            pattern_company_mapping = {}
            
            for company_data in processed_companies:
                company_code = company_data.get('company_code', '')
                company_name = company_data.get('company_name', '')
                quality_score = company_data.get('quality_score', 0)
                
                # ğŸ”§ é©—è­‰å…¬å¸æ˜¯å¦åœ¨è§€å¯Ÿåå–®ä¸­
                if not self._is_company_in_watchlist(company_code, company_name):
                    print(f"âš ï¸ è·³ééè§€å¯Ÿåå–®å…¬å¸: {company_name}({company_code})")
                    continue
                
                # æå–æŸ¥è©¢æ¨¡å¼
                original_patterns = self._extract_query_patterns_from_company_data(company_data)
                
                if original_patterns:
                    for original_pattern in original_patterns:
                        # ğŸ”§ å¼·åŒ–ç„¡æ•ˆæ¨¡å¼éæ¿¾
                        if self._is_invalid_pattern_enhanced(original_pattern):
                            print(f"âš ï¸ éæ¿¾ç„¡æ•ˆæ¨¡å¼: {original_pattern}")
                            continue
                        
                        # ğŸ”§ å¼·åŒ–æ¨™æº–åŒ–æŸ¥è©¢æ¨¡å¼
                        normalized_pattern = self._normalize_query_pattern_enhanced(
                            original_pattern, company_name, company_code
                        )
                        
                        if normalized_pattern and self._is_valid_normalized_pattern(normalized_pattern):
                            # æ”¶é›†æ¨™æº–åŒ–çš„æŸ¥è©¢æ¨¡å¼
                            all_query_patterns.append(normalized_pattern)
                            
                            # å»ºç«‹æ¨™æº–åŒ–æ¨¡å¼èˆ‡å“è³ªè©•åˆ†çš„å°æ‡‰
                            if normalized_pattern not in pattern_quality_mapping:
                                pattern_quality_mapping[normalized_pattern] = []
                            pattern_quality_mapping[normalized_pattern].append(quality_score)
                            
                            # å»ºç«‹æ¨™æº–åŒ–æ¨¡å¼èˆ‡å…¬å¸çš„å°æ‡‰
                            if normalized_pattern not in pattern_company_mapping:
                                pattern_company_mapping[normalized_pattern] = set()
                            pattern_company_mapping[normalized_pattern].add(f"{company_name}({company_code})")
                        else:
                            print(f"âš ï¸ æ¨™æº–åŒ–å¤±æ•—: {original_pattern} -> {normalized_pattern}")
            
            # çµ±è¨ˆæ¨™æº–åŒ–æŸ¥è©¢æ¨¡å¼ä½¿ç”¨é »ç‡
            pattern_counter = Counter(all_query_patterns)
            
            # ğŸ”§ é¡¯ç¤ºæ¨™æº–åŒ–çµæœçµ±è¨ˆ
            print(f"ğŸ“Š æ¨™æº–åŒ–çµæœ:")
            print(f"   åŸå§‹æ¨¡å¼æ•¸: {len([p for sublist in [self._extract_query_patterns_from_company_data(c) for c in processed_companies] for p in sublist])}")
            print(f"   æœ‰æ•ˆæ¨™æº–åŒ–æ¨¡å¼: {len(pattern_counter)}")
            print(f"   ç¸½ä½¿ç”¨æ¬¡æ•¸: {sum(pattern_counter.values())}")
            
            # è¨ˆç®—æŸ¥è©¢æ¨¡å¼çµ±è¨ˆ
            pattern_stats = {}
            for pattern, count in pattern_counter.items():
                quality_scores = pattern_quality_mapping.get(pattern, [])
                
                if quality_scores:
                    pattern_stats[pattern] = {
                        'usage_count': count,
                        'avg_quality_score': round(statistics.mean(quality_scores), 2),
                        'max_quality_score': max(quality_scores),
                        'min_quality_score': min(quality_scores),
                        'quality_std': round(statistics.stdev(quality_scores), 2) if len(quality_scores) > 1 else 0,
                        'company_count': len(pattern_company_mapping.get(pattern, set())),
                        'related_companies': list(pattern_company_mapping.get(pattern, set())),
                        'category': self._categorize_search_pattern(pattern),
                        'effectiveness_score': self._calculate_pattern_effectiveness_score(count, quality_scores),
                        'pattern_type': self._identify_pattern_type(pattern)
                    }
            
            # ğŸ”§ é¡¯ç¤ºå‰10å€‹æœ€å¸¸ç”¨çš„æ¨™æº–åŒ–æ¨¡å¼
            print(f"\nğŸ“Š å‰10å€‹æœ€å¸¸ç”¨çš„æ¨™æº–åŒ–æ¨¡å¼:")
            top_patterns = sorted(pattern_stats.items(), key=lambda x: x[1]['usage_count'], reverse=True)[:10]
            for i, (pattern, stats) in enumerate(top_patterns, 1):
                print(f"   {i:2d}. {pattern} (ä½¿ç”¨ {stats['usage_count']} æ¬¡, å“è³ª {stats['avg_quality_score']})")
            
            # ç”Ÿæˆåˆ†æçµæœ
            analysis_result = {
                'timestamp': datetime.now().isoformat(),
                'version': '3.6.1_enhanced',
                'analysis_type': 'search_query_patterns_normalized',
                'total_companies_analyzed': len(processed_companies),
                'valid_companies_processed': len([c for c in processed_companies if self._is_company_in_watchlist(c.get('company_code', ''), c.get('company_name', ''))]),
                'total_query_patterns_found': len(all_query_patterns),
                'unique_query_patterns': len(pattern_stats),
                'pattern_stats': pattern_stats,  # é—œéµï¼ç”¨æ–¼ CSV ç”Ÿæˆ
                'pattern_type_summary': self._generate_pattern_type_summary(pattern_stats),
                'top_query_patterns': self._get_top_query_patterns(pattern_stats, 20),
                'effectiveness_analysis': self._analyze_pattern_effectiveness(pattern_stats),
                'quality_correlation': self._analyze_pattern_quality_correlation(pattern_stats)
            }
            
            print(f"âœ… æ¨™æº–åŒ–æŸ¥è©¢æ¨¡å¼åˆ†æå®Œæˆ:")
            print(f"   æœ‰æ•ˆå…¬å¸æ•¸: {analysis_result['valid_companies_processed']}")
            print(f"   ç¸½æŸ¥è©¢æ¨¡å¼æ•¸: {analysis_result['total_query_patterns_found']}")
            print(f"   å”¯ä¸€æ¨™æº–åŒ–æŸ¥è©¢æ¨¡å¼: {analysis_result['unique_query_patterns']}")
            
            return analysis_result
            
        except Exception as e:
            print(f"âŒ æŸ¥è©¢æ¨¡å¼åˆ†æå¤±æ•—: {e}")
            return {'error': str(e)}

    def _is_company_in_watchlist(self, company_code: str, company_name: str) -> bool:
        """ğŸ†• æª¢æŸ¥å…¬å¸æ˜¯å¦åœ¨è§€å¯Ÿåå–®ä¸­"""
        if not self.watchlist_mapping:
            return True  # å¦‚æœæ²’æœ‰è§€å¯Ÿåå–®ï¼Œå…è¨±æ‰€æœ‰å…¬å¸
        
        # æª¢æŸ¥ä»£è™Ÿæˆ–åç¨±æ˜¯å¦åœ¨è§€å¯Ÿåå–®ä¸­
        return (company_code in self.watchlist_mapping or 
                company_name in self.watchlist_mapping)

    def _is_invalid_pattern_enhanced(self, pattern: str) -> bool:
        """ğŸ”§ å¢å¼·ç‰ˆï¼šæª¢æŸ¥æ˜¯å¦ç‚ºç„¡æ•ˆæ¨¡å¼"""
        if not pattern or not pattern.strip():
            return True
        
        pattern_clean = pattern.strip().lower()
        
        # ğŸ”§ å¢å¼·çš„ç„¡æ•ˆæ¨¡å¼æª¢æŸ¥ - å®Œå…¨éæ¿¾ result ç›¸é—œ
        for filter_pattern in self.invalid_pattern_filters:
            if re.match(filter_pattern, pattern_clean, re.IGNORECASE):
                print(f"ğŸš« éæ¿¾ç„¡æ•ˆæ¨¡å¼ (regex): {pattern}")
                return True
        
        # ğŸ”§ é¡å¤–çš„ result æ¨¡å¼æª¢æŸ¥
        if 'result' in pattern_clean:
            print(f"ğŸš« éæ¿¾ result æ¨¡å¼: {pattern}")
            return True
        
        # é¡å¤–æª¢æŸ¥
        if len(pattern_clean) <= 2:  # å¤ªçŸ­
            return True
        
        if pattern_clean.count(' ') == 0 and not any(char in pattern_clean for char in [':', '.']):  # å–®è©ä¸”éç¶²ç«™
            if len(pattern_clean) <= 4:
                return True
        
        return False

    def _normalize_query_pattern_enhanced(self, pattern: str, company_name: str, company_code: str) -> str:
        """ğŸ”§ å¢å¼·ç‰ˆæ¨™æº–åŒ–æŸ¥è©¢æ¨¡å¼ - æ›´å¥½çš„åç¨±è™•ç†"""
        if not pattern or not pattern.strip():
            return ""
        
        normalized = pattern.strip()
        
        # ğŸ”§ æ¸…ç†ç‰¹æ®Šå­—ç¬¦
        normalized = re.sub(r'["\']', '', normalized)  # ç§»é™¤å¼•è™Ÿ
        normalized = re.sub(r'[+\-()]', ' ', normalized)  # ç§»é™¤æœå°‹é‹ç®—ç¬¦
        
        # ğŸ”§ è™•ç†site:å‰ç¶´
        site_prefix = ""
        site_match = re.match(r'^(site:[^\s]+)\s+(.+)$', normalized)
        if site_match:
            site_prefix = site_match.group(1) + ' '
            normalized = site_match.group(2)
        
        # ğŸ”§ å¼·åŒ–ï¼šä½¿ç”¨è§€å¯Ÿåå–®é€²è¡Œç²¾ç¢ºæ›¿æ›
        normalized = self._replace_with_watchlist_enhanced(normalized, company_code, company_name)
        
        # ğŸ”§ æ¸…ç†å¤šé¤˜ç©ºæ ¼
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        
        # ğŸ”§ é‡æ–°åŠ ä¸Šsiteå‰ç¶´
        if site_prefix:
            normalized = site_prefix + normalized
        
        return normalized

    def _replace_with_watchlist_enhanced(self, text: str, company_code: str, company_name: str) -> str:
        """ğŸ”§ å¢å¼·ç‰ˆï¼šä½¿ç”¨è§€å¯Ÿåå–®é€²è¡Œç²¾ç¢ºæ›¿æ›ï¼Œè™•ç† -KY ç­‰ç‰¹æ®Šå¾Œç¶´"""
        result = text
        
        # ğŸ”§ 1. æ›¿æ›å…¬å¸ä»£è™Ÿ
        if company_code and company_code.isdigit() and len(company_code) == 4:
            # ä½¿ç”¨è©é‚Šç•Œç¢ºä¿ç²¾ç¢ºåŒ¹é…
            result = re.sub(rf'\b{re.escape(company_code)}\b', '{symbol}', result)
        
        # ğŸ”§ 2. å¢å¼·çš„å…¬å¸åç¨±æ›¿æ› - è™•ç†è§€å¯Ÿåå–®
        if self.watchlist_mapping and company_code in self.watchlist_mapping:
            # å¾è§€å¯Ÿåå–®å–å¾—æ¨™æº–åç¨±
            standard_name = self.watchlist_mapping[company_code]
            
            # ğŸ†• ç²å–å¢å¼·çš„åç¨±è®Šé«”ï¼ˆåŒ…å« -KY è™•ç†ï¼‰
            name_variations = self._get_company_name_variations_enhanced(standard_name)
            
            # æŒ‰é•·åº¦æ’åºï¼Œé•·çš„å„ªå…ˆåŒ¹é…
            name_variations.sort(key=len, reverse=True)
            
            for variation in name_variations:
                if variation and len(variation) >= 2:
                    # ä½¿ç”¨è©é‚Šç•Œé€²è¡Œç²¾ç¢ºåŒ¹é…
                    escaped_variation = re.escape(variation)
                    result = re.sub(rf'\b{escaped_variation}\b', '{name}', result, flags=re.IGNORECASE)
        
        # ğŸ”§ 3. å›é€€ï¼šæ›¿æ›æª”æ¡ˆä¸­çš„å…¬å¸åç¨±
        if company_name and '{name}' not in result:
            # ğŸ†• è™•ç†æª”æ¡ˆä¸­çš„å…¬å¸åç¨±ï¼ˆåŒ…å«ç‰¹æ®Šå¾Œç¶´ï¼‰
            name_variations = self._get_company_name_variations_enhanced(company_name)
            name_variations.sort(key=len, reverse=True)
            
            for variation in name_variations:
                if variation and len(variation) >= 2:
                    escaped_variation = re.escape(variation)
                    result = re.sub(rf'\b{escaped_variation}\b', '{name}', result, flags=re.IGNORECASE)
        
        # ğŸ”§ 4. æœ€å¾Œæª¢æŸ¥ï¼šå¦‚æœä»ç„¶æ²’æœ‰æ¨¡æ¿è®Šæ•¸ï¼Œå˜—è©¦æ™ºèƒ½æ¨æ¸¬
        if '{name}' not in result and '{symbol}' not in result:
            result = self._smart_detect_company_references(result, company_name, company_code)
        
        return result

    def _get_company_name_variations_enhanced(self, company_name: str) -> List[str]:
        """ğŸ†• å¢å¼·ç‰ˆï¼šå–å¾—å…¬å¸åç¨±çš„å„ç¨®è®ŠåŒ–å½¢å¼ï¼ŒåŒ…å«ç‰¹æ®Šå¾Œç¶´è™•ç†"""
        variations = [company_name]
        
        # ğŸ†• å®Œæ•´è™•ç†ç‰¹æ®Šå¾Œç¶´ï¼ˆå¦‚ -KY, -DR ç­‰å¤–åœ‹ä¼æ¥­æ¨™è¨˜ï¼‰
        special_suffix_types = ['KY', 'DR', 'GDR', 'ADR']
        name_without_special = company_name
        
        # ğŸ”§ ç³»çµ±æ€§è™•ç†ï¼šç‚ºæ¯å€‹å¾Œç¶´é¡å‹ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„æ ¼å¼è®Šé«”
        for suffix_type in special_suffix_types:
            possible_formats = [
                f'-{suffix_type}',     # å¥•åŠ›-KY
                f' {suffix_type}',     # å¥•åŠ› KY  
                f'{suffix_type}',      # å¥•åŠ›KY
            ]
            
            for format_suffix in possible_formats:
                if name_without_special.endswith(format_suffix):
                    base_name = name_without_special[:-len(format_suffix)].strip()
                    variations.append(base_name)  # åŸºæœ¬åç¨±ï¼šå¥•åŠ›
                    
                    # ğŸ†• ç‚ºé€™å€‹åŸºæœ¬åç¨±ç”Ÿæˆæ‰€æœ‰æ ¼å¼è®Šé«”
                    all_variations_for_base = [
                        base_name + f'-{suffix_type}',   # å¥•åŠ›-KY
                        base_name + f' {suffix_type}',   # å¥•åŠ› KY
                        base_name + f'{suffix_type}',    # å¥•åŠ›KY
                    ]
                    variations.extend(all_variations_for_base)
                    
                    # ğŸ†• æ›´æ–°åŸºç¤åç¨±
                    name_without_special = base_name
                    break  # æ‰¾åˆ°åŒ¹é…å¾Œè·³å‡ºå…§å±¤å¾ªç’°
        
        # ç§»é™¤å¸¸è¦‹ä¼æ¥­å¾Œç¶´
        company_suffixes = ['è‚¡ä»½æœ‰é™å…¬å¸', 'æœ‰é™å…¬å¸', 'å…¬å¸', 'é›†åœ˜', 'æ§è‚¡', 'ç§‘æŠ€', 'é›»å­', 'å·¥æ¥­']
        
        current_name = name_without_special
        for suffix in company_suffixes:
            if current_name.endswith(suffix):
                current_name = current_name[:-len(suffix)].strip()
                if current_name:  # ç¢ºä¿ä¸æ˜¯ç©ºå­—ä¸²
                    variations.append(current_name)
        
        # ğŸ†• ç‰¹åˆ¥è™•ç†ä¸­æ–‡å…¬å¸åç¨±çš„å¸¸è¦‹ç¸®å¯«
        name_aliases = {
            'å°ç©é›»': ['å°ç©', 'TSMC', 'å°ç£ç©é«”é›»è·¯'],
            'é´»æµ·': ['é´»æµ·ç²¾å¯†', 'Foxconn', 'å¯Œå£«åº·'],
            'è¯ç™¼ç§‘': ['è¯ç™¼', 'MediaTek', 'è¯ç™¼ç§‘æŠ€'],
            'å°é”é›»': ['å°é”', 'Delta', 'å°é”é›»å­'],
            'ä¸­è¯é›»ä¿¡': ['ä¸­è¯é›»', 'ä¸­è¯é›»ä¿¡'],
            'å¥•åŠ›': ['å¥•åŠ›ç§‘æŠ€'],  # ğŸ†• é‡å°æ‚¨çš„ä¾‹å­
            'å…‰ç„±': ['å…‰ç„±ç§‘æŠ€'],
        }
        
        # æª¢æŸ¥æ˜¯å¦æœ‰åˆ¥åå®šç¾©
        for name_part in variations:
            if name_part in name_aliases:
                variations.extend(name_aliases[name_part])
        
        # ğŸ†• è‡ªå‹•ç”Ÿæˆå¯èƒ½çš„ç¸®å¯«ï¼ˆé‡å°ä¸­æ–‡å…¬å¸åï¼‰
        if len(company_name) >= 3 and any('\u4e00' <= char <= '\u9fff' for char in company_name):
            # å¦‚æœæ˜¯ä¸­æ–‡åç¨±ï¼Œå˜—è©¦å–å‰2-3å€‹å­—ä½œç‚ºç¸®å¯«
            if len(company_name) >= 4:
                variations.append(company_name[:3])  # å‰3å€‹å­—
            if len(company_name) >= 3:
                variations.append(company_name[:2])  # å‰2å€‹å­—
        
        # å»é‡ä¸¦æŒ‰é•·åº¦æ’åºï¼ˆé•·çš„å„ªå…ˆï¼Œé¿å…èª¤æ›¿æ›ï¼‰
        variations = list(set(v for v in variations if v and len(v.strip()) >= 2))
        variations.sort(key=len, reverse=True)
        
        return variations

    def _smart_detect_company_references(self, text: str, company_name: str, company_code: str) -> str:
        """ğŸ”§ å¢å¼·ç‰ˆï¼šæ™ºèƒ½æª¢æ¸¬å…¬å¸å¼•ç”¨"""
        words = text.split()
        result_words = []
        
        for word in words:
            # æª¢æŸ¥æ˜¯å¦ç‚º4ä½æ•¸å­—ï¼ˆå¯èƒ½æ˜¯å…¬å¸ä»£è™Ÿï¼‰
            if word.isdigit() and len(word) == 4:
                result_words.append('{symbol}')
            # ğŸ†• æª¢æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡ä¸”å¯èƒ½æ˜¯å…¬å¸åç¨±ï¼ˆåŒ…å«ç‰¹æ®Šå¾Œç¶´ï¼‰
            elif re.search(r'[\u4e00-\u9fff]', word) and 2 <= len(word) <= 10:
                # æª¢æŸ¥æ˜¯å¦èˆ‡å·²çŸ¥å…¬å¸åç¨±ç›¸ä¼¼
                if self._is_likely_company_name_enhanced(word, company_name):
                    result_words.append('{name}')
                else:
                    result_words.append(word)
            else:
                result_words.append(word)
        
        return ' '.join(result_words)

    def _is_likely_company_name_enhanced(self, word: str, company_name: str) -> bool:
        """ğŸ†• å¢å¼·ç‰ˆï¼šæª¢æŸ¥å–®è©æ˜¯å¦å¯èƒ½æ˜¯å…¬å¸åç¨±"""
        if not word or not company_name:
            return False
        
        # ğŸ†• è™•ç†ç‰¹æ®Šå¾Œç¶´
        word_clean = word
        company_clean = company_name
        
        special_suffixes = ['-KY', '-DR', ' KY', ' DR']
        for suffix in special_suffixes:
            word_clean = word_clean.replace(suffix, '').strip()
            company_clean = company_clean.replace(suffix, '').strip()
        
        # å¦‚æœè§€å¯Ÿåå–®å¯ç”¨ï¼Œæª¢æŸ¥æ˜¯å¦åœ¨å…¶ä¸­
        if self.watchlist_mapping:
            for name in self.watchlist_mapping.values():
                if isinstance(name, str):
                    name_clean = name
                    for suffix in special_suffixes:
                        name_clean = name_clean.replace(suffix, '').strip()
                    
                    if (word_clean in name_clean or name_clean in word_clean or
                        word in name or name in word):
                        return True
        
        # å›é€€ï¼šåŸºæœ¬ç›¸ä¼¼æ€§æª¢æŸ¥
        if (word_clean in company_clean or company_clean in word_clean or
            word in company_name or company_name in word):
            return True
        
        # æª¢æŸ¥å…±åŒå­—ç¬¦æ¯”ä¾‹
        if len(word_clean) >= 2 and len(company_clean) >= 2:
            common_chars = set(word_clean) & set(company_clean)
            similarity = len(common_chars) / max(len(set(word_clean)), len(set(company_clean)))
            return similarity >= 0.6
        
        return False

    def _is_valid_normalized_pattern(self, pattern: str) -> bool:
        """ğŸ”§ æª¢æŸ¥æ¨™æº–åŒ–æ¨¡å¼æ˜¯å¦æœ‰æ•ˆ"""
        if not pattern or not pattern.strip():
            return False
        
        # å¿…é ˆåŒ…å«è‡³å°‘ä¸€å€‹æ¨¡æ¿è®Šæ•¸
        if '{name}' not in pattern and '{symbol}' not in pattern:
            return False
        
        # ä¸æ‡‰è©²åªæ˜¯æ¨¡æ¿è®Šæ•¸
        pattern_clean = pattern.replace('{name}', '').replace('{symbol}', '').strip()
        if not pattern_clean:
            return False
        
        # é•·åº¦æª¢æŸ¥
        if len(pattern) < 5:
            return False
        
        return True

    def _extract_query_patterns_from_company_data(self, company_data: Dict) -> List[str]:
        """å¾å…¬å¸è³‡æ–™ä¸­æå–æŸ¥è©¢æ¨¡å¼"""
        query_patterns = []
        
        # 1. å¾ YAML metadata ä¸­æå– search_query
        yaml_data = company_data.get('yaml_data', {})
        if yaml_data:
            search_query = yaml_data.get('search_query', '')
            if search_query and search_query.strip():
                cleaned_query = self._clean_query_pattern(search_query.strip())
                if cleaned_query:
                    query_patterns.append(cleaned_query)
        
        # 2. å¾æª”æ¡ˆå…§å®¹ä¸­æå– metadata ä¸­çš„æŸ¥è©¢æ¨¡å¼
        content = company_data.get('content', '')
        if content:
            metadata_patterns = self._extract_query_patterns_from_content_metadata(content)
            query_patterns.extend(metadata_patterns)
        
        # å»é‡ä½†ä¿æŒé †åº
        unique_patterns = []
        seen_patterns = set()
        for pattern in query_patterns:
            pattern_lower = pattern.lower()
            if pattern_lower not in seen_patterns:
                unique_patterns.append(pattern)
                seen_patterns.add(pattern_lower)
        
        return unique_patterns

    def _clean_query_pattern(self, query: str) -> str:
        """æ¸…ç†æŸ¥è©¢æ¨¡å¼ä½†ä¿æŒå®Œæ•´æ€§"""
        if not query or query.strip() == '':
            return ''
        
        cleaned = query.strip()
        cleaned = re.sub(r'[+\-"()]', ' ', cleaned)  # ç§»é™¤æœå°‹é‹ç®—ç¬¦
        cleaned = re.sub(r'\s+', ' ', cleaned)       # åˆä½µå¤šå€‹ç©ºæ ¼
        cleaned = cleaned.strip()
        
        # æª¢æŸ¥é•·åº¦
        if len(cleaned) < 3:
            return ''
        
        return cleaned

    def _extract_query_patterns_from_content_metadata(self, content: str) -> List[str]:
        """å¾å…§å®¹çš„ metadata ä¸­æå–æŸ¥è©¢æ¨¡å¼"""
        patterns = []
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ YAML frontmatter
        if content.startswith('---'):
            try:
                end_pos = content.find('---', 3)
                if end_pos != -1:
                    yaml_content = content[3:end_pos].strip()
                    
                    # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå–æŸ¥è©¢ç›¸é—œæ¬„ä½
                    for field_name, pattern in self.metadata_patterns.items():
                        matches = re.findall(pattern, yaml_content, re.MULTILINE | re.IGNORECASE)
                        for match in matches:
                            if match.strip():
                                cleaned_pattern = self._clean_query_pattern(match.strip())
                                if cleaned_pattern:
                                    patterns.append(cleaned_pattern)
            except Exception:
                pass
        
        return patterns

    # ä¿ç•™å…¶ä»–ç¾æœ‰æ–¹æ³•...
    def _identify_pattern_type(self, pattern: str) -> str:
        """è­˜åˆ¥æŸ¥è©¢æ¨¡å¼é¡å‹"""
        pattern_lower = pattern.lower()
        
        # factset_direct æ¨¡å¼
        if 'factset' in pattern_lower and 'site:' not in pattern_lower:
            return 'factset_direct'
        
        # cnyes_factset æ¨¡å¼
        if 'site:cnyes.com' in pattern_lower:
            return 'cnyes_factset'
        
        # eps_forecast æ¨¡å¼
        if any(term in pattern_lower for term in ['eps', 'æ¯è‚¡ç›ˆé¤˜']) and \
           any(term in pattern_lower for term in ['é ä¼°', '2025', '2026', '2027']):
            return 'eps_forecast'
        
        # analyst_consensus æ¨¡å¼
        if any(term in pattern_lower for term in ['åˆ†æå¸«', 'analyst', 'consensus']):
            return 'analyst_consensus'
        
        # taiwan_financial_simple æ¨¡å¼
        taiwan_sites = ['cnyes.com', 'statementdog.com', 'wantgoo.com', 'goodinfo.tw']
        if any(site in pattern_lower for site in taiwan_sites):
            return 'taiwan_financial_simple'
        
        return 'other'

    def _categorize_search_pattern(self, pattern: str) -> str:
        """å°‡æœå°‹æ¨¡å¼åˆ†é¡"""
        pattern_type = self._identify_pattern_type(pattern)
        
        category_mapping = {
            'factset_direct': 'FactSetç›´æ¥',
            'cnyes_factset': 'é‰…äº¨ç¶²FactSet',
            'eps_forecast': 'EPSé ä¼°',
            'analyst_consensus': 'åˆ†æå¸«å…±è­˜',
            'taiwan_financial_simple': 'å°ç£è²¡ç¶“ç¶²ç«™',
            'other': 'å…¶ä»–'
        }
        
        return category_mapping.get(pattern_type, 'å…¶ä»–')

    def _calculate_pattern_effectiveness_score(self, usage_count: int, quality_scores: List[float]) -> float:
        """è¨ˆç®—æŸ¥è©¢æ¨¡å¼æ•ˆæœè©•åˆ†"""
        if not quality_scores:
            return 0.0
        
        avg_quality = statistics.mean(quality_scores)
        frequency_weight = min(usage_count / 10, 1.0)
        effectiveness = avg_quality * (0.7 + 0.3 * frequency_weight)
        
        return round(effectiveness, 2)

    def _generate_pattern_type_summary(self, pattern_stats: Dict) -> Dict[str, Any]:
        """ç”ŸæˆæŸ¥è©¢æ¨¡å¼é¡å‹æ‘˜è¦"""
        type_summary = {}
        
        for pattern, stats in pattern_stats.items():
            pattern_type = stats.get('pattern_type', 'other')
            
            if pattern_type not in type_summary:
                type_summary[pattern_type] = {
                    'pattern_count': 0,
                    'total_usage': 0,
                    'quality_scores': [],
                    'patterns': []
                }
            
            type_summary[pattern_type]['pattern_count'] += 1
            type_summary[pattern_type]['total_usage'] += stats['usage_count']
            type_summary[pattern_type]['quality_scores'].append(stats['avg_quality_score'])
            type_summary[pattern_type]['patterns'].append(pattern)
        
        return type_summary

    def _get_top_query_patterns(self, pattern_stats: Dict, top_n: int = 20) -> List[Tuple[str, Dict]]:
        """å–å¾—æ•ˆæœæœ€å¥½çš„æŸ¥è©¢æ¨¡å¼"""
        sorted_patterns = sorted(pattern_stats.items(), 
                               key=lambda x: x[1]['effectiveness_score'], 
                               reverse=True)
        
        return sorted_patterns[:top_n]

    def _analyze_pattern_effectiveness(self, pattern_stats: Dict) -> Dict[str, Any]:
        """åˆ†ææŸ¥è©¢æ¨¡å¼æ•ˆæœ"""
        effectiveness_scores = [stats['effectiveness_score'] for stats in pattern_stats.values()]
        
        if not effectiveness_scores:
            return {}
        
        return {
            'average_effectiveness': round(statistics.mean(effectiveness_scores), 2),
            'median_effectiveness': round(statistics.median(effectiveness_scores), 2)
        }

    def _analyze_pattern_quality_correlation(self, pattern_stats: Dict) -> Dict[str, Any]:
        """åˆ†ææŸ¥è©¢æ¨¡å¼èˆ‡å“è³ªçš„é—œè¯æ€§"""
        correlations = []
        
        for pattern, stats in pattern_stats.items():
            if stats['usage_count'] >= 2:
                correlation_score = stats['avg_quality_score'] * (1 + min(stats['usage_count'] / 10, 0.5))
                correlations.append({
                    'pattern': pattern,
                    'correlation_score': round(correlation_score, 2)
                })
        
        correlations.sort(key=lambda x: x['correlation_score'], reverse=True)
        
        return {
            'strong_positive_correlation': correlations[:10],
            'recommended_patterns': [c['pattern'] for c in correlations[:5]]
        }

    def analyze_all_keywords(self, processed_companies: List[Dict]) -> Dict[str, Any]:
        """å‘ä¸‹ç›¸å®¹ï¼šè‡ªå‹•è½‰æ›ç‚ºæŸ¥è©¢æ¨¡å¼åˆ†æ"""
        print("ğŸ”„ æª¢æ¸¬åˆ°é—œéµå­—åˆ†æè«‹æ±‚ï¼Œè‡ªå‹•è½‰æ›ç‚ºæŸ¥è©¢æ¨¡å¼åˆ†æ...")
        return self.analyze_query_patterns(processed_companies)


# æ¸¬è©¦åŠŸèƒ½
if __name__ == "__main__":
    analyzer = KeywordAnalyzer()
    
    print("=== å¢å¼·ç‰ˆæŸ¥è©¢æ¨¡å¼åˆ†æå™¨æ¸¬è©¦ v3.6.1 ===")
    
    # æ¸¬è©¦è³‡æ–™ - åŒ…å«æ‚¨æåˆ°çš„æ¡ˆä¾‹
    test_companies = [
        {
            'company_code': '6962',
            'company_name': 'å¥•åŠ›-KY',
            'quality_score': 8.5,
            'yaml_data': {
                'search_query': '6962,å¥•åŠ›-KY factset'  # æ‚¨çš„ä¾‹å­
            }
        },
        {
            'company_code': '3089',
            'company_name': 'å…‰ç„±ç§‘æŠ€',
            'quality_score': 7.5,
            'yaml_data': {
                'search_query': 'å…‰ç„±ç§‘æŠ€ factset ç›®æ¨™åƒ¹'
            }
        },
        {
            'company_code': '2330',
            'company_name': 'å°ç©é›»',
            'quality_score': 9.5,
            'yaml_data': {
                'search_query': 'å°ç©é›» 2330 factset åˆ†æå¸«'
            }
        },
        {
            'company_code': '1234',
            'company_name': 'æ¸¬è©¦å…¬å¸',
            'quality_score': 5.0,
            'yaml_data': {
                'search_query': 'result_x'  # ç„¡æ•ˆæ¨¡å¼ - æ‡‰è©²è¢«éæ¿¾
            }
        },
        {
            'company_code': '5678',
            'company_name': 'å¦ä¸€æ¸¬è©¦',
            'quality_score': 6.0,
            'yaml_data': {
                'search_query': 'result_xx'  # ç„¡æ•ˆæ¨¡å¼ - æ‡‰è©²è¢«éæ¿¾
            }
        }
    ]
    
    print(f"\nğŸ§ª æ¸¬è©¦å¢å¼·ç‰ˆæ¨™æº–åŒ– (åŒ…å« -KY è™•ç†å’Œ result éæ¿¾):")
    analysis_result = analyzer.analyze_query_patterns(test_companies)
    
    if 'error' not in analysis_result:
        print(f"\nğŸ“Š å¢å¼·ç‰ˆåˆ†æçµæœ:")
        pattern_stats = analysis_result['pattern_stats']
        
        print(f"\nâœ… æ¨™æº–åŒ–å¾Œçš„æŸ¥è©¢æ¨¡å¼:")
        for pattern, stats in list(pattern_stats.items())[:10]:
            print(f"   {pattern}: ä½¿ç”¨ {stats['usage_count']} æ¬¡, åˆ†é¡: {stats['category']}")
        
        # é©—è­‰ -KY è™•ç†
        ky_patterns = [p for p in pattern_stats.keys() if '{name}' in p and 'factset' in p]
        print(f"\nğŸ” -KY è™•ç†é©—è­‰:")
        print(f"   æ‰¾åˆ°åŒ…å« {{name}} çš„ factset æ¨¡å¼: {len(ky_patterns)}")
        for pattern in ky_patterns[:3]:
            print(f"   - {pattern}")
        
        # é©—è­‰ result éæ¿¾
        result_patterns = [p for p in pattern_stats.keys() if 'result' in p.lower()]
        print(f"\nğŸš« Result éæ¿¾é©—è­‰:")
        print(f"   æ‰¾åˆ°åŒ…å« 'result' çš„æ¨¡å¼: {len(result_patterns)} (æ‡‰è©²ç‚º 0)")
        if result_patterns:
            print(f"   âŒ ç™¼ç¾æœªéæ¿¾çš„ result æ¨¡å¼: {result_patterns}")
        else:
            print(f"   âœ… æ‰€æœ‰ result æ¨¡å¼å·²æˆåŠŸéæ¿¾")
        
    else:
        print(f"âŒ åˆ†æå¤±æ•—: {analysis_result['error']}")
    
    print(f"\nâœ… å¢å¼·ç‰ˆæŸ¥è©¢æ¨¡å¼åˆ†æå™¨æ¸¬è©¦å®Œæˆï¼")
    print(f"ğŸ†• æ–°åŠŸèƒ½: -KY å¾Œç¶´è™•ç†ã€å¢å¼· result éæ¿¾ã€æ™ºèƒ½åç¨±è®Šé«”åŒ¹é…")