#!/usr/bin/env python3
"""
MD Parser - FactSet Pipeline v3.6.1 (Refined)
å¢å¼·ç‰ˆ MD æª”æ¡ˆè§£æå™¨ï¼Œæ”¯æ´æŸ¥è©¢æ¨¡å¼æå–å’Œè§€å¯Ÿåå–®é©—è­‰
å®Œå…¨æ•´åˆ v3.6.1 åŠŸèƒ½è¦æ±‚
"""

import os
import re
import yaml
import pandas as pd
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
import statistics
import json

class MDParser:
    def __init__(self):
        """åˆå§‹åŒ– MD è§£æå™¨ - v3.6.1 å¢å¼·ç‰ˆ"""
        
        self.version = "3.6.1"
        
        # ğŸ†• å¢å¼·çš„ metadata æ¨¡å¼ - æ”¯æ´æŸ¥è©¢æ¨¡å¼æå–
        self.metadata_patterns = {
            'search_query': r'search_query:\s*(.+?)(?:\n|$)',
            'keywords': r'keywords:\s*(.+?)(?:\n|$)',
            'search_terms': r'search_terms:\s*(.+?)(?:\n|$)',
            'query_pattern': r'query_pattern:\s*(.+?)(?:\n|$)',
            'original_query': r'original_query:\s*(.+?)(?:\n|$)',
            'company_code': r'company_code:\s*(.+?)(?:\n|$)',
            'company_name': r'company_name:\s*(.+?)(?:\n|$)',
            'data_source': r'data_source:\s*(.+?)(?:\n|$)',
            'timestamp': r'timestamp:\s*(.+?)(?:\n|$)',
            'extracted_date': r'extracted_date:\s*(.+?)(?:\n|$)'
        }
        
        # åŸæœ‰çš„æ—¥æœŸå’Œæ•¸æ“šæå–æ¨¡å¼
        self.date_patterns = [
            r'\*\s*(\d{4})-(\d{1,2})-(\d{1,2})\s+\d{1,2}:\d{1,2}',
            r'\*\s*æ›´æ–°[ï¼š:]\s*(\d{4})-(\d{1,2})-(\d{1,2})\s+\d{1,2}:\d{1,2}',
            r'æ›´æ–°[ï¼š:]\s*(\d{4})-(\d{1,2})-(\d{1,2})\s+\d{1,2}:\d{1,2}',
            r'é‰…äº¨ç¶²æ–°èä¸­å¿ƒ\s*\n\s*\n\s*(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥[é€±å‘¨]?[ä¸€äºŒä¸‰å››äº”å…­æ—¥å¤©]?\s*[ä¸Šä¸‹]åˆ\s*\d{1,2}:\d{1,2}',
            r'é‰…äº¨ç¶²æ–°èä¸­å¿ƒ\s*\n\s*\n\s*(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
            r'é‰…äº¨ç¶²æ–°èä¸­å¿ƒ[\s\n]+(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥[é€±å‘¨]?[ä¸€äºŒä¸‰å››äº”å…­æ—¥å¤©]?',
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥[é€±å‘¨]?[ä¸€äºŒä¸‰å››äº”å…­æ—¥å¤©]?\s*[ä¸Šä¸‹]åˆ\s*\d{1,2}:\d{1,2}',
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥[é€±å‘¨]?[ä¸€äºŒä¸‰å››äº”å…­æ—¥å¤©]?',
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
            r'(\d{4})-(\d{1,2})-(\d{1,2})\s+\d{1,2}:\d{1,2}:\d{1,2}',
            r'(\d{4})-(\d{1,2})-(\d{1,2})\s+\d{1,2}:\d{1,2}',
            r'(\d{4})-(\d{1,2})-(\d{1,2})',
            r'(\d{4})/(\d{1,2})/(\d{1,2})',
        ]
        
        self.eps_patterns = [
            r'(\d{4})å¹´[^|]*\|\s*([0-9]+\.?[0-9]*)',
            r'(\d{4})\s*å¹´\s*[:ï¼š]?\s*([0-9]+\.?[0-9]*)',
            r'(\d{4})\s*eps\s*[é ä¼°é æ¸¬ä¼°ç®—]*\s*[:ï¼š]\s*([0-9]+\.?[0-9]*)',
            r'eps\s*(\d{4})\s*[:ï¼š]\s*([0-9]+\.?[0-9]*)',
            r'å¹³å‡å€¼[^|]*(\d{4})[^|]*\|\s*([0-9]+\.?[0-9]*)',
            r'ä¸­ä½æ•¸[^|]*(\d{4})[^|]*\|\s*([0-9]+\.?[0-9]*)',
        ]
        
        self.target_price_patterns = [
            r'ç›®æ¨™åƒ¹\s*[:ï¼šç‚º]\s*([0-9]+\.?[0-9]*)\s*å…ƒ',
            r'é ä¼°ç›®æ¨™åƒ¹\s*[:ï¼šç‚º]\s*([0-9]+\.?[0-9]*)\s*å…ƒ',
            r'target\s*price\s*[:ï¼š]\s*([0-9]+\.?[0-9]*)',
        ]
        
        self.analyst_patterns = [
            r'å…±\s*(\d+)\s*ä½åˆ†æå¸«',
            r'(\d+)\s*ä½åˆ†æå¸«',
            r'(\d+)\s*analysts?',
        ]

        # ğŸ”§ è¼‰å…¥è§€å¯Ÿåå–®ä¸¦é€²è¡Œåš´æ ¼é©—è­‰
        self.watch_list_mapping = self._load_watch_list_mapping_enhanced()
        self.validation_enabled = len(self.watch_list_mapping) > 0
        
        print(f"ğŸ”§ MDParser v{self.version} åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“‹ è§€å¯Ÿåå–®é©—è­‰: {'å•Ÿç”¨' if self.validation_enabled else 'åœç”¨'} ({len(self.watch_list_mapping)} å®¶å…¬å¸)")

    def _load_watch_list_mapping_enhanced(self) -> Dict[str, str]:
        """ğŸ†• v3.6.1 å¢å¼·çš„è§€å¯Ÿåå–®è¼‰å…¥"""
        mapping = {}
        
        possible_paths = [
            'StockID_TWSE_TPEX.csv',
            '../StockID_TWSE_TPEX.csv',
            '../../StockID_TWSE_TPEX.csv',
            'data/StockID_TWSE_TPEX.csv',
            '../data/StockID_TWSE_TPEX.csv',
            'watchlist.csv',
            '../watchlist.csv'
        ]
        
        for csv_path in possible_paths:
            if os.path.exists(csv_path):
                try:
                    print(f"ğŸ” å˜—è©¦è¼‰å…¥è§€å¯Ÿåå–®: {csv_path}")
                    
                    # ğŸ”§ ä½¿ç”¨å¤šç¨®ç·¨ç¢¼å˜—è©¦è®€å–
                    encodings = ['utf-8', 'utf-8-sig', 'big5', 'gbk', 'cp950']
                    df = None
                    
                    for encoding in encodings:
                        try:
                            df = pd.read_csv(csv_path, header=None, names=['code', 'name'], encoding=encoding)
                            print(f"âœ… æˆåŠŸä½¿ç”¨ {encoding} ç·¨ç¢¼è®€å–")
                            break
                        except UnicodeDecodeError:
                            continue
                        except Exception as e:
                            print(f"âš ï¸ ä½¿ç”¨ {encoding} ç·¨ç¢¼è®€å–å¤±æ•—: {e}")
                            continue
                    
                    if df is None:
                        print(f"âŒ ç„¡æ³•ä½¿ç”¨ä»»ä½•ç·¨ç¢¼è®€å– {csv_path}")
                        continue
                    
                    # ğŸ”§ åš´æ ¼é©—è­‰å’Œæ¸…ç†æ•¸æ“š
                    valid_count = 0
                    invalid_count = 0
                    duplicate_count = 0
                    
                    for idx, row in df.iterrows():
                        try:
                            # æå–ä¸¦æ¸…ç†ä»£è™Ÿå’Œåç¨±
                            code = str(row['code']).strip()
                            name = str(row['name']).strip()
                            
                            # ğŸ”§ åš´æ ¼é©—è­‰å…¬å¸ä»£è™Ÿæ ¼å¼
                            if not self._is_valid_company_code(code):
                                print(f"âš ï¸ ç„¡æ•ˆå…¬å¸ä»£è™Ÿæ ¼å¼: '{code}' (ç¬¬{idx+1}è¡Œ)")
                                invalid_count += 1
                                continue
                            
                            # ğŸ”§ é©—è­‰å…¬å¸åç¨±
                            if not self._is_valid_company_name(name):
                                print(f"âš ï¸ ç„¡æ•ˆå…¬å¸åç¨±: '{name}' (ä»£è™Ÿ: {code}, ç¬¬{idx+1}è¡Œ)")
                                invalid_count += 1
                                continue
                            
                            # ğŸ”§ æª¢æŸ¥é‡è¤‡ä»£è™Ÿ
                            if code in mapping:
                                print(f"âš ï¸ é‡è¤‡å…¬å¸ä»£è™Ÿ: {code} - åŸæœ‰: {mapping[code]}, æ–°çš„: {name}")
                                duplicate_count += 1
                                continue
                            
                            # æ·»åŠ åˆ°æ˜ å°„
                            mapping[code] = name
                            valid_count += 1
                            
                        except Exception as e:
                            print(f"âŒ è™•ç†ç¬¬{idx+1}è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                            invalid_count += 1
                            continue
                    
                    # ğŸ”§ é©—è­‰è¼‰å…¥çµæœ
                    total_rows = len(df)
                    if valid_count == 0:
                        print(f"âŒ è§€å¯Ÿåå–®ç„¡æœ‰æ•ˆæ•¸æ“š: {csv_path}")
                        continue
                    
                    print(f"ğŸ“Š è§€å¯Ÿåå–®è¼‰å…¥çµ±è¨ˆ:")
                    print(f"   æª”æ¡ˆ: {csv_path}")
                    print(f"   ç¸½è¡Œæ•¸: {total_rows}")
                    print(f"   æœ‰æ•ˆæ•¸æ“š: {valid_count}")
                    print(f"   ç„¡æ•ˆæ•¸æ“š: {invalid_count}")
                    print(f"   é‡è¤‡æ•¸æ“š: {duplicate_count}")
                    print(f"   æˆåŠŸç‡: {valid_count/total_rows*100:.1f}%")
                    
                    # ğŸ”§ é¡å¤–é©—è­‰ï¼šæª¢æŸ¥æ˜¯å¦æœ‰å·²çŸ¥çš„æ¸¬è©¦å…¬å¸
                    self._validate_watch_list_content_enhanced(mapping)
                    
                    return mapping
                    
                except Exception as e:
                    print(f"âŒ è®€å–è§€å¯Ÿåå–®å¤±æ•— {csv_path}: {e}")
                    continue
        
        # ğŸ”§ å¦‚æœè§€å¯Ÿåå–®è¼‰å…¥å¤±æ•—ï¼Œè¿”å›ç©ºå­—å…¸ä½†ä¸åœæ­¢ç³»çµ±
        print("âŒ æ‰€æœ‰è§€å¯Ÿåå–®è¼‰å…¥å˜—è©¦å‡å¤±æ•—")
        print("âš ï¸ ç³»çµ±å°‡åœ¨ç„¡é©—è­‰æ¨¡å¼ä¸‹é‹è¡Œ")
        return {}

    def parse_md_file(self, file_path: str) -> Dict[str, Any]:
        """ğŸ†• v3.6.1 å¢å¼·ç‰ˆ MD æª”æ¡ˆè§£æ"""
        try:
            # è®€å–æª”æ¡ˆå…§å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åŸºæœ¬æª”æ¡ˆè³‡è¨Š
            file_info = self._extract_file_info(file_path)
            company_code = file_info.get('company_code', '')
            company_name = file_info.get('company_name', '')
            
            # ğŸ†• å¢å¼·çš„ YAML front matter è§£æ
            yaml_data = self._extract_yaml_frontmatter_enhanced(content)
            
            # ğŸ†• æŸ¥è©¢æ¨¡å¼æå– (v3.6.1 æ ¸å¿ƒåŠŸèƒ½)
            search_keywords = self._extract_search_keywords_enhanced(content, yaml_data)
            
            # ğŸ”§ æ ¸å¿ƒé©—è­‰ï¼šå°ç…§è§€å¯Ÿåå–® (å¢å¼·ç‰ˆ)
            validation_result = self._validate_against_watch_list_enhanced(company_code, company_name)
            
            # åŸæœ‰åŠŸèƒ½ï¼šæ—¥æœŸæå–
            content_date = self._extract_content_date_bulletproof(content)
            extraction_status = "content_extraction" if content_date else "no_date_found"
            
            # åŸæœ‰åŠŸèƒ½ï¼šEPS ç­‰è³‡æ–™æå–
            eps_data = self._extract_eps_data(content)
            eps_stats = self._calculate_eps_statistics(eps_data)
            target_price = self._extract_target_price(content)
            analyst_count = self._extract_analyst_count(content)
            data_richness = self._calculate_data_richness(eps_stats, target_price, analyst_count)
            
            # ğŸ†• å…§å®¹å“è³ªè©•ä¼° (v3.6.1)
            content_quality_metrics = self._assess_content_quality(content)
            
            # çµ„åˆçµæœ
            result = {
                # åŸºæœ¬è³‡è¨Š
                'filename': os.path.basename(file_path),
                'company_code': company_code,
                'company_name': company_name,
                'data_source': file_info.get('data_source', ''),
                'file_mtime': datetime.fromtimestamp(os.path.getmtime(file_path)),
                
                # æ—¥æœŸè³‡è¨Š
                'content_date': content_date,
                'extracted_date': yaml_data.get('extracted_date'),
                'filename_date': file_info.get('parsed_timestamp'),
                
                # EPS è³‡æ–™
                **eps_stats,
                
                # å…¶ä»–è²¡å‹™è³‡æ–™
                'target_price': target_price,
                'analyst_count': analyst_count,
                
                # è³‡æ–™ç‹€æ…‹
                'has_eps_data': len(eps_data) > 0,
                'has_target_price': target_price is not None,
                'has_analyst_info': analyst_count > 0,
                'data_richness_score': data_richness,
                
                # ğŸ†• v3.6.1 å¢å¼·åŠŸèƒ½
                'search_keywords': search_keywords,  # é—œéµï¼æŸ¥è©¢æ¨¡å¼åˆ†æéœ€è¦
                'content_quality_metrics': content_quality_metrics,
                
                # YAML è³‡æ–™
                'yaml_data': yaml_data,
                
                # ğŸ”§ å¢å¼·ç‰ˆé©—è­‰çµæœ
                'validation_result': validation_result,
                'content_validation_passed': validation_result['overall_status'] == 'valid',
                'validation_warnings': validation_result.get('warnings', []),
                'validation_errors': validation_result.get('errors', []),
                'validation_enabled': self.validation_enabled,
                
                # åŸå§‹å…§å®¹
                'content': content,
                'content_length': len(content),
                'parsed_at': datetime.now(),
                
                # èª¿è©¦è³‡è¨Š
                'parser_version': self.version,
                'date_extraction_method': extraction_status,
                'debug_info': self._get_debug_info_enhanced(content, content_date, search_keywords)
            }
            
            return result
            
        except Exception as e:
            print(f"âŒ è§£ææª”æ¡ˆå¤±æ•— {file_path}: {e}")
            return self._create_empty_result_enhanced(file_path, str(e))

    def _extract_search_keywords_enhanced(self, content: str, yaml_data: Dict) -> List[str]:
        """ğŸ†• v3.6.1 å¢å¼·çš„æœå°‹é—œéµå­—æå–"""
        keywords = []
        
        try:
            # 1. å¾ YAML metadata ä¸­æå–
            for field_name in ['search_query', 'keywords', 'search_terms', 'query_pattern', 'original_query']:
                field_value = yaml_data.get(field_name, '')
                if field_value and isinstance(field_value, str):
                    # æ¸…ç†å’Œåˆ†å‰²é—œéµå­—
                    cleaned_keywords = self._clean_and_split_keywords(field_value)
                    keywords.extend(cleaned_keywords)
            
            # 2. å¾å…§å®¹çš„ metadata ä¸­æå–
            if content.startswith('---'):
                try:
                    end_pos = content.find('---', 3)
                    if end_pos != -1:
                        yaml_content = content[3:end_pos].strip()
                        
                        # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå–æŸ¥è©¢ç›¸é—œæ¬„ä½
                        for field_name, pattern in self.metadata_patterns.items():
                            matches = re.findall(pattern, yaml_content, re.MULTILINE | re.IGNORECASE)
                            for match in matches:
                                if match.strip():
                                    cleaned_keywords = self._clean_and_split_keywords(match.strip())
                                    keywords.extend(cleaned_keywords)
                except Exception as e:
                    print(f"âš ï¸ YAML metadata è§£æå¤±æ•—: {e}")
            
            # 3. å»é‡ä¸¦éæ¿¾
            unique_keywords = []
            seen_keywords = set()
            
            for keyword in keywords:
                keyword_lower = keyword.lower().strip()
                if (keyword_lower not in seen_keywords and 
                    self._is_valid_keyword(keyword_lower) and
                    len(keyword_lower) >= 2):
                    unique_keywords.append(keyword.strip())
                    seen_keywords.add(keyword_lower)
            
            # 4. æŒ‰é‡è¦æ€§æ’åº
            sorted_keywords = self._sort_keywords_by_importance(unique_keywords)
            
            return sorted_keywords[:20]  # é™åˆ¶æœ€å¤š20å€‹é—œéµå­—
            
        except Exception as e:
            print(f"âš ï¸ æœå°‹é—œéµå­—æå–å¤±æ•—: {e}")
            return []

    def _clean_and_split_keywords(self, text: str) -> List[str]:
        """æ¸…ç†å’Œåˆ†å‰²é—œéµå­—"""
        if not text or not isinstance(text, str):
            return []
        
        # ç§»é™¤å¸¸è¦‹çš„æœå°‹é‹ç®—ç¬¦
        cleaned = re.sub(r'[+\-"():]', ' ', text)
        
        # åˆ†å‰²é—œéµå­— (æ”¯æ´å¤šç¨®åˆ†éš”ç¬¦)
        keywords = re.split(r'[,ï¼Œ;ï¼›\s]+', cleaned)
        
        # æ¸…ç†æ¯å€‹é—œéµå­—
        cleaned_keywords = []
        for keyword in keywords:
            keyword = keyword.strip()
            if keyword and len(keyword) >= 2:
                cleaned_keywords.append(keyword)
        
        return cleaned_keywords

    def _is_valid_keyword(self, keyword: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºæœ‰æ•ˆé—œéµå­—"""
        if not keyword or len(keyword) < 2:
            return False
        
        # åœç”¨è©åˆ—è¡¨
        stop_words = {
            'çš„', 'å’Œ', 'èˆ‡', 'æˆ–', 'åŠ', 'åœ¨', 'ç‚º', 'æ˜¯', 'æœ‰', 'æ­¤', 'å°‡', 'æœƒ', 'äº†', 'å°±', 'éƒ½',
            'and', 'or', 'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'with'
        }
        
        if keyword.lower() in stop_words:
            return False
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºç´”æ•¸å­—æˆ–ç¬¦è™Ÿ
        if keyword.isdigit() or not re.search(r'[a-zA-Z\u4e00-\u9fff]', keyword):
            return False
        
        return True

    def _sort_keywords_by_importance(self, keywords: List[str]) -> List[str]:
        """æŒ‰é‡è¦æ€§æ’åºé—œéµå­—"""
        def get_importance_score(keyword: str) -> int:
            score = 0
            keyword_lower = keyword.lower()
            
            # é«˜é‡è¦æ€§é—œéµå­—
            high_importance = ['factset', 'eps', 'ç›®æ¨™åƒ¹', 'åˆ†æå¸«', 'é ä¼°', 'bloomberg', 'reuters']
            if any(imp in keyword_lower for imp in high_importance):
                score += 10
            
            # ä¸­é‡è¦æ€§é—œéµå­—
            medium_importance = ['è²¡å ±', 'ç‡Ÿæ”¶', 'ç²åˆ©', 'è‚¡åƒ¹', 'è©•ç­‰', 'analyst', 'forecast', 'estimate']
            if any(imp in keyword_lower for imp in medium_importance):
                score += 5
            
            # å…¬å¸åç¨±å’Œä»£è™Ÿ
            if re.search(r'\d{4}', keyword) or len(keyword) <= 4:
                score += 8
            
            # ä¸­æ–‡é—œéµå­—ç¨å¾®æé«˜å„ªå…ˆç´š
            if re.search(r'[\u4e00-\u9fff]', keyword):
                score += 2
            
            return score
        
        return sorted(keywords, key=get_importance_score, reverse=True)

    def _assess_content_quality(self, content: str) -> Dict[str, Any]:
        """ğŸ†• v3.6.1 è©•ä¼°å…§å®¹å“è³ª"""
        metrics = {
            'content_length': len(content),
            'paragraph_count': len(content.split('\n\n')),
            'line_count': len(content.split('\n')),
            'chinese_char_ratio': 0,
            'financial_keyword_count': 0,
            'table_count': 0,
            'number_count': 0,
            'has_metadata': content.startswith('---'),
            'structure_score': 0
        }
        
        try:
            # ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', content))
            if metrics['content_length'] > 0:
                metrics['chinese_char_ratio'] = round(chinese_chars / metrics['content_length'], 3)
            
            # è²¡å‹™é—œéµå­—è¨ˆæ•¸
            financial_keywords = [
                'eps', 'æ¯è‚¡ç›ˆé¤˜', 'ç‡Ÿæ”¶', 'ç²åˆ©', 'æ·¨åˆ©', 'æ¯›åˆ©', 'ç›®æ¨™åƒ¹', 'åˆ†æå¸«', 
                'é ä¼°', 'è©•ç­‰', 'factset', 'bloomberg', 'è‚¡åƒ¹', 'å¸‚å€¼'
            ]
            for keyword in financial_keywords:
                metrics['financial_keyword_count'] += len(re.findall(keyword, content, re.IGNORECASE))
            
            # è¡¨æ ¼è¨ˆæ•¸
            metrics['table_count'] = content.count('|')
            
            # æ•¸å­—è¨ˆæ•¸
            metrics['number_count'] = len(re.findall(r'\d+\.?\d*', content))
            
            # çµæ§‹è©•åˆ† (0-10)
            structure_score = 0
            if metrics['has_metadata']:
                structure_score += 3
            if metrics['paragraph_count'] >= 3:
                structure_score += 2
            if metrics['table_count'] > 0:
                structure_score += 2
            if metrics['financial_keyword_count'] >= 5:
                structure_score += 2
            if metrics['content_length'] >= 1000:
                structure_score += 1
            
            metrics['structure_score'] = min(structure_score, 10)
            
        except Exception as e:
            print(f"âš ï¸ å…§å®¹å“è³ªè©•ä¼°å¤±æ•—: {e}")
        
        return metrics

    def _extract_yaml_frontmatter_enhanced(self, content: str) -> Dict[str, Any]:
        """ğŸ†• v3.6.1 å¢å¼·çš„ YAML front matter æå–"""
        try:
            if content.startswith('---'):
                end_pos = content.find('---', 3)
                if end_pos != -1:
                    yaml_content = content[3:end_pos].strip()
                    
                    # å˜—è©¦ä½¿ç”¨ yaml è§£æå™¨
                    try:
                        yaml_data = yaml.safe_load(yaml_content) or {}
                        
                        # é¡å¤–æ¸…ç†å’Œé©—è­‰
                        cleaned_data = {}
                        for key, value in yaml_data.items():
                            if isinstance(value, str):
                                cleaned_data[key] = value.strip()
                            else:
                                cleaned_data[key] = value
                        
                        return cleaned_data
                        
                    except yaml.YAMLError as e:
                        print(f"âš ï¸ YAML è§£æå¤±æ•—ï¼Œå˜—è©¦æ‰‹å‹•è§£æ: {e}")
                        
                        # æ‰‹å‹•è§£æé—œéµæ¬„ä½
                        manual_data = {}
                        for field_name, pattern in self.metadata_patterns.items():
                            matches = re.findall(pattern, yaml_content, re.MULTILINE | re.IGNORECASE)
                            if matches:
                                manual_data[field_name] = matches[0].strip()
                        
                        return manual_data
                        
        except Exception as e:
            print(f"âš ï¸ YAML frontmatter æå–å¤±æ•—: {e}")
        
        return {}

    def _validate_against_watch_list_enhanced(self, company_code: str, company_name: str) -> Dict[str, Any]:
        """ğŸ†• v3.6.1 å¢å¼·çš„è§€å¯Ÿåå–®é©—è­‰"""
        
        validation_result = {
            'overall_status': 'valid',
            'warnings': [],
            'errors': [],
            'confidence_score': 10.0,
            'validation_method': 'enhanced_v3.6.1',
            'detailed_checks': []
        }
        
        # ğŸ”§ å¦‚æœè§€å¯Ÿåå–®æœªè¼‰å…¥ï¼Œè¨˜éŒ„ä½†ä¸é˜»æ­¢è™•ç†
        if not self.validation_enabled:
            validation_result['warnings'].append("è§€å¯Ÿåå–®æœªè¼‰å…¥ï¼Œè·³éé©—è­‰")
            validation_result['confidence_score'] = 5.0
            validation_result['validation_method'] = 'disabled'
            validation_result['detailed_checks'].append("é©—è­‰åŠŸèƒ½å·²åœç”¨")
            print(f"âš ï¸ è§€å¯Ÿåå–®é©—è­‰å·²åœç”¨: {company_code} - {company_name}")
            return validation_result
        
        # ğŸ”§ åš´æ ¼æª¢æŸ¥è¼¸å…¥åƒæ•¸
        if not company_code or not company_name:
            validation_result['overall_status'] = 'error'
            validation_result['confidence_score'] = 0.0
            error_msg = f"å…¬å¸ä»£è™Ÿæˆ–åç¨±ç‚ºç©º: ä»£è™Ÿ='{company_code}', åç¨±='{company_name}'"
            validation_result['errors'].append(error_msg)
            validation_result['detailed_checks'].append("è¼¸å…¥åƒæ•¸æª¢æŸ¥å¤±æ•—")
            print(f"âŒ åƒæ•¸éŒ¯èª¤: {error_msg}")
            return validation_result
        
        # ğŸ”§ æ¸…ç†è¼¸å…¥æ•¸æ“š
        clean_code = str(company_code).strip().strip('\'"')
        clean_name = str(company_name).strip()
        
        # ğŸ”§ æª¢æŸ¥ 1: å…¬å¸ä»£è™Ÿæ ¼å¼é©—è­‰
        validation_result['detailed_checks'].append("æª¢æŸ¥å…¬å¸ä»£è™Ÿæ ¼å¼")
        if not self._is_valid_company_code(clean_code):
            validation_result['overall_status'] = 'error'
            validation_result['confidence_score'] = 0.0
            error_msg = f"å…¬å¸ä»£è™Ÿæ ¼å¼ç„¡æ•ˆ: '{clean_code}'"
            validation_result['errors'].append(error_msg)
            validation_result['detailed_checks'].append("å…¬å¸ä»£è™Ÿæ ¼å¼æª¢æŸ¥å¤±æ•—")
            print(f"âŒ ä»£è™Ÿæ ¼å¼ç„¡æ•ˆ: {clean_code}")
            return validation_result
        
        # ğŸ”§ æª¢æŸ¥ 2: å…¬å¸ä»£è™Ÿæ˜¯å¦åœ¨è§€å¯Ÿåå–®ä¸­
        validation_result['detailed_checks'].append("æª¢æŸ¥è§€å¯Ÿåå–®åŒ…å«ç‹€æ…‹")
        if clean_code not in self.watch_list_mapping:
            validation_result['overall_status'] = 'error'
            validation_result['confidence_score'] = 0.0
            error_msg = f"ä»£è™Ÿ{clean_code}ä¸åœ¨è§€å¯Ÿåå–®ä¸­ï¼Œä¸å…è¨±è™•ç†"
            validation_result['errors'].append(error_msg)
            validation_result['detailed_checks'].append("è§€å¯Ÿåå–®åŒ…å«æª¢æŸ¥å¤±æ•—")
            print(f"âŒ ä¸åœ¨è§€å¯Ÿåå–®: {clean_code}")
            
            # ğŸ”§ é¡å¤–ä¿¡æ¯ï¼šæä¾›ç›¸ä¼¼çš„ä»£è™Ÿå»ºè­°
            similar_codes = self._find_similar_codes(clean_code)
            if similar_codes:
                suggestion_msg = f"ç›¸ä¼¼ä»£è™Ÿå»ºè­°: {', '.join(similar_codes[:3])}"
                validation_result['warnings'].append(suggestion_msg)
                validation_result['detailed_checks'].append(f"æ‰¾åˆ°ç›¸ä¼¼ä»£è™Ÿ: {len(similar_codes)}å€‹")
            
            return validation_result
        
        # ğŸ”§ æª¢æŸ¥ 3: å…¬å¸åç¨±æ˜¯å¦èˆ‡è§€å¯Ÿåå–®ä¸€è‡´ (å¢å¼·æ¯”è¼ƒ)
        validation_result['detailed_checks'].append("æª¢æŸ¥å…¬å¸åç¨±ä¸€è‡´æ€§")
        correct_name = self.watch_list_mapping[clean_code]
        
        # ğŸ”§ å¤šå±¤æ¬¡åç¨±æ¯”è¼ƒ
        name_match = self._compare_company_names_enhanced(clean_name, correct_name)
        
        if not name_match['is_match']:
            validation_result['overall_status'] = 'error'
            validation_result['confidence_score'] = 0.0
            error_msg = f"å…¬å¸åç¨±ä¸ç¬¦è§€å¯Ÿåå–®ï¼šæª”æ¡ˆç‚º{clean_name}({clean_code})ï¼Œè§€å¯Ÿåå–®é¡¯ç¤ºæ‡‰ç‚º{correct_name}({clean_code})"
            validation_result['errors'].append(error_msg)
            validation_result['detailed_checks'].append("å…¬å¸åç¨±ä¸€è‡´æ€§æª¢æŸ¥å¤±æ•—")
            
            # ğŸ”§ é¡å¤–ä¿¡æ¯ï¼šè©³ç´°çš„ä¸åŒ¹é…åˆ†æ
            if name_match['details']:
                validation_result['errors'].append(f"è©³ç´°æ¯”è¼ƒ: {name_match['details']}")
                validation_result['detailed_checks'].append(f"åç¨±æ¯”è¼ƒè©³æƒ…: {name_match['match_type']}")
            
            print(f"âŒ åç¨±ä¸ç¬¦: {clean_code}")
            print(f"   æª”æ¡ˆåç¨±: '{clean_name}'")
            print(f"   è§€å¯Ÿåå–®: '{correct_name}'")
            print(f"   æ¯”è¼ƒè©³æƒ…: {name_match['details']}")
            
            return validation_result
        
        # ğŸ”§ æª¢æŸ¥é€šé
        validation_result['confidence_score'] = name_match['confidence_score']
        validation_result['detailed_checks'].append(f"æ‰€æœ‰æª¢æŸ¥é€šéï¼Œåç¨±åŒ¹é…é¡å‹: {name_match['match_type']}")
        
        if name_match['confidence_score'] < 10.0:
            validation_result['warnings'].append(f"åç¨±åŒ¹é…åº¦: {name_match['confidence_score']}/10")
        
        print(f"âœ… é©—è­‰é€šé: {clean_code} - {clean_name} (ä¿¡å¿ƒåº¦: {name_match['confidence_score']})")
        return validation_result

    def _compare_company_names_enhanced(self, name1: str, name2: str) -> Dict[str, Any]:
        """ğŸ†• v3.6.1 å¢å¼·çš„å…¬å¸åç¨±æ¯”è¼ƒ"""
        comparison_result = {
            'is_match': False,
            'confidence_score': 0.0,
            'details': '',
            'match_type': 'no_match'
        }
        
        # å±¤æ¬¡ 1: å®Œå…¨åŒ¹é…
        if name1 == name2:
            comparison_result.update({
                'is_match': True,
                'confidence_score': 10.0,
                'details': 'å®Œå…¨åŒ¹é…',
                'match_type': 'exact_match'
            })
            return comparison_result
        
        # å±¤æ¬¡ 2: ç§»é™¤ç©ºç™½å¾ŒåŒ¹é…
        clean_name1 = re.sub(r'\s+', '', name1)
        clean_name2 = re.sub(r'\s+', '', name2)
        
        if clean_name1 == clean_name2:
            comparison_result.update({
                'is_match': True,
                'confidence_score': 9.5,
                'details': 'ç§»é™¤ç©ºç™½å¾ŒåŒ¹é…',
                'match_type': 'whitespace_normalized'
            })
            return comparison_result
        
        # å±¤æ¬¡ 3: ç§»é™¤å¸¸è¦‹å¾Œç¶´è©å¾ŒåŒ¹é…
        suffixes = ['è‚¡ä»½æœ‰é™å…¬å¸', 'æœ‰é™å…¬å¸', 'å…¬å¸', 'é›†åœ˜', 'æ§è‚¡', 'Corporation', 'Corp', 'Inc', 'Ltd', 'Group']
        
        def remove_suffixes(name):
            for suffix in suffixes:
                if name.endswith(suffix):
                    name = name[:-len(suffix)].strip()
            return name
        
        core_name1 = remove_suffixes(clean_name1)
        core_name2 = remove_suffixes(clean_name2)
        
        if core_name1 == core_name2:
            comparison_result.update({
                'is_match': True,
                'confidence_score': 9.0,
                'details': 'ç§»é™¤å¾Œç¶´è©å¾ŒåŒ¹é…',
                'match_type': 'suffix_removed'
            })
            return comparison_result
        
        # å±¤æ¬¡ 4: éƒ¨åˆ†åŒ…å«åŒ¹é…
        if core_name1 in core_name2 or core_name2 in core_name1:
            comparison_result.update({
                'is_match': True,
                'confidence_score': 7.0,
                'details': f'éƒ¨åˆ†åŒ…å«åŒ¹é…: "{core_name1}" vs "{core_name2}"',
                'match_type': 'partial_contain'
            })
            return comparison_result
        
        # å±¤æ¬¡ 5: ç·¨è¼¯è·é›¢åŒ¹é… (æ–°å¢)
        similarity = self._calculate_string_similarity(core_name1, core_name2)
        if similarity >= 0.8:  # 80% ç›¸ä¼¼åº¦
            comparison_result.update({
                'is_match': True,
                'confidence_score': round(similarity * 6, 1),  # æœ€é«˜6åˆ†
                'details': f'é«˜ç›¸ä¼¼åº¦åŒ¹é…: {similarity:.2f}',
                'match_type': 'high_similarity'
            })
            return comparison_result
        
        # ä¸åŒ¹é…
        comparison_result.update({
            'details': f'å®Œå…¨ä¸åŒ¹é…: "{name1}" vs "{name2}" (ç›¸ä¼¼åº¦: {similarity:.2f})',
            'match_type': 'no_match'
        })
        return comparison_result

    def _calculate_string_similarity(self, str1: str, str2: str) -> float:
        """è¨ˆç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ (ç°¡åŒ–ç‰ˆç·¨è¼¯è·é›¢)"""
        try:
            if not str1 or not str2:
                return 0.0
            
            if str1 == str2:
                return 1.0
            
            # ç°¡åŒ–çš„ç›¸ä¼¼åº¦è¨ˆç®—
            longer = str1 if len(str1) > len(str2) else str2
            shorter = str2 if len(str1) > len(str2) else str1
            
            # è¨ˆç®—å…±åŒå­—ç¬¦
            common_chars = sum(1 for char in shorter if char in longer)
            similarity = common_chars / len(longer)
            
            return similarity
            
        except Exception:
            return 0.0

    def _get_debug_info_enhanced(self, content: str, extracted_date: Optional[str], 
                                search_keywords: List[str]) -> Dict[str, Any]:
        """ğŸ†• v3.6.1 å¢å¼·çš„èª¿è©¦è³‡è¨Š"""
        return {
            'content_preview': content[:200] + "..." if len(content) > 200 else content,
            'extracted_date': extracted_date,
            'yaml_detected': content.startswith('---'),
            'content_length': len(content),
            'search_keywords_count': len(search_keywords),
            'search_keywords_preview': search_keywords[:5],
            'watch_list_loaded': self.validation_enabled,
            'watch_list_size': len(self.watch_list_mapping),
            'parser_version': self.version,
            'content_structure': {
                'has_metadata': content.startswith('---'),
                'paragraph_count': len(content.split('\n\n')),
                'line_count': len(content.split('\n')),
                'chinese_detected': bool(re.search(r'[\u4e00-\u9fff]', content))
            }
        }

    def _create_empty_result_enhanced(self, file_path: str, error_msg: str) -> Dict[str, Any]:
        """ğŸ†• v3.6.1 å¢å¼·çš„ç©ºçµæœå»ºç«‹"""
        file_info = self._extract_file_info(file_path)
        
        return {
            'filename': os.path.basename(file_path),
            'company_code': file_info.get('company_code'),
            'company_name': file_info.get('company_name'),
            'data_source': file_info.get('data_source'),
            'file_mtime': datetime.fromtimestamp(os.path.getmtime(file_path)) if os.path.exists(file_path) else None,
            'content_date': None,
            'eps_2025_high': None, 'eps_2025_low': None, 'eps_2025_avg': None,
            'eps_2026_high': None, 'eps_2026_low': None, 'eps_2026_avg': None,
            'eps_2027_high': None, 'eps_2027_low': None, 'eps_2027_avg': None,
            'target_price': None,
            'analyst_count': 0,
            'has_eps_data': False,
            'has_target_price': False,
            'has_analyst_info': False,
            'data_richness_score': 0.0,
            'search_keywords': [],  # é‡è¦ï¼
            'content_quality_metrics': {},
            'yaml_data': {},
            'content': '',
            'content_length': 0,
            'parsed_at': datetime.now(),
            'parser_version': self.version,
            'error': error_msg,
            'date_extraction_method': 'error',
            'validation_result': {
                'overall_status': 'error', 
                'errors': [error_msg],
                'validation_method': 'error_state'
            },
            'content_validation_passed': False,
            'validation_warnings': [],
            'validation_errors': [error_msg],
            'validation_enabled': self.validation_enabled
        }

    # ä¿ç•™åŸæœ‰çš„ç§æœ‰æ–¹æ³• (ç•¥å¾®èª¿æ•´ä»¥æ”¯æ´å¢å¼·åŠŸèƒ½)
    def _is_valid_company_code(self, code: str) -> bool:
        """é©—è­‰å…¬å¸ä»£è™Ÿæ ¼å¼"""
        if not code or code in ['nan', 'NaN', 'null', 'None', '', 'NULL']:
            return False
        
        clean_code = code.strip().strip('\'"')
        
        if not clean_code.isdigit():
            return False
            
        if len(clean_code) != 4:
            return False
        
        try:
            code_num = int(clean_code)
            if not (1000 <= code_num <= 9999):
                return False
        except ValueError:
            return False
        
        return True

    def _is_valid_company_name(self, name: str) -> bool:
        """é©—è­‰å…¬å¸åç¨±"""
        if not name or name in ['nan', 'NaN', 'null', 'None', '', 'NULL']:
            return False
        
        clean_name = name.strip()
        
        if len(clean_name) < 1 or len(clean_name) > 30:
            return False
        
        invalid_chars = ['|', '\t', '\n', '\r', '\x00']
        if any(char in clean_name for char in invalid_chars):
            return False
        
        return True

    def _find_similar_codes(self, target_code: str) -> List[str]:
        """å°‹æ‰¾ç›¸ä¼¼çš„å…¬å¸ä»£è™Ÿ"""
        if not self.watch_list_mapping:
            return []
        
        similar_codes = []
        target_num = None
        
        try:
            target_num = int(target_code)
        except ValueError:
            return similar_codes
        
        for code in self.watch_list_mapping.keys():
            try:
                code_num = int(code)
                if abs(code_num - target_num) <= 10:
                    similar_codes.append(code)
            except ValueError:
                continue
        
        return sorted(similar_codes)

    def _validate_watch_list_content_enhanced(self, mapping: Dict[str, str]):
        """ğŸ†• v3.6.1 å¢å¼·çš„è§€å¯Ÿåå–®å…§å®¹é©—è­‰"""
        if not mapping:
            return
        
        # æª¢æŸ¥æ˜¯å¦æœ‰å¸¸è¦‹çš„æ¸¬è©¦å…¬å¸
        test_companies = {
            '2330': 'å°ç©é›»',
            '2317': 'é´»æµ·', 
            '2454': 'è¯ç™¼ç§‘',
            '2882': 'åœ‹æ³°é‡‘',
            '2412': 'ä¸­è¯é›»'
        }
        
        found_test_companies = 0
        for code, expected_name in test_companies.items():
            if code in mapping:
                actual_name = mapping[code]
                name_match = self._compare_company_names_enhanced(actual_name, expected_name)
                if name_match['is_match']:
                    found_test_companies += 1
                    print(f"âœ… æ‰¾åˆ°æ¸¬è©¦å…¬å¸: {code} - {actual_name} (åŒ¹é…é¡å‹: {name_match['match_type']})")
                else:
                    print(f"âš ï¸ æ¸¬è©¦å…¬å¸åç¨±ä¸ç¬¦: {code} - æœŸæœ›:{expected_name}, å¯¦éš›:{actual_name}")
        
        # çµ±è¨ˆåˆ†æ
        code_ranges = self._analyze_code_ranges(mapping)
        print(f"ğŸ“Š è§€å¯Ÿåå–®ä»£è™Ÿåˆ†å¸ƒ: {code_ranges}")
        
        if found_test_companies == 0:
            print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å·²çŸ¥æ¸¬è©¦å…¬å¸ï¼Œè«‹æª¢æŸ¥è§€å¯Ÿåå–®å…§å®¹")
        else:
            print(f"âœ… æ‰¾åˆ° {found_test_companies}/{len(test_companies)} å€‹æ¸¬è©¦å…¬å¸")

    def _analyze_code_ranges(self, mapping: Dict[str, str]) -> Dict[str, int]:
        """åˆ†æå…¬å¸ä»£è™Ÿç¯„åœåˆ†å¸ƒ"""
        ranges = {
            '1000-1999': 0,
            '2000-2999': 0, 
            '3000-3999': 0,
            '4000-4999': 0,
            '5000-5999': 0,
            '6000-6999': 0,
            '7000-7999': 0,
            '8000-8999': 0,
            '9000-9999': 0
        }
        
        for code in mapping.keys():
            try:
                code_num = int(code)
                range_key = f"{(code_num // 1000) * 1000}-{(code_num // 1000) * 1000 + 999}"
                if range_key in ranges:
                    ranges[range_key] += 1
            except ValueError:
                continue
        
        return ranges

    # ä¿ç•™åŸæœ‰æ–¹æ³• (ç•¥ä½œèª¿æ•´ä»¥æ”¯æ´æ–°åŠŸèƒ½)
    # [åŸæœ‰çš„æ‰€æœ‰å…¶ä»–æ–¹æ³•ä¿æŒä¸è®Šï¼Œåªæ˜¯ç¢ºä¿èˆ‡æ–°åŠŸèƒ½ç›¸å®¹]
    
    def _extract_content_date_bulletproof(self, content: str) -> Optional[str]:
        """çµ•å°é˜²å½ˆçš„æ—¥æœŸæå– - æ’é™¤ YAML frontmatter"""
        actual_content = self._get_content_without_yaml(content)
        found_dates = []
        
        for i, pattern in enumerate(self.date_patterns):
            matches = re.findall(pattern, actual_content, re.MULTILINE | re.DOTALL)
            
            if matches:
                for match in matches:
                    try:
                        if len(match) >= 3:
                            year, month, day = match[0], match[1], match[2]
                            
                            if self._validate_date(year, month, day):
                                date_str = f"{year}/{int(month)}/{int(day)}"
                                confidence = self._calculate_date_confidence(pattern, match, actual_content, i)
                                
                                found_dates.append({
                                    'date': date_str,
                                    'pattern_index': i,
                                    'pattern': pattern,
                                    'confidence': confidence,
                                    'match': match
                                })
                                
                    except (ValueError, IndexError) as e:
                        continue
        
        if found_dates:
            found_dates.sort(key=lambda x: x['confidence'], reverse=True)
            best_date = found_dates[0]
            return best_date['date']
        
        return None

    def _get_content_without_yaml(self, content: str) -> str:
        """ç§»é™¤ YAML frontmatterï¼Œåªè¿”å›å¯¦éš›å…§å®¹"""
        try:
            if content.startswith('---'):
                end_pos = content.find('---', 3)
                if end_pos != -1:
                    actual_content = content[end_pos + 3:].strip()
                    return actual_content
        except Exception as e:
            pass
        return content

    # [å…¶ä»–åŸæœ‰æ–¹æ³•ä¿æŒä¸è®Š...]
    def _validate_date(self, year: str, month: str, day: str) -> bool:
        """é©—è­‰æ—¥æœŸçš„åˆç†æ€§"""
        try:
            y, m, d = int(year), int(month), int(day)
            if not (2020 <= y <= 2030):
                return False
            if not (1 <= m <= 12):
                return False
            if not (1 <= d <= 31):
                return False
            datetime(y, m, d)
            return True
        except (ValueError, TypeError):
            return False

    def _calculate_date_confidence(self, pattern: str, match: tuple, content: str, pattern_index: int) -> float:
        """è¨ˆç®—æ—¥æœŸåŒ¹é…çš„å¯ä¿¡åº¦"""
        confidence = 5.0
        
        if pattern_index == 0:
            confidence += 6.0
        elif pattern_index == 1:
            confidence += 5.5
        elif pattern_index == 2:
            confidence += 5.0
        elif pattern_index <= 6:
            confidence += 4.0
        elif 'cmoney' in content.lower() or 'CMoney' in content:
            confidence += 2.5
        elif 'é‰…äº¨ç¶²' in pattern:
            confidence += 2.0
        elif 'å¹´' in pattern and 'æœˆ' in pattern:
            confidence += 1.5
        elif '-' in pattern:
            confidence += 1.0
        
        match_text = ''.join(match)
        position = content.find(match_text)
        if position != -1:
            if position < len(content) * 0.1:
                confidence += 2.0
            elif position < len(content) * 0.3:
                confidence += 1.0
        
        try:
            year = int(match[0])
            current_year = datetime.now().year
            if year == current_year:
                confidence += 1.5
            elif year == current_year - 1:
                confidence += 1.0
        except:
            pass
        
        return confidence

    def _extract_file_info(self, file_path: str) -> Dict[str, Any]:
        """å¾æª”æ¡ˆè·¯å¾‘æå–åŸºæœ¬è³‡è¨Š"""
        filename = os.path.basename(file_path)
        name_without_ext = filename.replace('.md', '')
        parts = name_without_ext.split('_')
        
        result = {
            'company_code': None,
            'company_name': None,
            'data_source': None,
            'timestamp': None,
            'parsed_timestamp': None
        }
        
        if len(parts) >= 2:
            if parts[0].isdigit() and len(parts[0]) == 4:
                result['company_code'] = parts[0]
            
            if len(parts) > 1:
                result['company_name'] = parts[1]
            
            if len(parts) > 2:
                result['data_source'] = parts[2]
        
        return result

    def _extract_eps_data(self, content: str) -> Dict[str, List[float]]:
        """æå– EPS è³‡æ–™"""
        eps_data = {'2025': [], '2026': [], '2027': []}
        
        eps_data.update(self._extract_eps_from_table(content))
        
        for pattern in self.eps_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                try:
                    year = match[0]
                    value = float(match[1])
                    
                    if year in eps_data and 0 < value < 1000:
                        eps_data[year].append(value)
                except (ValueError, IndexError):
                    continue
        
        for year in eps_data:
            eps_data[year] = list(set(eps_data[year]))
        
        return eps_data

    def _extract_eps_from_table(self, content: str) -> Dict[str, List[float]]:
        """å¾è¡¨æ ¼ä¸­æå– EPS è³‡æ–™"""
        eps_data = {'2025': [], '2026': [], '2027': []}
        
        table_patterns = [
            r'\|\s*(æœ€é«˜å€¼|æœ€ä½å€¼|å¹³å‡å€¼|ä¸­ä½æ•¸)[^|]*\|\s*([0-9]+\.?[0-9]*)[^|]*\|\s*([0-9]+\.?[0-9]*)\s*\|\s*([0-9]+\.?[0-9]*)',
        ]
        
        for pattern in table_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                try:
                    years = ['2025', '2026', '2027']
                    for i, year in enumerate(years):
                        if i + 1 < len(match):
                            value_str = match[i + 1].strip()
                            value_str = re.sub(r'\([^)]*\)', '', value_str)
                            value = float(value_str)
                            if 0 < value < 1000:
                                eps_data[year].append(value)
                except (ValueError, IndexError):
                    continue
        
        return eps_data

    def _extract_target_price(self, content: str) -> Optional[float]:
        """æå–ç›®æ¨™åƒ¹æ ¼"""
        for pattern in self.target_price_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                try:
                    price = float(matches[0])
                    if 0 < price < 10000:
                        return price
                except ValueError:
                    continue
        return None

    def _extract_analyst_count(self, content: str) -> int:
        """æå–åˆ†æå¸«æ•¸é‡"""
        for pattern in self.analyst_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                try:
                    count = int(matches[0])
                    if 0 < count < 1000:
                        return count
                except ValueError:
                    continue
        return 0

    def _calculate_eps_statistics(self, eps_data: Dict[str, List[float]]) -> Dict[str, Any]:
        """è¨ˆç®— EPS çµ±è¨ˆè³‡æ–™"""
        result = {}
        
        for year in ['2025', '2026', '2027']:
            values = eps_data.get(year, [])
            
            if values:
                result[f'eps_{year}_high'] = max(values)
                result[f'eps_{year}_low'] = min(values)
                result[f'eps_{year}_avg'] = round(statistics.mean(values), 2)
            else:
                result[f'eps_{year}_high'] = None
                result[f'eps_{year}_low'] = None
                result[f'eps_{year}_avg'] = None
        
        return result

    def _calculate_data_richness(self, eps_stats: Dict, target_price: Optional[float], analyst_count: int) -> float:
        """è¨ˆç®—è³‡æ–™è±å¯Œåº¦åˆ†æ•¸ (0-10)"""
        score = 0
        
        eps_years = ['2025', '2026', '2027']
        eps_available = sum(1 for year in eps_years if eps_stats.get(f'eps_{year}_avg') is not None)
        score += (eps_available / len(eps_years)) * 6
        
        if target_price is not None:
            score += 2
        
        if analyst_count > 0:
            if analyst_count >= 20:
                score += 2
            elif analyst_count >= 10:
                score += 1.5
            elif analyst_count >= 5:
                score += 1
            else:
                score += 0.5
        
        return round(min(score, 10), 2)


# æ¸¬è©¦åŠŸèƒ½
if __name__ == "__main__":
    parser = MDParser()
    
    print(f"=== MD Parser v{parser.version} æ¸¬è©¦ (å¢å¼·ç‰ˆè§€å¯Ÿåå–®é©—è­‰å’ŒæŸ¥è©¢æ¨¡å¼æå–) ===")
    print(f"ğŸ“‹ è§€å¯Ÿåå–®è¼‰å…¥: {len(parser.watch_list_mapping)} å®¶å…¬å¸")
    print(f"ğŸ”§ é©—è­‰åŠŸèƒ½: {'å•Ÿç”¨' if parser.validation_enabled else 'åœç”¨'}")
    
    if parser.validation_enabled:
        # æ¸¬è©¦å¢å¼·ç‰ˆé©—è­‰é‚è¼¯
        test_cases = [
            ('6462', 'ase'),      # éŒ¯èª¤åç¨±
            ('6811', 'fubon'),    # éŒ¯èª¤åç¨±  
            ('9999', 'ä¸å­˜åœ¨'),    # ä¸å­˜åœ¨çš„ä»£è™Ÿ
            ('abc', 'test'),      # ç„¡æ•ˆæ ¼å¼
            ('2330', 'å°ç©é›»')     # æ­£å¸¸å…¬å¸ (å¦‚æœåœ¨è§€å¯Ÿåå–®ä¸­)
        ]
        
        print(f"\nğŸ§ª å¢å¼·ç‰ˆé©—è­‰æ¸¬è©¦:")
        for code, name in test_cases:
            result = parser._validate_against_watch_list_enhanced(code, name)
            status = result['overall_status']
            errors = len(result.get('errors', []))
            confidence = result.get('confidence_score', 0)
            method = result.get('validation_method', 'unknown')
            checks = len(result.get('detailed_checks', []))
            
            print(f"  {code} ({name}): {status} - ä¿¡å¿ƒåº¦:{confidence} - æ–¹æ³•:{method} - æª¢æŸ¥:{checks} - éŒ¯èª¤:{errors}")
            
            if errors > 0:
                for error in result.get('errors', [])[:1]:  # åªé¡¯ç¤ºç¬¬ä¸€å€‹éŒ¯èª¤
                    print(f"    âŒ {error}")
    else:
        print("âš ï¸ è§€å¯Ÿåå–®é©—è­‰å·²åœç”¨")
    
    # æ¸¬è©¦æŸ¥è©¢æ¨¡å¼æå–
    print(f"\nğŸ§ª æŸ¥è©¢æ¨¡å¼æå–æ¸¬è©¦:")
    test_content = '''---
search_query: å°ç©é›» 2330 factset eps é ä¼°
keywords: åŠå°é«”, æ™¶åœ“ä»£å·¥, å°ç©é›», factset
original_query: "å°ç©é›»" factset åˆ†æå¸« ç›®æ¨™åƒ¹
---

å°ç©é›»ç¬¬ä¸‰å­£è²¡å ±åˆ†æ...
'''
    
    test_yaml = {
        'search_query': 'ç¥ç›¾ 6462 factset ç”Ÿç‰©è¾¨è­˜',
        'keywords': 'ç¥ç›¾, factset, æŒ‡ç´‹è¾¨è­˜'
    }
    
    keywords = parser._extract_search_keywords_enhanced(test_content, test_yaml)
    print(f"   æå–çš„é—œéµå­—: {keywords}")
    print(f"   é—œéµå­—æ•¸é‡: {len(keywords)}")
    
    print(f"\nâœ… v{parser.version} å¢å¼·ç‰ˆ MD Parser å·²å•Ÿå‹•ï¼")
    print(f"ğŸ†• æ–°åŠŸèƒ½: å¢å¼·è§€å¯Ÿåå–®é©—è­‰ã€æŸ¥è©¢æ¨¡å¼æå–ã€å…§å®¹å“è³ªè©•ä¼°")
    print(f"ğŸ”§ ä¸»è¦ä¿®æ­£: å¤šå±¤æ¬¡åç¨±æ¯”è¼ƒã€ç›¸ä¼¼åº¦è¨ˆç®—ã€è©³ç´°é©—è­‰æ—¥èªŒ")