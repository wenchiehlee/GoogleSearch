#!/usr/bin/env python3
"""
MD Parser - FactSet Pipeline v3.5.0 (Enhanced with Content Validation)
å¢åŠ å…§å®¹é©—è­‰æ©Ÿåˆ¶ï¼Œåµæ¸¬å…¬å¸åç¨±ä¸ç¬¦ç­‰å•é¡Œ
"""

import os
import re
import yaml
import pandas as pd
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
import statistics

class MDParser:
    def __init__(self):
        """åˆå§‹åŒ– MD è§£æå™¨ - å¢å¼·é©—è­‰ç‰ˆ"""
        
        # åŸæœ‰çš„æ—¥æœŸæ¨¡å¼ä¿æŒä¸è®Š
        self.date_patterns = [
            r'\*\s*(\d{4})-(\d{1,2})-(\d{1,2})\s+\d{1,2}:\d{1,2}',
            r'\*\s*æ›´æ–°[ï¼š:]\s*(\d{4})-(\d{1,2})-(\d{1,2})\s+\d{1,2}:\d{1,2}',
            r'æ›´æ–°[ï¼š:]\s*(\d{4})-(\d{1,2})-(\d{1,2})\s+\d{1,2}:\d{1,2}',
            r'é‰…äº¨ç¶²æ–°èä¸­å¿ƒ\s*\n\s*\n\s*(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥[é€±å‘¨]?[ä¸€äºŒä¸‰å››äº”å…­æ—¥å¤©]?\s*[ä¸Šä¸‹]åˆ\s*\d{1,2}:\d{1,2}',
            r'é‰…äº¨ç¶²æ–°èä¸­å¿ƒ\s*\n\s*\n\s*(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
            r'é‰…äº¨ç¶²æ–°èä¸­å¿ƒ[\s\n]+(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥[é€±å‘¨]?[ä¸€äºŒä¸‰å››äº”å…­æ—¥å¤©]?',
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥[é€±å‘¨]?[ä¸€äºŒä¸‰å››äº”å…­æ—¥å¤©]?\s*[ä¸Šä¸‹]åˆ\s*\d{1,2}:\d{1,2}',
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥[é€±å‘¨]?[ä¸€äºŒä¸‰å››äº”å…­æ—¥å¤©]?',
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
            r'(\d{4})-(\d{1,2})-(\d{1,2})\s+\d{1,2}:\d{1,2}:\d{1,2}',
            r'(\d{4})-(\d{1,2})-(\d{1,2})\s+\d{1,2}:\d{1,2}',
            r'(\d{4})-(\d{1,2})-(\d{1,2})',
            r'(\d{4})/(\d{1,2})/(\d{1,2})',
        ]
        
        # EPS å’Œå…¶ä»–åŸæœ‰æ¨¡å¼ä¿æŒä¸è®Š
        self.eps_patterns = [
            r'(\d{4})å¹´[^|]*\|\s*([0-9]+\.?[0-9]*)',
            r'(\d{4})\s*å¹´\s*[:ï¼š]?\s*([0-9]+\.?[0-9]*)',
            r'(\d{4})\s*eps\s*[é ä¼°é æ¸¬ä¼°ç®—]*\s*[:ï¼š]\s*([0-9]+\.?[0-9]*)',
            r'eps\s*(\d{4})\s*[:ï¼š]\s*([0-9]+\.?[0-9]*)',
            r'å¹³å‡å€¼[^|]*(\d{4})[^|]*\|\s*([0-9]+\.?[0-9]*)',
            r'ä¸­ä½æ•¸[^|]*(\d{4})[^|]*\|\s*([0-9]+\.?[0-9]*)',
        ]
        
        self.target_price_patterns = [
            r'ç›®æ¨™åƒ¹\s*[:ï¼šç‚º]\s*([0-9]+\.?[0-9]*)\s*å…ƒ',
            r'é ä¼°ç›®æ¨™åƒ¹\s*[:ï¼šç‚º]\s*([0-9]+\.?[0-9]*)\s*å…ƒ',
            r'target\s*price\s*[:ï¼š]\s*([0-9]+\.?[0-9]*)',
        ]
        
        self.analyst_patterns = [
            r'å…±\s*(\d+)\s*ä½åˆ†æå¸«',
            r'(\d+)\s*ä½åˆ†æå¸«',
            r'(\d+)\s*analysts?',
        ]

        # ğŸ†• æ–°å¢ï¼šå…§å®¹é©—è­‰æ¨¡å¼
        self.validation_patterns = {
            # å°è‚¡ä»£è™Ÿæ¨¡å¼
            'taiwan_stock_codes': [
                r'(\d{4})[ï¼-]?TW',
                r'(\d{4})\.TW',
                r'å°è‚¡\s*(\d{4})',
                r'è­‰åˆ¸ä»£è™Ÿ\s*[:ï¼š]\s*(\d{4})'
            ],
            
            # ç¾è‚¡ä»£è™Ÿæ¨¡å¼
            'us_stock_codes': [
                r'([A-Z]{2,5})[ï¼-]?US',
                r'([A-Z]{2,5})\.US',
                r'ç¾è‚¡\s*([A-Z]{2,5})',
                r'ç´æ–¯é”å…‹\s*([A-Z]{2,5})',
                r'\(([A-Z]{2,5}[-]?US)\)'
            ],
            
            # å…¬å¸åç¨±æ¨¡å¼
            'company_names': [
                r'([\u4e00-\u9fa5]{2,8})\s*\(',  # ä¸­æ–‡å…¬å¸å + æ‹¬è™Ÿ
                r'å…¬å¸[ï¼š:]\s*([\u4e00-\u9fa5]{2,8})',
                r'([\u4e00-\u9fa5]{2,8})\s*è‚¡åƒ¹',
                r'([\u4e00-\u9fa5]{2,8})\s*æœ€æ–°',
                r'é—œæ–¼\s*([\u4e00-\u9fa5]{2,8})'
            ],
            
            # ç‰¹æ®Šæ¡ˆä¾‹ï¼šæ„›ç«‹ä¿¡ç›¸é—œ
            'ericsson_indicators': [
                r'æ„›ç«‹ä¿¡',
                r'Ericsson',
                r'ERIC[-]?US',
                r'ç‘å…¸.*é›»ä¿¡',
                r'é€šè¨Šè¨­å‚™.*å·¨é ­'
            ]
        }

        # ğŸ†• è¼‰å…¥è§€å¯Ÿåå–®é€²è¡Œé©—è­‰ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        self.watch_list_mapping = self._load_watch_list_mapping()

    def _load_watch_list_mapping(self) -> Dict[str, str]:
        """ğŸ†• è¼‰å…¥è§€å¯Ÿåå–®ä½œç‚ºæ¬Šå¨æ˜ å°„"""
        mapping = {}
        
        # å˜—è©¦å¤šå€‹å¯èƒ½çš„è§€å¯Ÿåå–®è·¯å¾‘
        possible_paths = [
            'è§€å¯Ÿåå–®.csv',
            '../è§€å¯Ÿåå–®.csv',
            '../../è§€å¯Ÿåå–®.csv',
            'data/è§€å¯Ÿåå–®.csv',
            '../data/è§€å¯Ÿåå–®.csv'
        ]
        
        for csv_path in possible_paths:
            if os.path.exists(csv_path):
                try:
                    import pandas as pd
                    df = pd.read_csv(csv_path, header=None, names=['code', 'name'])
                    
                    # å»ºç«‹ä»£è™Ÿ->åç¨±çš„æ˜ å°„
                    for _, row in df.iterrows():
                        code = str(row['code']).strip()
                        name = str(row['name']).strip()
                        if code and name and code != 'nan' and name != 'nan':
                            mapping[code] = name
                    
                    print(f"âœ… è¼‰å…¥è§€å¯Ÿåå–®: {csv_path} ({len(mapping)} å®¶å…¬å¸)")
                    break
                    
                except Exception as e:
                    print(f"âš ï¸ è®€å–è§€å¯Ÿåå–®å¤±æ•— {csv_path}: {e}")
                    continue
        
        if not mapping:
            print("âš ï¸ æœªæ‰¾åˆ°è§€å¯Ÿåå–®.csvï¼Œè·³éè§€å¯Ÿåå–®é©—è­‰")
        
        return mapping

    def parse_md_file(self, file_path: str) -> Dict[str, Any]:
        """ğŸ”’ å¢å¼·ç‰ˆè§£ææ–¹æ³• - åŠ å…¥å…§å®¹é©—è­‰"""
        try:
            # è®€å–æª”æ¡ˆå…§å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åŸºæœ¬æª”æ¡ˆè³‡è¨Š
            file_info = self._extract_file_info(file_path)
            
            # è§£æ YAML front matter
            yaml_data = self._extract_yaml_frontmatter(content)
            
            # åŸæœ‰åŠŸèƒ½ï¼šæ—¥æœŸæå–
            content_date = self._extract_content_date_bulletproof(content)
            extraction_status = "content_extraction" if content_date else "no_date_found"
            
            # ğŸ†• æ–°å¢ï¼šå…§å®¹é©—è­‰
            validation_result = self._validate_content_consistency(content, file_info)
            
            # åŸæœ‰åŠŸèƒ½ï¼šEPS ç­‰è³‡æ–™æå–
            eps_data = self._extract_eps_data(content)
            eps_stats = self._calculate_eps_statistics(eps_data)
            target_price = self._extract_target_price(content)
            analyst_count = self._extract_analyst_count(content)
            data_richness = self._calculate_data_richness(eps_stats, target_price, analyst_count)
            
            # çµ„åˆçµæœ
            result = {
                # åŸºæœ¬è³‡è¨Š
                'filename': os.path.basename(file_path),
                'company_code': file_info['company_code'],
                'company_name': file_info['company_name'],
                'data_source': file_info['data_source'],
                'file_mtime': datetime.fromtimestamp(os.path.getmtime(file_path)),
                
                # æ—¥æœŸè³‡è¨Š
                'content_date': content_date,
                'extracted_date': yaml_data.get('extracted_date'),
                'filename_date': file_info.get('parsed_timestamp'),
                
                # EPS è³‡æ–™
                **eps_stats,
                
                # å…¶ä»–è²¡å‹™è³‡æ–™
                'target_price': target_price,
                'analyst_count': analyst_count,
                
                # è³‡æ–™ç‹€æ…‹
                'has_eps_data': len(eps_data) > 0,
                'has_target_price': target_price is not None,
                'has_analyst_info': analyst_count > 0,
                'data_richness_score': data_richness,
                
                # YAML è³‡æ–™
                'yaml_data': yaml_data,
                
                # ğŸ†• å…§å®¹é©—è­‰çµæœ
                'validation_result': validation_result,
                'content_validation_passed': validation_result['overall_status'] == 'valid',
                'validation_warnings': validation_result.get('warnings', []),
                'validation_errors': validation_result.get('errors', []),
                
                # åŸå§‹å…§å®¹
                'content': content,
                'content_length': len(content),
                'parsed_at': datetime.now(),
                
                # èª¿è©¦è³‡è¨Š
                'date_extraction_method': extraction_status,
                'debug_info': self._get_debug_info(content, content_date)
            }
            
            return result
            
        except Exception as e:
            print(f"âŒ è§£ææª”æ¡ˆå¤±æ•— {file_path}: {e}")
            return self._create_empty_result(file_path, str(e))

    def _validate_content_consistency(self, content: str, file_info: Dict[str, Any]) -> Dict[str, Any]:
        """ğŸ†• å¼·åŒ–é©—è­‰ - æª¢æ¸¬å°ç¾è‚¡éŒ¯èª¤åŒ¹é…"""
        
        # å¾æª”æ¡ˆåç¨±æå–çš„è³‡è¨Š
        expected_company_code = file_info.get('company_code', '')
        expected_company_name = file_info.get('company_name', '')
        
        validation_result = {
            'overall_status': 'valid',
            'warnings': [],
            'errors': [],
            'detected_companies': [],
            'detected_stock_codes': [],
            'detected_regions': [],
            'confidence_score': 10.0
        }
        
        # ğŸ” æª¢æ¸¬å…§å®¹ä¸­çš„è‚¡ç¥¨ä»£è™Ÿ
        taiwan_codes = self._extract_patterns(content, self.validation_patterns['taiwan_stock_codes'])
        us_codes = self._extract_patterns(content, self.validation_patterns['us_stock_codes'])
        
        # ğŸ†• æ–°å¢ï¼šæª¢æ¸¬ç¾è‚¡æ ¼å¼çš„ä»£è™Ÿ (å¦‚ SU-US)
        us_ticker_patterns = [
            r'([A-Z]{1,5})[ï¼-]?US\b',      # SU-US, AAPL-US
            r'\(([A-Z]{1,5})[ï¼-]?US\)',    # (SU-US), (AAPL-US)
            r'([A-Z]{1,5})\.US\b',          # SU.US
            r'è‚¡ç¥¨ä»£è™Ÿ[ï¼š:]?\s*([A-Z]{1,5})[ï¼-]?US'  # è‚¡ç¥¨ä»£è™Ÿ: SU-US
        ]
        us_tickers = self._extract_patterns(content, us_ticker_patterns)
        
        # ğŸ†• æ–°å¢ï¼šæª¢æ¸¬å…¬å¸åç¨±ä¸­çš„åœ°å€æŒ‡æ¨™
        region_indicators = {
            'us_companies': [
                r'([^ï¼Œã€‚]*å…¬å¸)\([A-Z]{1,5}[ï¼-]?US\)',  # æ£®ç§‘èƒ½æºå…¬å¸(SU-US)
                r'([^ï¼Œã€‚]*)\s*\([A-Z]{1,5}[ï¼-]?US\)',   # ä»»ä½•åç¨±(TICKER-US)
                r'ç¾è‚¡.*?([^ï¼Œã€‚]*å…¬å¸)',                  # ç¾è‚¡XXXå…¬å¸
                r'ç´æ–¯é”å…‹.*?([^ï¼Œã€‚]*)',                  # ç´æ–¯é”å…‹XXX
                r'ç´ç´„è­‰äº¤æ‰€.*?([^ï¼Œã€‚]*)'                # ç´ç´„è­‰äº¤æ‰€XXX
            ],
            'taiwan_companies': [
                r'å°è‚¡.*?([^ï¼Œã€‚]*)',
                r'([^ï¼Œã€‚]*)\s*\((\d{4})[ï¼-]?TW\)',      # XXX(2480-TW)
                r'è‚¡ç¥¨ä»£è™Ÿ[ï¼š:]?\s*(\d{4})'                # è‚¡ç¥¨ä»£è™Ÿ: 2480
            ]
        }
        
        # æª¢æ¸¬ç¾è‚¡å…¬å¸
        us_company_names = self._extract_patterns(content, region_indicators['us_companies'])
        taiwan_company_names = self._extract_patterns(content, region_indicators['taiwan_companies'])
        
        validation_result['detected_stock_codes'] = {
            'taiwan': taiwan_codes,
            'us': us_codes,
            'us_tickers': us_tickers  # æ–°å¢
        }
        
        validation_result['detected_companies'] = {
            'us_companies': us_company_names,
            'taiwan_companies': taiwan_company_names
        }
        
        # ğŸš¨ é—œéµæª¢æŸ¥ï¼šå°è‚¡æª”æ¡ˆä½†å…§å®¹æ˜¯ç¾è‚¡
        if expected_company_code and expected_company_code.isdigit() and len(expected_company_code) == 4:
            # é æœŸæ˜¯å°è‚¡ (å¦‚2480)
            validation_result['detected_regions'].append('taiwan_expected')
            
            # æª¢æŸ¥æ˜¯å¦å…§å®¹åŒ…å«ç¾è‚¡è³‡è¨Š
            if us_tickers or us_company_names:
                validation_result['overall_status'] = 'error'
                validation_result['errors'].append(
                    f"å°è‚¡æª”æ¡ˆ({expected_company_code}-{expected_company_name})ä½†å…§å®¹åŒ…å«ç¾è‚¡è³‡è¨Š: "
                    f"ç¾è‚¡ä»£è™Ÿ{us_tickers}, ç¾è‚¡å…¬å¸{us_company_names}"
                )
                validation_result['confidence_score'] = 0.0
                
                validation_result['mismatch_details'] = {
                    'expected': {'company': expected_company_name, 'code': expected_company_code, 'region': 'TW'},
                    'detected': {'us_tickers': us_tickers, 'us_companies': us_company_names, 'region': 'US'},
                    'mismatch_type': 'taiwan_vs_us_content'
                }
                
                print(f"ğŸš¨ åš´é‡éŒ¯èª¤: å°è‚¡æª”æ¡ˆ{expected_company_code}({expected_company_name})åŒ…å«ç¾è‚¡å…§å®¹")
        
        # ğŸ†• æª¢æŸ¥å…¬å¸åç¨±ç›¸ä¼¼åº¦
        if expected_company_name and (us_company_names or taiwan_company_names):
            all_detected_names = us_company_names + taiwan_company_names
            
            # ç°¡å–®çš„ç›¸ä¼¼åº¦æª¢æŸ¥ï¼šæ˜¯å¦æœ‰ä»»ä½•ç›¸åŒå­—ç¬¦
            name_similarity_found = False
            for detected_name in all_detected_names:
                if self._names_are_similar(expected_company_name, detected_name):
                    name_similarity_found = True
                    break
            
            if not name_similarity_found and all_detected_names:
                validation_result['warnings'].append(
                    f"æª”æ¡ˆå…¬å¸åç¨±'{expected_company_name}'èˆ‡å…§å®¹æª¢æ¸¬åˆ°çš„å…¬å¸åç¨±å®Œå…¨ä¸ç¬¦: {all_detected_names}"
                )
                validation_result['confidence_score'] -= 3.0
        
        # ğŸ†• ç‰¹æ®Šæª¢æŸ¥ï¼šæ•¦é™½ç§‘ vs æ£®ç§‘
        if expected_company_name == 'æ•¦é™½ç§‘':
            if 'æ£®ç§‘' in content or 'Suncor' in content or 'SU-US' in content:
                validation_result['overall_status'] = 'error'
                validation_result['errors'].append(
                    f"æª”æ¡ˆæ¨™ç¤ºç‚ºæ•¦é™½ç§‘(2480)ä½†å…§å®¹æ˜¯é—œæ–¼æ£®ç§‘èƒ½æº(SU-US)ï¼Œå®Œå…¨ä¸åŒçš„å…¬å¸"
                )
                validation_result['confidence_score'] = 0.0
                print(f"ğŸš¨ æª¢æ¸¬åˆ°æ•¦é™½ç§‘/æ£®ç§‘éŒ¯èª¤åŒ¹é…")
        
        # åŸæœ‰çš„å…¶ä»–é©—è­‰é‚è¼¯...
        # (æ„›æ´¾å¸/æ„›ç«‹ä¿¡æª¢æŸ¥ã€è§€å¯Ÿåå–®æª¢æŸ¥ç­‰ä¿æŒä¸è®Š)
        
        # ğŸ¯ æœ€çµ‚ç‹€æ…‹åˆ¤æ–·
        if validation_result['confidence_score'] <= 2.0:
            validation_result['overall_status'] = 'error'
        elif validation_result['confidence_score'] <= 6.0:
            validation_result['overall_status'] = 'warning'
        elif validation_result['errors']:
            validation_result['overall_status'] = 'error'
        
        return validation_result

    def _extract_patterns(self, content: str, patterns: List[str]) -> List[str]:
        """å¾å…§å®¹ä¸­æå–ç¬¦åˆæ¨¡å¼çš„æ–‡å­—"""
        results = []
        for pattern in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                # è™•ç†ä¸åŒçš„åŒ¹é…æ ¼å¼
                for match in matches:
                    if isinstance(match, tuple):
                        results.extend([m for m in match if m])
                    else:
                        results.append(match)
        
        # å»é‡ä¸¦éæ¿¾
        unique_results = []
        for result in results:
            if result and result not in unique_results and len(result.strip()) > 0:
                unique_results.append(result.strip())
        
        return unique_results
    
    def _names_are_similar(self, name1: str, name2: str) -> bool:
        """ğŸ†• æª¢æŸ¥å…©å€‹å…¬å¸åç¨±æ˜¯å¦ç›¸ä¼¼"""
        # ç§»é™¤å¸¸è¦‹çš„å…¬å¸å¾Œç¶´
        clean_name1 = name1.replace('å…¬å¸', '').replace('è‚¡ä»½æœ‰é™å…¬å¸', '').replace('ç§‘æŠ€', '').strip()
        clean_name2 = name2.replace('å…¬å¸', '').replace('è‚¡ä»½æœ‰é™å…¬å¸', '').replace('ç§‘æŠ€', '').strip()
        
        # æª¢æŸ¥æ˜¯å¦æœ‰å…±åŒå­—ç¬¦ï¼ˆè‡³å°‘2å€‹å­—ï¼‰
        if len(clean_name1) >= 2 and len(clean_name2) >= 2:
            common_chars = set(clean_name1) & set(clean_name2)
            return len(common_chars) >= 2
        
        return False

    # åŸæœ‰æ–¹æ³•ä¿æŒä¸è®Š
    def _extract_content_date_bulletproof(self, content: str) -> Optional[str]:
        """ğŸ”’ çµ•å°é˜²å½ˆçš„æ—¥æœŸæå– - æ’é™¤ YAML frontmatter"""
        actual_content = self._get_content_without_yaml(content)
        found_dates = []
        
        for i, pattern in enumerate(self.date_patterns):
            matches = re.findall(pattern, actual_content, re.MULTILINE | re.DOTALL)
            
            if matches:
                for match in matches:
                    try:
                        if len(match) >= 3:
                            year, month, day = match[0], match[1], match[2]
                            
                            if self._validate_date(year, month, day):
                                date_str = f"{year}/{int(month)}/{int(day)}"
                                confidence = self._calculate_date_confidence(pattern, match, actual_content, i)
                                
                                found_dates.append({
                                    'date': date_str,
                                    'pattern_index': i,
                                    'pattern': pattern,
                                    'confidence': confidence,
                                    'match': match
                                })
                                
                    except (ValueError, IndexError) as e:
                        continue
        
        if found_dates:
            found_dates.sort(key=lambda x: x['confidence'], reverse=True)
            best_date = found_dates[0]
            print(f"ğŸ¯ æ‰¾åˆ°å…§å®¹æ—¥æœŸ: {best_date['date']} (æ¨¡å¼ {best_date['pattern_index']}, å¯ä¿¡åº¦ {best_date['confidence']})")
            return best_date['date']
        
        print(f"ğŸ”’ æœªæ‰¾åˆ°ä»»ä½•å…§å®¹æ—¥æœŸï¼Œè¿”å› None")
        return None

    def _get_content_without_yaml(self, content: str) -> str:
        """ğŸ”’ ç§»é™¤ YAML frontmatterï¼Œåªè¿”å›å¯¦éš›å…§å®¹"""
        try:
            if content.startswith('---'):
                end_pos = content.find('---', 3)
                if end_pos != -1:
                    actual_content = content[end_pos + 3:].strip()
                    print(f"ğŸ”’ æ’é™¤ YAML frontmatterï¼Œå¯¦éš›å…§å®¹é•·åº¦: {len(actual_content)}")
                    return actual_content
        except Exception as e:
            print(f"âš ï¸ YAML è™•ç†éŒ¯èª¤: {e}")
        return content

    def _validate_date(self, year: str, month: str, day: str) -> bool:
        """é©—è­‰æ—¥æœŸçš„åˆç†æ€§"""
        try:
            y, m, d = int(year), int(month), int(day)
            if not (2020 <= y <= 2030):
                return False
            if not (1 <= m <= 12):
                return False
            if not (1 <= d <= 31):
                return False
            datetime(y, m, d)
            return True
        except (ValueError, TypeError):
            return False

    def _calculate_date_confidence(self, pattern: str, match: tuple, content: str, pattern_index: int) -> float:
        """è¨ˆç®—æ—¥æœŸåŒ¹é…çš„å¯ä¿¡åº¦"""
        confidence = 5.0
        
        if pattern_index == 0:
            confidence += 6.0
        elif pattern_index == 1:
            confidence += 5.5
        elif pattern_index == 2:
            confidence += 5.0
        elif pattern_index <= 6:
            confidence += 4.0
        elif 'cmoney' in content.lower() or 'CMoney' in content:
            confidence += 2.5
        elif 'é‰…äº¨ç¶²' in pattern:
            confidence += 2.0
        elif 'å¹´' in pattern and 'æœˆ' in pattern:
            confidence += 1.5
        elif '-' in pattern:
            confidence += 1.0
        
        match_text = ''.join(match)
        position = content.find(match_text)
        if position != -1:
            if position < len(content) * 0.1:
                confidence += 2.0
            elif position < len(content) * 0.3:
                confidence += 1.0
        
        try:
            year = int(match[0])
            current_year = datetime.now().year
            if year == current_year:
                confidence += 1.5
            elif year == current_year - 1:
                confidence += 1.0
        except:
            pass
        
        return confidence

    # å…¶ä»–åŸæœ‰æ–¹æ³•ä¿æŒä¸è®Šï¼Œé€™è£¡åªåˆ—å‡ºç°½å
    def _get_debug_info(self, content: str, extracted_date: Optional[str]) -> Dict[str, Any]:
        """ç”Ÿæˆèª¿è©¦è³‡è¨Š"""
        # ... åŸæœ‰å¯¦ä½œ ...
        pass

    def _extract_file_info(self, file_path: str) -> Dict[str, Any]:
        """å¾æª”æ¡ˆè·¯å¾‘æå–åŸºæœ¬è³‡è¨Š"""
        # ... åŸæœ‰å¯¦ä½œä¿æŒä¸è®Š ...
        filename = os.path.basename(file_path)
        name_without_ext = filename.replace('.md', '')
        parts = name_without_ext.split('_')
        
        result = {
            'company_code': None,
            'company_name': None,
            'data_source': None,
            'timestamp': None,
            'parsed_timestamp': None
        }
        
        if len(parts) >= 2:
            if parts[0].isdigit() and len(parts[0]) == 4:
                result['company_code'] = parts[0]
            
            if len(parts) > 1:
                result['company_name'] = parts[1]
            
            if len(parts) > 2:
                result['data_source'] = parts[2]
        
        return result

    def _extract_yaml_frontmatter(self, content: str) -> Dict[str, Any]:
        """æå– YAML front matter"""
        # ... åŸæœ‰å¯¦ä½œä¿æŒä¸è®Š ...
        try:
            if content.startswith('---'):
                end_pos = content.find('---', 3)
                if end_pos != -1:
                    yaml_content = content[3:end_pos].strip()
                    return yaml.safe_load(yaml_content) or {}
        except:
            pass
        return {}

    def _extract_eps_data(self, content: str) -> Dict[str, List[float]]:
        """æå– EPS è³‡æ–™"""
        # ... åŸæœ‰å¯¦ä½œä¿æŒä¸è®Š ...
        eps_data = {'2025': [], '2026': [], '2027': []}
        
        eps_data.update(self._extract_eps_from_table(content))
        
        for pattern in self.eps_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                try:
                    year = match[0]
                    value = float(match[1])
                    
                    if year in eps_data and 0 < value < 1000:
                        eps_data[year].append(value)
                except (ValueError, IndexError):
                    continue
        
        for year in eps_data:
            eps_data[year] = list(set(eps_data[year]))
        
        return eps_data

    def _extract_eps_from_table(self, content: str) -> Dict[str, List[float]]:
        """å¾è¡¨æ ¼ä¸­æå– EPS è³‡æ–™"""
        # ... åŸæœ‰å¯¦ä½œä¿æŒä¸è®Š ...
        eps_data = {'2025': [], '2026': [], '2027': []}
        
        table_patterns = [
            r'\|\s*(æœ€é«˜å€¼|æœ€ä½å€¼|å¹³å‡å€¼|ä¸­ä½æ•¸)[^|]*\|\s*([0-9]+\.?[0-9]*)[^|]*\|\s*([0-9]+\.?[0-9]*)\s*\|\s*([0-9]+\.?[0-9]*)',
        ]
        
        for pattern in table_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                try:
                    years = ['2025', '2026', '2027']
                    for i, year in enumerate(years):
                        if i + 1 < len(match):
                            value_str = match[i + 1].strip()
                            value_str = re.sub(r'\([^)]*\)', '', value_str)
                            value = float(value_str)
                            if 0 < value < 1000:
                                eps_data[year].append(value)
                except (ValueError, IndexError):
                    continue
        
        return eps_data

    def _extract_target_price(self, content: str) -> Optional[float]:
        """æå–ç›®æ¨™åƒ¹æ ¼"""
        # ... åŸæœ‰å¯¦ä½œä¿æŒä¸è®Š ...
        for pattern in self.target_price_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                try:
                    price = float(matches[0])
                    if 0 < price < 10000:
                        return price
                except ValueError:
                    continue
        return None

    def _extract_analyst_count(self, content: str) -> int:
        """æå–åˆ†æå¸«æ•¸é‡"""
        # ... åŸæœ‰å¯¦ä½œä¿æŒä¸è®Š ...
        for pattern in self.analyst_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                try:
                    count = int(matches[0])
                    if 0 < count < 1000:
                        return count
                except ValueError:
                    continue
        return 0

    def _calculate_eps_statistics(self, eps_data: Dict[str, List[float]]) -> Dict[str, Any]:
        """è¨ˆç®— EPS çµ±è¨ˆè³‡æ–™"""
        # ... åŸæœ‰å¯¦ä½œä¿æŒä¸è®Š ...
        result = {}
        
        for year in ['2025', '2026', '2027']:
            values = eps_data.get(year, [])
            
            if values:
                result[f'eps_{year}_high'] = max(values)
                result[f'eps_{year}_low'] = min(values)
                result[f'eps_{year}_avg'] = round(statistics.mean(values), 2)
            else:
                result[f'eps_{year}_high'] = None
                result[f'eps_{year}_low'] = None
                result[f'eps_{year}_avg'] = None
        
        return result

    def _calculate_data_richness(self, eps_stats: Dict, target_price: Optional[float], analyst_count: int) -> float:
        """è¨ˆç®—è³‡æ–™è±å¯Œåº¦åˆ†æ•¸ (0-10)"""
        # ... åŸæœ‰å¯¦ä½œä¿æŒä¸è®Š ...
        score = 0
        
        eps_years = ['2025', '2026', '2027']
        eps_available = sum(1 for year in eps_years if eps_stats.get(f'eps_{year}_avg') is not None)
        score += (eps_available / len(eps_years)) * 6
        
        if target_price is not None:
            score += 2
        
        if analyst_count > 0:
            if analyst_count >= 20:
                score += 2
            elif analyst_count >= 10:
                score += 1.5
            elif analyst_count >= 5:
                score += 1
            else:
                score += 0.5
        
        return round(min(score, 10), 2)

    def _create_empty_result(self, file_path: str, error_msg: str) -> Dict[str, Any]:
        """å»ºç«‹ç©ºçš„è§£æçµæœ"""
        # ... åŸæœ‰å¯¦ä½œä¿æŒä¸è®Šï¼Œä½†åŠ å…¥é©—è­‰çµæœ ...
        file_info = self._extract_file_info(file_path)
        
        return {
            'filename': os.path.basename(file_path),
            'company_code': file_info.get('company_code'),
            'company_name': file_info.get('company_name'),
            'data_source': file_info.get('data_source'),
            'file_mtime': datetime.fromtimestamp(os.path.getmtime(file_path)) if os.path.exists(file_path) else None,
            'content_date': None,
            'eps_2025_high': None, 'eps_2025_low': None, 'eps_2025_avg': None,
            'eps_2026_high': None, 'eps_2026_low': None, 'eps_2026_avg': None,
            'eps_2027_high': None, 'eps_2027_low': None, 'eps_2027_avg': None,
            'target_price': None,
            'analyst_count': 0,
            'has_eps_data': False,
            'has_target_price': False,
            'has_analyst_info': False,
            'data_richness_score': 0.0,
            'yaml_data': {},
            'content': '',
            'content_length': 0,
            'parsed_at': datetime.now(),
            'error': error_msg,
            'date_extraction_method': 'error',
            # ğŸ†• åŠ å…¥ç©ºçš„é©—è­‰çµæœ
            'validation_result': {'overall_status': 'error', 'errors': [error_msg]},
            'content_validation_passed': False,
            'validation_warnings': [],
            'validation_errors': [error_msg]
        }


# æ¸¬è©¦åŠŸèƒ½
if __name__ == "__main__":
    parser = MDParser()
    
    print("=== ğŸ”’ å¢å¼·ç‰ˆ MD Parser æ¸¬è©¦ (å…§å®¹é©—è­‰) ===")
    
    # æ¸¬è©¦æ„›æ´¾å¸ vs æ„›ç«‹ä¿¡å•é¡Œ
    test_content = """---
company: æ„›æ´¾å¸
stock_code: 6918
---

é‰…äº¨é€Ÿå ± - Factset æœ€æ–°èª¿æŸ¥ï¼šæ„›ç«‹ä¿¡ERIC-USçš„ç›®æ¨™åƒ¹èª¿å‡è‡³9.06å…ƒï¼Œå¹…åº¦ç´„5.36%

æ ¹æ“šFactSetæœ€æ–°èª¿æŸ¥ï¼Œå…±18ä½åˆ†æå¸«ï¼Œå°æ„›ç«‹ä¿¡(ERIC-US)æå‡ºç›®æ¨™åƒ¹ä¼°å€¼ï¼šä¸­ä½æ•¸ç”±8.6å…ƒä¸Šä¿®è‡³9.06å…ƒï¼Œèª¿å‡å¹…åº¦5.36%ã€‚

æ„›ç«‹ä¿¡ä»Š(19æ—¥)æ”¶ç›¤åƒ¹ç‚º8.33å…ƒã€‚
"""
    
    # æ¨¡æ“¬æª”æ¡ˆè³‡è¨Š
    test_file_info = {
        'company_code': '6918',
        'company_name': 'æ„›æ´¾å¸',
        'data_source': 'yahoo'
    }
    
    print("æ¸¬è©¦å…§å®¹é©—è­‰ - æ„›æ´¾å¸ vs æ„›ç«‹ä¿¡å•é¡Œ:")
    validation_result = parser._validate_content_consistency(test_content, test_file_info)
    
    print(f"é©—è­‰ç‹€æ…‹: {validation_result['overall_status']}")
    print(f"å¯ä¿¡åº¦è©•åˆ†: {validation_result['confidence_score']}")
    print(f"éŒ¯èª¤è¨Šæ¯: {validation_result.get('errors', [])}")
    print(f"è­¦å‘Šè¨Šæ¯: {validation_result.get('warnings', [])}")
    print(f"åµæ¸¬åˆ°æ„›ç«‹ä¿¡: {validation_result.get('ericsson_detected', False)}")
    
    if validation_result.get('mismatch_details'):
        mismatch = validation_result['mismatch_details']
        print(f"ä¸ä¸€è‡´è©³æƒ…:")
        print(f"  é æœŸ: {mismatch['expected']}")
        print(f"  åµæ¸¬: {mismatch['detected']}")
    
    print(f"\nâœ… é æœŸçµæœ: é©—è­‰å¤±æ•—ï¼Œåµæ¸¬åˆ°æ„›æ´¾å¸/æ„›ç«‹ä¿¡ä¸ä¸€è‡´")
    print(f"âœ… å¯¦éš›çµæœ: {'ç¬¦åˆé æœŸ' if validation_result['overall_status'] == 'error' else 'ä¸ç¬¦é æœŸ'}")
    
    print("\nğŸ‰ æ¸¬è©¦å®Œæˆ!")