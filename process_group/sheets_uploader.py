#!/usr/bin/env python3
"""
Sheets Uploader - FactSet Pipeline v3.6.1 (ä¿®å¾©ç‰ˆ)
ä¿®å¾© columnWidth API éŒ¯èª¤ï¼Œå¢åŠ  CSV-only æ¨¡å¼
"""

import os
import gspread
import pandas as pd
import math
import time
from datetime import datetime
from typing import Dict, Any, List, Optional
from google.oauth2.service_account import Credentials
import json

# ğŸ”§ è¼‰å…¥ç’°å¢ƒè®Šæ•¸
try:
    from dotenv import load_dotenv
    # è¼‰å…¥ .env æª”æ¡ˆ - å˜—è©¦å¤šå€‹è·¯å¾‘
    env_paths = [
        '.env',
        '../.env', 
        '../../.env',
        os.path.join(os.path.dirname(__file__), '.env'),
        os.path.join(os.path.dirname(__file__), '../.env')
    ]
    
    for env_path in env_paths:
        if os.path.exists(env_path):
            load_dotenv(env_path)
            break
except ImportError:
    pass

class SheetsUploader:
    """Google Sheets ä¸Šå‚³å™¨ v3.6.1 - ä¿®å¾©ç‰ˆæœ¬ + CSV-only æ¨¡å¼"""
    
    def __init__(self, github_repo_base="https://raw.githubusercontent.com/wenchiehlee/GoogleSearch/refs/heads/main"):
        self.github_repo_base = github_repo_base
        self.client = None
        self.spreadsheet = None
        self.sheet_id = os.getenv('GOOGLE_SHEET_ID')
        
        # ğŸ”§ v3.6.1 æ›´æ–°çš„é©—è­‰è¨­å®š
        self.validation_settings = {
            'check_before_upload': True,
            'allow_warning_data': True,
            'allow_error_data': False,
            'max_validation_errors': 5,
            'skip_not_block': True,
            'enhanced_validation': True,
            'generate_validation_csv': True,
            'upload_validation_to_sheets': True,  # ğŸ”§ é»˜èªé—œé–‰ Sheets ä¸Šå‚³
            'csv_output_dir': 'data/reports',
            'csv_only_mode': False  # ğŸ†• æ–°å¢ CSV-only æ¨¡å¼
        }
        
        # ğŸ†• v3.6.1 å·¥ä½œè¡¨åç¨±
        self.worksheet_names = {
            'portfolio': 'æŠ•è³‡çµ„åˆæ‘˜è¦',
            'detailed': 'è©³ç´°å ±å‘Š',
            'validation': 'é©—è­‰æ‘˜è¦',  
            'keywords': 'æŸ¥è©¢æ¨¡å¼æ‘˜è¦',
            'watchlist': 'è§€å¯Ÿåå–®æ‘˜è¦'  # æ–°å¢
        }
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        os.makedirs(self.validation_settings['csv_output_dir'], exist_ok=True)
        
        # ğŸ†• API é™åˆ¶è¨­å®š
        self.api_settings = {
            'max_retries': 3,
            'retry_delay': 2,  # ç§’
            'batch_size': 100,  # æ¯æ¬¡æ‰¹é‡æ“ä½œçš„è¡Œæ•¸
            'rate_limit_delay': 0.5  # API èª¿ç”¨é–“éš”
        }

    def upload_all_reports(self, portfolio_df: pd.DataFrame, detailed_df: pd.DataFrame, 
                          keyword_df: pd.DataFrame = None, watchlist_df: pd.DataFrame = None, 
                          csv_only=False) -> bool:
        """ğŸ”§ v3.6.1 ä¸»è¦ä¸Šå‚³æ–¹æ³• - æ”¯æ´ CSV-only æ¨¡å¼"""
        
        # ğŸ†• å¦‚æœå•Ÿç”¨ CSV-only æ¨¡å¼ï¼Œåªç”Ÿæˆ CSV
        if csv_only or self.validation_settings.get('csv_only_mode', False):
            return self._csv_only_mode(portfolio_df, detailed_df, keyword_df, watchlist_df)
        
        try:
            # ä¸Šå‚³å‰é©—è­‰
            if self.validation_settings['check_before_upload']:
                validation_result = self._validate_before_upload_v361(portfolio_df, detailed_df, watchlist_df)
                
                if not validation_result['safe_to_upload']:
                    print(f"ğŸš¨ ä¸Šå‚³é©—è­‰å¤±æ•—: {validation_result['reason']}")
                    print(f"ğŸ“Š å•é¡Œæ‘˜è¦: {validation_result['summary']}")
                    
                    if validation_result['severity'] == 'critical':
                        print("âŒ ç™¼ç¾é—œéµå•é¡Œï¼Œæ”¹ç”¨ CSV-only æ¨¡å¼")
                        return self._csv_only_mode(portfolio_df, detailed_df, keyword_df, watchlist_df)
                    elif validation_result['severity'] == 'warning':
                        print("âš ï¸ ç™¼ç¾è­¦å‘Šï¼Œå»ºè­°ä½¿ç”¨ CSV-only æ¨¡å¼")
                        return self._csv_only_mode(portfolio_df, detailed_df, keyword_df, watchlist_df)
                else:
                    if validation_result.get('reason'):
                        print(f"ğŸ“Š é©—è­‰çµ±è¨ˆ: {validation_result['reason']}")
                        print(f"ğŸ“Š å•é¡Œæ‘˜è¦: {validation_result['summary']}")
                    else:
                        print("âœ… ä¸Šå‚³å‰é©—è­‰é€šé")
            
            # ğŸ”§ å¢åŠ  API é™åˆ¶æª¢æŸ¥
            if not self._check_api_availability():
                print("âš ï¸ Google Sheets API å¯èƒ½æ¥è¿‘é™åˆ¶ï¼Œæ”¹ç”¨ CSV-only æ¨¡å¼")
                return self._csv_only_mode(portfolio_df, detailed_df, keyword_df, watchlist_df)
            
            # è¨­å®šé€£ç·š
            if not self._setup_connection():
                print("âŒ Google Sheets é€£ç·šå¤±æ•—ï¼Œæ”¹ç”¨ CSV-only æ¨¡å¼")
                return self._csv_only_mode(portfolio_df, detailed_df, keyword_df, watchlist_df)
            
            # æ¨™è¨˜å•é¡Œè³‡æ–™
            portfolio_df_marked = self._mark_problematic_data_v361(portfolio_df)
            detailed_df_marked = self._mark_problematic_data_v361(detailed_df)
            
            success_count = 0
            total_uploads = 4  # åŸºæœ¬ä¸Šå‚³æ•¸é‡
            
            # ä¸Šå‚³æŠ•è³‡çµ„åˆæ‘˜è¦
            if self._upload_portfolio_summary_safe(portfolio_df_marked):
                success_count += 1
                print("ğŸ“Š æŠ•è³‡çµ„åˆæ‘˜è¦ä¸Šå‚³æˆåŠŸ")
            else:
                print("âŒ æŠ•è³‡çµ„åˆæ‘˜è¦ä¸Šå‚³å¤±æ•—")
            
            # ä¸Šå‚³è©³ç´°å ±å‘Š
            if self._upload_detailed_report_safe(detailed_df_marked):
                success_count += 1
                print("ğŸ“Š è©³ç´°å ±å‘Šä¸Šå‚³æˆåŠŸ")
            else:
                print("âŒ è©³ç´°å ±å‘Šä¸Šå‚³å¤±æ•—")
            
            # ä¸Šå‚³é—œéµå­—å ±å‘Š
            if keyword_df is not None and not keyword_df.empty:
                if self._upload_keyword_summary_safe(keyword_df):
                    success_count += 1
                    print("ğŸ“Š é—œéµå­—å ±å‘Šä¸Šå‚³æˆåŠŸ")
                else:
                    print("âš ï¸ é—œéµå­—å ±å‘Šä¸Šå‚³å¤±æ•—")
            
            # ä¸Šå‚³è§€å¯Ÿåå–®å ±å‘Š
            if watchlist_df is not None and not watchlist_df.empty:
                if self._upload_watchlist_summary_safe(watchlist_df):
                    success_count += 1
                    print("ğŸ“Š è§€å¯Ÿåå–®å ±å‘Šä¸Šå‚³æˆåŠŸ")
                else:
                    print("âš ï¸ è§€å¯Ÿåå–®å ±å‘Šä¸Šå‚³å¤±æ•—")
            
            # ğŸ”§ åŒæ™‚ç”Ÿæˆ CSV å‚™ä»½
            self._generate_csv_backup(portfolio_df, detailed_df, keyword_df, watchlist_df)
            
            # è™•ç†é©—è­‰æ‘˜è¦
            self._handle_validation_summary_v361_safe(portfolio_df, detailed_df, watchlist_df)
            
            # è©•ä¼°ä¸Šå‚³æˆåŠŸç‡
            success_rate = success_count / max(total_uploads, 1)
            if success_rate >= 0.5:
                print(f"âœ… ä¸Šå‚³å®Œæˆ (æˆåŠŸç‡: {success_rate:.1%})")
                return True
            else:
                print(f"âš ï¸ éƒ¨åˆ†ä¸Šå‚³å¤±æ•— (æˆåŠŸç‡: {success_rate:.1%})")
                print("ğŸ’¡ å»ºè­°ä½¿ç”¨ç”Ÿæˆçš„ CSV æª”æ¡ˆ")
                return False
            
        except Exception as e:
            print(f"âŒ ä¸Šå‚³éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            print("ğŸ”„ æ”¹ç”¨ CSV-only æ¨¡å¼...")
            return self._csv_only_mode(portfolio_df, detailed_df, keyword_df, watchlist_df)

    def _csv_only_mode(self, portfolio_df: pd.DataFrame, detailed_df: pd.DataFrame, 
                      keyword_df: pd.DataFrame = None, watchlist_df: pd.DataFrame = None) -> bool:
        """ğŸ†• CSV-only æ¨¡å¼ - å®Œå…¨é¿å… Google Sheets API"""
        try:
            print("ğŸ“ å•Ÿå‹• CSV-only æ¨¡å¼...")
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            saved_files = {}
            
            # 1. æŠ•è³‡çµ„åˆæ‘˜è¦ CSV
            portfolio_path = os.path.join(self.validation_settings['csv_output_dir'], f'portfolio_summary_{timestamp}.csv')
            portfolio_latest = os.path.join(self.validation_settings['csv_output_dir'], 'factset_portfolio_summary_latest.csv')
            
            portfolio_df_clean = portfolio_df.fillna('')
            portfolio_df_clean.to_csv(portfolio_path, index=False, encoding='utf-8-sig')
            portfolio_df_clean.to_csv(portfolio_latest, index=False, encoding='utf-8-sig')
            
            saved_files['portfolio'] = portfolio_path
            print(f"âœ… æŠ•è³‡çµ„åˆæ‘˜è¦ CSV: {os.path.basename(portfolio_path)}")
            
            # 2. è©³ç´°å ±å‘Š CSV
            detailed_path = os.path.join(self.validation_settings['csv_output_dir'], f'detailed_report_{timestamp}.csv')
            detailed_latest = os.path.join(self.validation_settings['csv_output_dir'], 'factset_detailed_report_latest.csv')
            
            detailed_df_clean = detailed_df.fillna('')
            detailed_df_clean.to_csv(detailed_path, index=False, encoding='utf-8-sig')
            detailed_df_clean.to_csv(detailed_latest, index=False, encoding='utf-8-sig')
            
            saved_files['detailed'] = detailed_path
            print(f"âœ… è©³ç´°å ±å‘Š CSV: {os.path.basename(detailed_path)}")
            
            # 3. é—œéµå­—/æŸ¥è©¢æ¨¡å¼å ±å‘Š CSV
            if keyword_df is not None and not keyword_df.empty:
                # æª¢æŸ¥æ˜¯å¦ç‚ºæŸ¥è©¢æ¨¡å¼å ±å‘Š
                if len(keyword_df.columns) > 0 and keyword_df.columns[0] == 'Query pattern':
                    keyword_path = os.path.join(self.validation_settings['csv_output_dir'], 'factset_query_pattern_summary_latest.csv')
                    report_type = "æŸ¥è©¢æ¨¡å¼çµ±è¨ˆ"
                else:
                    keyword_path = os.path.join(self.validation_settings['csv_output_dir'], 'keyword_summary_latest.csv')
                    report_type = "é—œéµå­—çµ±è¨ˆ"

                keyword_df_clean = keyword_df.fillna('')
                keyword_df_clean.to_csv(keyword_path, index=False, encoding='utf-8-sig')

                saved_files['keyword'] = keyword_path
                print(f"âœ… {report_type} CSV: {os.path.basename(keyword_path)}")
            
            # 4. è§€å¯Ÿåå–®å ±å‘Š CSV
            if watchlist_df is not None and not watchlist_df.empty:
                watchlist_path = os.path.join(self.validation_settings['csv_output_dir'], 'watchlist_summary_latest.csv')

                watchlist_df_clean = watchlist_df.fillna('')
                watchlist_df_clean.to_csv(watchlist_path, index=False, encoding='utf-8-sig')

                saved_files['watchlist'] = watchlist_path
                print(f"âœ… è§€å¯Ÿåå–®çµ±è¨ˆ CSV: {os.path.basename(watchlist_path)}")

            # 5. ç”Ÿæˆé©—è­‰æ‘˜è¦ CSV
            validation_data = self._generate_validation_summary_data_v361(portfolio_df, detailed_df, watchlist_df)
            validation_path = os.path.join(self.validation_settings['csv_output_dir'], 'validation_summary_latest.csv')

            validation_data.to_csv(validation_path, index=False, encoding='utf-8-sig')

            saved_files['validation'] = validation_path
            print(f"âœ… é©—è­‰æ‘˜è¦ CSV: {os.path.basename(validation_path)}")
            
            # 6. ç”Ÿæˆä½¿ç”¨æŒ‡å—
            self._generate_usage_guide(saved_files, timestamp)
            
            print(f"\nğŸ‰ CSV-only æ¨¡å¼å®Œæˆï¼")
            print(f"ğŸ“ æ‰€æœ‰æª”æ¡ˆä½æ–¼: {os.path.abspath(self.validation_settings['csv_output_dir'])}")
            print(f"ğŸ“‹ ä½¿ç”¨æŒ‡å—: generation_guide_latest.md")
            print(f"\nğŸ’¡ æ‰‹å‹•ä¸Šå‚³å»ºè­°:")
            print(f"   1. é–‹å•Ÿ Google Sheets")
            print(f"   2. åŒ¯å…¥å„å€‹ *_latest.csv æª”æ¡ˆ")
            print(f"   3. æ¯å€‹ CSV å»ºç«‹ç‚ºä¸€å€‹å·¥ä½œè¡¨")
            
            return True
            
        except Exception as e:
            print(f"âŒ CSV-only æ¨¡å¼å¤±æ•—: {e}")
            return False

    def _upload_watchlist_summary_safe(self, watchlist_df: pd.DataFrame) -> bool:
        """ğŸ”§ å®‰å…¨ç‰ˆæœ¬çš„è§€å¯Ÿåå–®ä¸Šå‚³ - ä¿®å¾© columnWidth å•é¡Œ"""
        try:
            # å»ºç«‹æˆ–å–å¾—è§€å¯Ÿåå–®å·¥ä½œè¡¨
            try:
                worksheet = self.spreadsheet.worksheet(self.worksheet_names['watchlist'])
            except gspread.WorksheetNotFound:
                print("ğŸ“Š å»ºç«‹è§€å¯Ÿåå–®å·¥ä½œè¡¨...")
                worksheet = self.spreadsheet.add_worksheet(
                    title=self.worksheet_names['watchlist'], 
                    rows=1000, 
                    cols=15
                )
            
            # æ¸…ç©ºç¾æœ‰è³‡æ–™
            worksheet.clear()
            time.sleep(self.api_settings['rate_limit_delay'])
            
            # æ¸…ç†è³‡æ–™
            watchlist_df_clean = watchlist_df.copy()
            watchlist_df_clean = watchlist_df_clean.fillna('')
            
            # ç¢ºä¿å…¬å¸ä»£è™Ÿæ ¼å¼æ­£ç¢º
            if 'å…¬å¸ä»£è™Ÿ' in watchlist_df_clean.columns:
                watchlist_df_clean['å…¬å¸ä»£è™Ÿ'] = watchlist_df_clean['å…¬å¸ä»£è™Ÿ'].apply(self._clean_stock_code)
            
            # æ ¼å¼åŒ–æ•¸å€¼æ¬„ä½
            numeric_columns = ['MDæª”æ¡ˆæ•¸é‡', 'å¹³å‡å“è³ªè©•åˆ†', 'æœ€é«˜å“è³ªè©•åˆ†', 'æœå°‹é—œéµå­—æ•¸é‡', 'é—œéµå­—å¹³å‡å“è³ª']
            for col in numeric_columns:
                if col in watchlist_df_clean.columns:
                    watchlist_df_clean[col] = watchlist_df_clean[col].apply(self._format_numeric_value)
            
            # æº–å‚™ä¸Šå‚³è³‡æ–™
            headers = watchlist_df_clean.columns.tolist()
            data = watchlist_df_clean.values.tolist()
            
            # ç¢ºä¿æ‰€æœ‰è³‡æ–™éƒ½æ˜¯ JSON ç›¸å®¹çš„
            data = [[self._ensure_json_compatible(cell) for cell in row] for row in data]
            
            # ä¸Šå‚³æ¨™é¡Œ
            worksheet.update('A1', [headers])
            time.sleep(self.api_settings['rate_limit_delay'])
            
            # åˆ†æ‰¹ä¸Šå‚³è³‡æ–™
            if data:
                batch_size = self.api_settings['batch_size']
                for i in range(0, len(data), batch_size):
                    batch_data = data[i:i + batch_size]
                    start_row = i + 2  # +2 å› ç‚ºæ¨™é¡Œä½”ç¬¬1è¡Œï¼Œè³‡æ–™å¾ç¬¬2è¡Œé–‹å§‹
                    range_name = f'A{start_row}'
                    
                    worksheet.update(range_name, batch_data)
                    time.sleep(self.api_settings['rate_limit_delay'])
                    
                    if i + batch_size < len(data):
                        print(f"   å·²ä¸Šå‚³ {i + batch_size}/{len(data)} è¡Œ...")
            
            # ğŸ”§ ä¿®å¾©å¾Œçš„æ ¼å¼è¨­å®š - ä¸ä½¿ç”¨ columnWidth
            self._format_watchlist_worksheet_fixed(worksheet, len(watchlist_df_clean))
            
            print("ğŸ“Š è§€å¯Ÿåå–®å ±å‘Šä¸Šå‚³å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ è§€å¯Ÿåå–®å ±å‘Šä¸Šå‚³å¤±æ•—: {e}")
            return False

    def _format_watchlist_worksheet_fixed(self, worksheet, data_rows: int):
        """ğŸ”§ ä¿®å¾©ç‰ˆæœ¬ï¼šæ ¼å¼åŒ–è§€å¯Ÿåå–®å·¥ä½œè¡¨ - ä¸ä½¿ç”¨ columnWidth"""
        try:
            # è¨­å®šæ¨™é¡Œåˆ—æ ¼å¼
            worksheet.format('A1:L1', {
                'backgroundColor': {'red': 0.2, 'green': 0.7, 'blue': 0.9},
                'textFormat': {'bold': True, 'foregroundColor': {'red': 1, 'green': 1, 'blue': 1}}
            })
            time.sleep(self.api_settings['rate_limit_delay'])
            
            if data_rows > 0:
                try:
                    # è¨­å®šæ•¸å€¼æ¬„ä½æ ¼å¼ - åˆ†æ‰¹è™•ç†
                    ranges_to_format = [
                        ('E:F', {'numberFormat': {'type': 'NUMBER', 'pattern': '0.00'}}),  # å“è³ªè©•åˆ†æ¬„ä½
                        ('I:I', {'numberFormat': {'type': 'NUMBER', 'pattern': '0.00'}})   # é—œéµå­—å¹³å‡å“è³ª
                    ]
                    
                    for range_name, format_dict in ranges_to_format:
                        try:
                            worksheet.format(range_name, format_dict)
                            time.sleep(self.api_settings['rate_limit_delay'])
                        except Exception as format_error:
                            print(f"âš ï¸ è¨­å®š {range_name} æ ¼å¼å¤±æ•—: {format_error}")
                
                except Exception as number_format_error:
                    print(f"âš ï¸ è¨­å®šæ•¸å€¼æ ¼å¼å¤±æ•—: {number_format_error}")
                
                # ğŸ”§ ç§»é™¤ columnWidth è¨­å®šï¼Œæ”¹ç”¨ç°¡å–®çš„æ–‡å­—æ ¼å¼
                try:
                    # è¨­å®šæ–‡å­—å°é½Š
                    worksheet.format('A:A', {'horizontalAlignment': 'CENTER'})   # å…¬å¸ä»£è™Ÿç½®ä¸­
                    time.sleep(self.api_settings['rate_limit_delay'])
                    
                    worksheet.format('B:B', {'horizontalAlignment': 'LEFT'})     # å…¬å¸åç¨±å·¦å°é½Š
                    time.sleep(self.api_settings['rate_limit_delay'])
                    
                except Exception as align_error:
                    print(f"âš ï¸ è¨­å®šå°é½Šæ ¼å¼å¤±æ•—: {align_error}")
                    
        except Exception as e:
            print(f"âš ï¸ è§€å¯Ÿåå–®å·¥ä½œè¡¨æ ¼å¼è¨­å®šå¤±æ•—: {e}")

    def _upload_portfolio_summary_safe(self, portfolio_df: pd.DataFrame) -> bool:
        """å®‰å…¨ç‰ˆæœ¬çš„æŠ•è³‡çµ„åˆæ‘˜è¦ä¸Šå‚³"""
        return self._upload_with_retry("æŠ•è³‡çµ„åˆæ‘˜è¦", portfolio_df, self._upload_portfolio_summary)

    def _upload_detailed_report_safe(self, detailed_df: pd.DataFrame) -> bool:
        """å®‰å…¨ç‰ˆæœ¬çš„è©³ç´°å ±å‘Šä¸Šå‚³"""
        return self._upload_with_retry("è©³ç´°å ±å‘Š", detailed_df, self._upload_detailed_report)

    def _upload_keyword_summary_safe(self, keyword_df: pd.DataFrame) -> bool:
        """å®‰å…¨ç‰ˆæœ¬çš„é—œéµå­—å ±å‘Šä¸Šå‚³"""
        return self._upload_with_retry("é—œéµå­—å ±å‘Š", keyword_df, self._upload_keyword_summary)

    def _upload_with_retry(self, report_name: str, df: pd.DataFrame, upload_func) -> bool:
        """é€šç”¨çš„é‡è©¦ä¸Šå‚³æ–¹æ³•"""
        for attempt in range(self.api_settings['max_retries']):
            try:
                result = upload_func(df)
                if result:
                    return True
                else:
                    print(f"âš ï¸ {report_name} ä¸Šå‚³å˜—è©¦ {attempt + 1} å¤±æ•—")
                    
            except Exception as e:
                print(f"âš ï¸ {report_name} ä¸Šå‚³å˜—è©¦ {attempt + 1} éŒ¯èª¤: {e}")
                
                # æª¢æŸ¥æ˜¯å¦ç‚º API é™åˆ¶éŒ¯èª¤
                if "quota exceeded" in str(e).lower() or "429" in str(e):
                    print(f"ğŸš« API é™åˆ¶ï¼Œåœæ­¢é‡è©¦ {report_name}")
                    return False
                
                if attempt < self.api_settings['max_retries'] - 1:
                    wait_time = self.api_settings['retry_delay'] * (attempt + 1)
                    print(f"â³ ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                    time.sleep(wait_time)
        
        print(f"âŒ {report_name} æ‰€æœ‰é‡è©¦å¤±æ•—")
        return False

    def _check_api_availability(self) -> bool:
        """æª¢æŸ¥ API å¯ç”¨æ€§"""
        try:
            if not self.sheet_id:
                return False
            
            # ç°¡å–®çš„é€£ç·šæ¸¬è©¦
            if self.client is None:
                return self._setup_connection()
            
            return True
            
        except Exception as e:
            if "quota exceeded" in str(e).lower() or "429" in str(e):
                return False
            return True

    def _generate_csv_backup(self, portfolio_df: pd.DataFrame, detailed_df: pd.DataFrame, 
                           keyword_df: pd.DataFrame = None, watchlist_df: pd.DataFrame = None):
        """ç”Ÿæˆ CSV å‚™ä»½æª”æ¡ˆ"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_dir = os.path.join(self.validation_settings['csv_output_dir'], 'backup')
            os.makedirs(backup_dir, exist_ok=True)
            
            # å‚™ä»½ä¸»è¦å ±å‘Š
            portfolio_df.to_csv(os.path.join(backup_dir, f'portfolio_backup_{timestamp}.csv'), 
                               index=False, encoding='utf-8-sig')
            detailed_df.to_csv(os.path.join(backup_dir, f'detailed_backup_{timestamp}.csv'), 
                              index=False, encoding='utf-8-sig')
            
            if keyword_df is not None and not keyword_df.empty:
                keyword_df.to_csv(os.path.join(backup_dir, f'keyword_backup_{timestamp}.csv'), 
                                 index=False, encoding='utf-8-sig')
            
            if watchlist_df is not None and not watchlist_df.empty:
                watchlist_df.to_csv(os.path.join(backup_dir, f'watchlist_backup_{timestamp}.csv'), 
                                   index=False, encoding='utf-8-sig')
            
            print(f"ğŸ’¾ CSV å‚™ä»½å·²ç”Ÿæˆ: {backup_dir}")
            
        except Exception as e:
            print(f"âš ï¸ CSV å‚™ä»½ç”Ÿæˆå¤±æ•—: {e}")

    def _generate_usage_guide(self, saved_files: Dict[str, str], timestamp: str):
        """ç”Ÿæˆä½¿ç”¨æŒ‡å—"""
        guide_content = f"""
# FactSet Pipeline v3.6.1 - CSV å ±å‘Šä½¿ç”¨æŒ‡å—
ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
æ™‚é–“æˆ³: {timestamp}

## ğŸ“ ç”Ÿæˆçš„æª”æ¡ˆæ¸…å–®

### ä¸»è¦å ±å‘Šæª”æ¡ˆ
"""
        
        for report_type, file_path in saved_files.items():
            filename = os.path.basename(file_path)
            guide_content += f"- {report_type}: `{filename}`\n"
        
        guide_content += f"""

### æœ€æ–°ç‰ˆæœ¬æª”æ¡ˆ (ç„¡æ™‚é–“æˆ³)
- `factset_portfolio_summary_latest.csv` - æŠ•è³‡çµ„åˆæ‘˜è¦
- `factset_detailed_report_latest.csv` - è©³ç´°å ±å‘Š
- `factset_query_pattern_summary_latest.csv` - æŸ¥è©¢æ¨¡å¼çµ±è¨ˆ (å¦‚æœ‰)
- `watchlist_summary_latest.csv` - è§€å¯Ÿåå–®çµ±è¨ˆ (å¦‚æœ‰)
- `validation_summary_latest.csv` - é©—è­‰æ‘˜è¦

## ğŸ”§ Google Sheets æ‰‹å‹•ä¸Šå‚³æ­¥é©Ÿ

### æ–¹æ³• 1: æª”æ¡ˆåŒ¯å…¥
1. é–‹å•Ÿ Google Sheets
2. å»ºç«‹æ–°çš„è©¦ç®—è¡¨æˆ–é–‹å•Ÿç¾æœ‰è©¦ç®—è¡¨
3. é»é¸ã€Œæª”æ¡ˆã€â†’ã€ŒåŒ¯å…¥ã€
4. ä¸Šå‚³ CSV æª”æ¡ˆ
5. é¸æ“‡ã€Œå»ºç«‹æ–°å·¥ä½œè¡¨ã€
6. é‡è¤‡æ­¥é©Ÿ 3-5 for æ¯å€‹ CSV æª”æ¡ˆ

### æ–¹æ³• 2: è¤‡è£½è²¼ä¸Š
1. ç”¨ Excel æˆ–æ–‡å­—ç·¨è¼¯å™¨é–‹å•Ÿ CSV æª”æ¡ˆ
2. å…¨é¸ä¸¦è¤‡è£½å…§å®¹
3. åœ¨ Google Sheets ä¸­å»ºç«‹æ–°å·¥ä½œè¡¨
4. è²¼ä¸Šå…§å®¹
5. å¦‚æœæ ¼å¼æœ‰å•é¡Œï¼Œä½¿ç”¨ã€Œè³‡æ–™ã€â†’ã€Œåˆ†æ¬„ã€åŠŸèƒ½

## ğŸ“Š å„å ±å‘Šæª”æ¡ˆèªªæ˜

### æŠ•è³‡çµ„åˆæ‘˜è¦ (portfolio_summary_*.csv)
- **ç”¨é€”**: æ¯å®¶å…¬å¸çš„é—œéµæŒ‡æ¨™æ‘˜è¦
- **æ¬„ä½**: 14 å€‹æ¬„ä½ï¼ŒåŒ…å«ä»£è™Ÿã€åç¨±ã€EPS é æ¸¬ã€å“è³ªè©•åˆ†ç­‰
- **é©ç”¨**: é«˜éšä¸»ç®¡å ±å‘Šã€å¿«é€Ÿæ¦‚è¦½

### è©³ç´°å ±å‘Š (detailed_report_*.csv)
- **ç”¨é€”**: æ‰€æœ‰è¨˜éŒ„çš„å®Œæ•´è©³ç´°è³‡è¨Š
- **æ¬„ä½**: 22 å€‹æ¬„ä½ï¼ŒåŒ…å«æ‰€æœ‰ EPS æ•¸æ“šã€é©—è­‰ç‹€æ…‹ã€MD æª”æ¡ˆé€£çµ
- **é©ç”¨**: åˆ†æå¸«æ·±åº¦åˆ†æã€è³‡æ–™é©—è­‰

### æŸ¥è©¢æ¨¡å¼çµ±è¨ˆ (query_pattern_summary_*.csv)
- **ç”¨é€”**: æœå°‹æŸ¥è©¢æ¨¡å¼æ•ˆæœåˆ†æ
- **æ¬„ä½**: 10 å€‹æ¬„ä½ï¼ŒåŒ…å«æ¨¡å¼ä½¿ç”¨æ¬¡æ•¸ã€å“è³ªè©•åˆ†ã€åˆ†é¡
- **é©ç”¨**: ç³»çµ±å„ªåŒ–ã€æœå°‹ç­–ç•¥æ”¹é€²

### è§€å¯Ÿåå–®çµ±è¨ˆ (watchlist_summary_*.csv)
- **ç”¨é€”**: è§€å¯Ÿåå–®å…¬å¸è™•ç†ç‹€æ…‹åˆ†æ
- **æ¬„ä½**: 12 å€‹æ¬„ä½ï¼ŒåŒ…å«è™•ç†ç‹€æ…‹ã€å“è³ªè©•åˆ†ã€é—œéµå­—åˆ†æ
- **é©ç”¨**: è¦†è“‹ç‡ç›£æ§ã€å„ªå…ˆè™•ç†è¦åŠƒ

### é©—è­‰æ‘˜è¦ (validation_summary_*.csv)
- **ç”¨é€”**: æ•´é«”è™•ç†ç‹€æ…‹å’Œçµ±è¨ˆæ‘˜è¦
- **æ¬„ä½**: 5 å€‹æ¬„ä½ï¼ŒåŒ…å«çµ±è¨ˆé …ç›®ã€æ•¸å€¼ã€èªªæ˜
- **é©ç”¨**: ç³»çµ±å¥åº·åº¦ç›£æ§ã€è™•ç†å“è³ªè©•ä¼°

## ğŸ’¡ ä½¿ç”¨å»ºè­°

### Excel åˆ†æ
1. ä½¿ç”¨ Excel çš„æ¨ç´åˆ†æè¡¨åŠŸèƒ½é€²è¡Œæ·±åº¦åˆ†æ
2. å»ºç«‹åœ–è¡¨è¦–è¦ºåŒ–å‘ˆç¾è³‡æ–™
3. è¨­å®šæ¢ä»¶æ ¼å¼çªå‡ºé—œéµè³‡è¨Š

### è‡ªå‹•åŒ–è™•ç†
1. å¯ä»¥å¯«ç¨‹å¼å®šæœŸè™•ç† `*_latest.csv` æª”æ¡ˆ
2. å»ºç«‹ç›£æ§å„€è¡¨æ¿è®€å–é€™äº› CSV
3. è¨­å®šè­¦å ±ç³»çµ±ç›£æ§å“è³ªè©•åˆ†è®ŠåŒ–

### è³‡æ–™æ•´åˆ
1. ä½¿ç”¨ SQL è³‡æ–™åº«åŒ¯å…¥é€™äº› CSV é€²è¡Œè¤‡é›œæŸ¥è©¢
2. çµåˆå…¶ä»–è³‡æ–™æºé€²è¡Œäº¤å‰åˆ†æ
3. å»ºç«‹æ­·å²è³‡æ–™è¶¨å‹¢åˆ†æ

## âš ï¸ æ³¨æ„äº‹é …

### æª”æ¡ˆç·¨ç¢¼
- æ‰€æœ‰ CSV æª”æ¡ˆä½¿ç”¨ UTF-8-BOM ç·¨ç¢¼
- Excel é–‹å•Ÿæ™‚ä¸­æ–‡æ‡‰æ­£å¸¸é¡¯ç¤º
- å¦‚æœ‰äº‚ç¢¼ï¼Œè«‹ç¢ºèªç·¨ç¢¼è¨­å®š

### è³‡æ–™æ ¼å¼
- æ•¸å€¼æ¬„ä½å·²ç¶“æ ¼å¼åŒ–ï¼Œå¯ç›´æ¥ç”¨æ–¼è¨ˆç®—
- æ—¥æœŸæ ¼å¼ç‚º YYYY-MM-DD
- è‚¡ç¥¨ä»£è™Ÿç‚ºç´”æ•¸å­—æ ¼å¼

### æ›´æ–°é »ç‡
- æ™‚é–“æˆ³ç‰ˆæœ¬æª”æ¡ˆä¿ç•™æ­·å²è¨˜éŒ„
- `*_latest.csv` æª”æ¡ˆç¸½æ˜¯æœ€æ–°ç‰ˆæœ¬
- å»ºè­°å®šæœŸå‚™ä»½é‡è¦çš„æ™‚é–“æˆ³ç‰ˆæœ¬

---
FactSet Pipeline v3.6.1 - CSV Only Mode
é¿å… Google Sheets API é™åˆ¶ï¼Œæä¾›ç©©å®šå¯é çš„è¼¸å‡ºæ–¹æ¡ˆ
"""

        # å„²å­˜ä½¿ç”¨æŒ‡å— (åªç”Ÿæˆ latest ç‰ˆæœ¬)
        guide_path = os.path.join(self.validation_settings['csv_output_dir'], 'generation_guide_latest.md')

        with open(guide_path, 'w', encoding='utf-8') as f:
            f.write(guide_content)

    def _handle_validation_summary_v361_safe(self, portfolio_df: pd.DataFrame, detailed_df: pd.DataFrame, 
                                            watchlist_df: pd.DataFrame = None):
        """ğŸ”§ å®‰å…¨ç‰ˆæœ¬çš„é©—è­‰æ‘˜è¦è™•ç†"""
        try:
            # 1. ç”Ÿæˆé©—è­‰æ‘˜è¦æ•¸æ“š
            validation_data = self._generate_validation_summary_data_v361(portfolio_df, detailed_df, watchlist_df)
            
            # 2. ç”Ÿæˆ CSV æª”æ¡ˆï¼ˆç¸½æ˜¯åŸ·è¡Œï¼‰
            csv_file = self._save_validation_summary_csv(validation_data)
            if csv_file:
                print(f"ğŸ“Š é©—è­‰æ‘˜è¦ CSV å·²ç”Ÿæˆ: {os.path.basename(csv_file)}")
            
            # 3. å˜—è©¦ä¸Šå‚³åˆ° Google Sheetsï¼ˆå¯é¸ï¼Œæœ‰ API é™åˆ¶ä¿è­·ï¼‰
            if self.validation_settings.get('upload_validation_to_sheets', False):
                try:
                    if self._check_api_availability():
                        self._upload_validation_summary_simple(validation_data)
                        print("ğŸ“Š é©—è­‰æ‘˜è¦å·²ä¸Šå‚³åˆ° Google Sheets")
                    else:
                        print("âš ï¸ API é™åˆ¶ï¼Œè·³é Google Sheets é©—è­‰æ‘˜è¦ä¸Šå‚³")
                except Exception as e:
                    print(f"âš ï¸ Google Sheets é©—è­‰æ‘˜è¦ä¸Šå‚³å¤±æ•—: {e}")
                    print("ğŸ’¡ ä½† CSV æª”æ¡ˆå·²ç”Ÿæˆï¼Œå¯æ‰‹å‹•ä¸Šå‚³")
            
        except Exception as e:
            print(f"âš ï¸ é©—è­‰æ‘˜è¦è™•ç†å¤±æ•—: {e}")

    # ä¿æŒå…¶ä»–åŸæœ‰æ–¹æ³•ä¸è®Š...
    def _validate_before_upload_v361(self, portfolio_df: pd.DataFrame, detailed_df: pd.DataFrame, 
                                    watchlist_df: pd.DataFrame = None) -> Dict[str, Any]:
        """ä¸Šå‚³å‰é©—è­‰æª¢æŸ¥"""
        validation_result = {
            'safe_to_upload': True,
            'reason': '',
            'summary': {},
            'issues': [],
            'severity': 'info'
        }
        
        if portfolio_df.empty:
            validation_result['safe_to_upload'] = False
            validation_result['reason'] = 'æŠ•è³‡çµ„åˆæ‘˜è¦ç‚ºç©º'
            validation_result['severity'] = 'critical'
            return validation_result
        
        if detailed_df.empty:
            validation_result['safe_to_upload'] = False
            validation_result['reason'] = 'è©³ç´°å ±å‘Šç‚ºç©º'
            validation_result['severity'] = 'critical'
            return validation_result
        
        # é©—è­‰ç‹€æ…‹åˆ†æ
        validation_issues = []
        critical_issues = 0
        warning_issues = 0
        validation_disabled_count = 0
        
        if 'é©—è­‰ç‹€æ…‹' in detailed_df.columns:
            for idx, row in detailed_df.iterrows():
                validation_status = str(row.get('é©—è­‰ç‹€æ…‹', ''))
                company_name = row.get('åç¨±', 'Unknown')
                company_code = row.get('ä»£è™Ÿ', 'Unknown')
                
                if 'ğŸš«' in validation_status or 'âŒ' in validation_status:
                    critical_issues += 1
                    validation_issues.append({
                        'company': f"{company_name}({company_code})",
                        'type': 'critical',
                        'status': validation_status
                    })
                elif 'âš ï¸' in validation_status:
                    if 'é©—è­‰åœç”¨' in validation_status:
                        validation_disabled_count += 1
                    else:
                        warning_issues += 1
                    validation_issues.append({
                        'company': f"{company_name}({company_code})",
                        'type': 'warning',
                        'status': validation_status
                    })
        
        total_companies = len(detailed_df)
        
        # è§€å¯Ÿåå–®ç›¸é—œçµ±è¨ˆ
        watchlist_summary = {
            'watchlist_provided': watchlist_df is not None and not watchlist_df.empty,
            'watchlist_companies': 0,
            'coverage_rate': 0.0
        }
        
        if watchlist_df is not None and not watchlist_df.empty:
            watchlist_companies = len(watchlist_df)
            processed_companies = len([idx for idx, row in watchlist_df.iterrows() 
                                     if row.get('è™•ç†ç‹€æ…‹', '') == 'âœ… å·²è™•ç†'])
            
            watchlist_summary.update({
                'watchlist_companies': watchlist_companies,
                'processed_companies': processed_companies,
                'coverage_rate': (processed_companies / watchlist_companies) * 100 if watchlist_companies > 0 else 0
            })
        
        validation_result['summary'] = {
            'total_companies': total_companies,
            'critical_issues': critical_issues,
            'warning_issues': warning_issues,
            'validation_disabled': validation_disabled_count,
            'validation_issues': len(validation_issues),
            'watchlist_summary': watchlist_summary
        }
        
        validation_result['issues'] = validation_issues
        
        if self.validation_settings.get('enhanced_validation', False):
            if critical_issues > 0:
                validation_result['safe_to_upload'] = False
                validation_result['severity'] = 'critical'
                validation_result['reason'] = f'ç™¼ç¾ {critical_issues} å€‹é—œéµé©—è­‰å•é¡Œ'
            elif warning_issues > total_companies * 0.5:
                validation_result['safe_to_upload'] = False
                validation_result['severity'] = 'warning'
                validation_result['reason'] = f'è­¦å‘Šå•é¡Œéå¤š: {warning_issues}/{total_companies}'
            else:
                validation_result['safe_to_upload'] = True
                status_parts = []
                if validation_disabled_count > 0:
                    status_parts.append(f'{validation_disabled_count} å€‹é©—è­‰åœç”¨')
                if warning_issues > 0:
                    status_parts.append(f'{warning_issues} å€‹è­¦å‘Š')
                if watchlist_summary['watchlist_provided']:
                    status_parts.append(f"è§€å¯Ÿåå–®è¦†è“‹ç‡ {watchlist_summary['coverage_rate']:.1f}%")
                
                if status_parts:
                    validation_result['reason'] = f'ç™¼ç¾ {", ".join(status_parts)}ï¼Œå°‡ç¹¼çºŒä¸Šå‚³'
                    validation_result['severity'] = 'info'
        
        return validation_result

    def _mark_problematic_data_v361(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¨™è¨˜å•é¡Œè³‡æ–™"""
        df_marked = df.copy()
        
        if 'é©—è­‰ç‹€æ…‹' in df_marked.columns and 'åç¨±' in df_marked.columns:
            for idx, row in df_marked.iterrows():
                validation_status = str(row.get('é©—è­‰ç‹€æ…‹', ''))
                company_name = str(row.get('åç¨±', ''))
                
                if 'ğŸš«' in validation_status:
                    df_marked.at[idx, 'åç¨±'] = f"ğŸš« {company_name}"
                elif 'âŒ' in validation_status:
                    df_marked.at[idx, 'åç¨±'] = f"âŒ {company_name}"
                elif 'ğŸ“' in validation_status:
                    df_marked.at[idx, 'åç¨±'] = f"ğŸ“ {company_name}"
                elif 'ğŸ”„' in validation_status:
                    df_marked.at[idx, 'åç¨±'] = f"ğŸ”„ {company_name}"
                elif 'âš ï¸ é©—è­‰åœç”¨' in validation_status:
                    df_marked.at[idx, 'åç¨±'] = f"âš ï¸ {company_name}"
                elif 'âš ï¸' in validation_status:
                    df_marked.at[idx, 'åç¨±'] = f"âš ï¸ {company_name}"
        
        return df_marked

    def _generate_validation_summary_data_v361(self, portfolio_df: pd.DataFrame, detailed_df: pd.DataFrame, 
                                             watchlist_df: pd.DataFrame = None) -> pd.DataFrame:
        """ç”Ÿæˆé©—è­‰æ‘˜è¦æ•¸æ“š"""
        summary_rows = []
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # åŸºæœ¬çµ±è¨ˆ
        summary_rows.append({
            'é …ç›®': 'ç¸½å…¬å¸æ•¸',
            'æ•¸å€¼': len(portfolio_df),
            'èªªæ˜': 'æŠ•è³‡çµ„åˆä¸­çš„å…¬å¸ç¸½æ•¸',
            'è©³ç´°è³‡è¨Š': f'è©³ç´°è¨˜éŒ„: {len(detailed_df)}',
            'æ›´æ–°æ™‚é–“': current_time
        })
        
        # è§€å¯Ÿåå–®çµ±è¨ˆ
        if watchlist_df is not None and not watchlist_df.empty:
            total_watchlist = len(watchlist_df)
            processed_count = len([idx for idx, row in watchlist_df.iterrows() 
                                 if row.get('è™•ç†ç‹€æ…‹', '') == 'âœ… å·²è™•ç†'])
            not_found_count = len([idx for idx, row in watchlist_df.iterrows() 
                                 if row.get('è™•ç†ç‹€æ…‹', '') == 'âŒ æœªæ‰¾åˆ°'])
            coverage_rate = (processed_count / total_watchlist) * 100 if total_watchlist > 0 else 0
            
            summary_rows.extend([
                {
                    'é …ç›®': 'è§€å¯Ÿåå–®ç¸½æ•¸',
                    'æ•¸å€¼': total_watchlist,
                    'èªªæ˜': 'è§€å¯Ÿåå–®ä¸­çš„å…¬å¸ç¸½æ•¸',
                    'è©³ç´°è³‡è¨Š': f'é€™æ˜¯è™•ç†çš„åŸºæº–æ¸…å–®',
                    'æ›´æ–°æ™‚é–“': current_time
                },
                {
                    'é …ç›®': 'è§€å¯Ÿåå–®å·²è™•ç†',
                    'æ•¸å€¼': processed_count,
                    'èªªæ˜': 'è§€å¯Ÿåå–®ä¸­å·²æˆåŠŸè™•ç†çš„å…¬å¸æ•¸',
                    'è©³ç´°è³‡è¨Š': f'è¦†è“‹ç‡: {coverage_rate:.1f}%',
                    'æ›´æ–°æ™‚é–“': current_time
                },
                {
                    'é …ç›®': 'è§€å¯Ÿåå–®æœªæ‰¾åˆ°',
                    'æ•¸å€¼': not_found_count,
                    'èªªæ˜': 'è§€å¯Ÿåå–®ä¸­æœªæ‰¾åˆ°MDæª”æ¡ˆçš„å…¬å¸æ•¸',
                    'è©³ç´°è³‡è¨Š': f'éœ€è¦åŠ å¼·æœå°‹çš„å…¬å¸',
                    'æ›´æ–°æ™‚é–“': current_time
                }
            ])
        
        return pd.DataFrame(summary_rows)

    def _save_validation_summary_csv(self, validation_df: pd.DataFrame) -> str:
        """å„²å­˜é©—è­‰æ‘˜è¦ç‚º CSV æª”æ¡ˆ (åªç”Ÿæˆ latest ç‰ˆæœ¬)"""
        try:
            csv_path = os.path.join(self.validation_settings['csv_output_dir'], 'validation_summary_latest.csv')

            # å„²å­˜ CSV
            validation_df.to_csv(csv_path, index=False, encoding='utf-8-sig')

            return csv_path
            
        except Exception as e:
            print(f"âŒ å„²å­˜é©—è­‰æ‘˜è¦ CSV å¤±æ•—: {e}")
            return ""

    def _upload_validation_summary_simple(self, validation_df: pd.DataFrame):
        """ç°¡åŒ–ç‰ˆé©—è­‰æ‘˜è¦ä¸Šå‚³"""
        try:
            # å˜—è©¦æ‰¾åˆ°æˆ–å»ºç«‹é©—è­‰æ‘˜è¦å·¥ä½œè¡¨
            try:
                validation_worksheet = self.spreadsheet.worksheet(self.worksheet_names['validation'])
            except gspread.WorksheetNotFound:
                print("ğŸ“Š å»ºç«‹é©—è­‰æ‘˜è¦å·¥ä½œè¡¨...")
                validation_worksheet = self.spreadsheet.add_worksheet(title=self.worksheet_names['validation'], rows=200, cols=10)
            
            # æ¸…ç©ºç¾æœ‰å…§å®¹
            validation_worksheet.clear()
            time.sleep(self.api_settings['rate_limit_delay'])
            
            # æº–å‚™æ•¸æ“š
            headers = validation_df.columns.tolist()
            data = validation_df.values.tolist()
            
            # ç¢ºä¿æ‰€æœ‰æ•¸æ“šéƒ½æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼Œé¿å…æ ¼å¼å•é¡Œ
            clean_data = []
            for row in data:
                clean_row = []
                for cell in row:
                    if pd.isna(cell):
                        clean_row.append('')
                    else:
                        clean_row.append(str(cell))
                clean_data.append(clean_row)
            
            # ä¸Šå‚³æ¨™é¡Œ
            validation_worksheet.update('A1', [headers])
            time.sleep(self.api_settings['rate_limit_delay'])
            
            # ä¸Šå‚³æ•¸æ“š
            if clean_data:
                validation_worksheet.update('A2', clean_data)
                time.sleep(self.api_settings['rate_limit_delay'])
            
            # åªè¨­å®šæœ€åŸºæœ¬çš„æ¨™é¡Œæ ¼å¼
            try:
                validation_worksheet.format('A1:E1', {
                    'textFormat': {'bold': True}
                })
            except:
                pass
            
        except Exception as e:
            raise Exception(f"ç°¡åŒ–ä¸Šå‚³å¤±æ•—: {e}")

    # å…¶ä»–è¼”åŠ©æ–¹æ³•
    def _clean_stock_code(self, code):
        """æ¸…ç†è‚¡ç¥¨ä»£è™Ÿæ ¼å¼"""
        if pd.isna(code) or code is None:
            return ''
        
        code_str = str(code).strip()
        
        if code_str.startswith("'"):
            code_str = code_str[1:]
        
        if code_str.isdigit() and len(code_str) == 4:
            return int(code_str)
        
        if '-TW' in code_str:
            parts = code_str.split('-TW')
            if len(parts) == 2 and parts[0].isdigit() and len(parts[0]) == 4:
                return f"{int(parts[0])}-TW"
        
        return code_str

    def _format_numeric_value(self, value):
        """æ ¼å¼åŒ–æ•¸å€¼ï¼Œè™•ç† NaN å’Œç‰¹æ®Šå€¼"""
        if pd.isna(value) or value is None:
            return ''
        
        if isinstance(value, (int, float)):
            if math.isnan(value) or math.isinf(value):
                return ''
            if isinstance(value, float):
                if value.is_integer():
                    return str(int(value))
                else:
                    return f"{value:.2f}"
            else:
                return str(value)
        
        return str(value)

    def _ensure_json_compatible(self, value):
        """ç¢ºä¿å€¼èˆ‡ JSON ç›¸å®¹"""
        if pd.isna(value) or value is None:
            return ''
        
        if isinstance(value, (int, float)):
            if math.isnan(value) or math.isinf(value):
                return ''
            return str(value)
        
        str_value = str(value)
        if str_value.startswith("'"):
            str_value = str_value[1:]
        
        return str_value if str_value != '' else ''

    def _setup_connection(self) -> bool:
        """è¨­å®š Google Sheets é€£ç·š"""
        try:
            if not self.sheet_id:
                print("âŒ æœªè¨­å®š GOOGLE_SHEET_ID ç’°å¢ƒè®Šæ•¸")
                return False
            
            credentials_json = os.getenv('GOOGLE_SHEETS_CREDENTIALS')
            if not credentials_json:
                print("âŒ æœªè¨­å®š GOOGLE_SHEETS_CREDENTIALS ç’°å¢ƒè®Šæ•¸")
                return False
            
            credentials_info = json.loads(credentials_json)
            
            scopes = [
                'https://www.googleapis.com/auth/spreadsheets',
                'https://www.googleapis.com/auth/drive'
            ]
            
            credentials = Credentials.from_service_account_info(
                credentials_info, scopes=scopes
            )
            
            self.client = gspread.authorize(credentials)
            self.spreadsheet = self.client.open_by_key(self.sheet_id)
            
            return True
            
        except Exception as e:
            print(f"âŒ Google Sheets é€£ç·šè¨­å®šå¤±æ•—: {e}")
            return False

    def _upload_portfolio_summary(self, portfolio_df: pd.DataFrame) -> bool:
        """ä¸Šå‚³æŠ•è³‡çµ„åˆæ‘˜è¦"""
        try:
            try:
                portfolio_worksheet = self.spreadsheet.worksheet(self.worksheet_names['portfolio'])
            except gspread.WorksheetNotFound:
                print("ğŸ“Š å»ºç«‹æŠ•è³‡çµ„åˆæ‘˜è¦å·¥ä½œè¡¨...")
                portfolio_worksheet = self.spreadsheet.add_worksheet(title=self.worksheet_names['portfolio'], rows=1000, cols=20)
            
            portfolio_worksheet.clear()
            time.sleep(self.api_settings['rate_limit_delay'])
            
            portfolio_df_clean = portfolio_df.copy()
            portfolio_df_clean = portfolio_df_clean.fillna('')
            
            if 'ä»£è™Ÿ' in portfolio_df_clean.columns:
                portfolio_df_clean['ä»£è™Ÿ'] = portfolio_df_clean['ä»£è™Ÿ'].apply(self._clean_stock_code)
            
            if 'è‚¡ç¥¨ä»£è™Ÿ' in portfolio_df_clean.columns:
                portfolio_df_clean['è‚¡ç¥¨ä»£è™Ÿ'] = portfolio_df_clean['è‚¡ç¥¨ä»£è™Ÿ'].apply(self._clean_stock_code)
            
            numeric_columns = ['åˆ†æå¸«æ•¸é‡', 'ç›®æ¨™åƒ¹', '2025EPSå¹³å‡å€¼', '2026EPSå¹³å‡å€¼', '2027EPSå¹³å‡å€¼', 'å“è³ªè©•åˆ†']
            for col in numeric_columns:
                if col in portfolio_df_clean.columns:
                    portfolio_df_clean[col] = portfolio_df_clean[col].apply(self._format_numeric_value)
            
            headers = portfolio_df_clean.columns.tolist()
            data = portfolio_df_clean.values.tolist()
            
            data = [[self._ensure_json_compatible(cell) for cell in row] for row in data]
            
            portfolio_worksheet.update('A1', [headers])
            time.sleep(self.api_settings['rate_limit_delay'])
            
            if data:
                portfolio_worksheet.update('A2', data)
                time.sleep(self.api_settings['rate_limit_delay'])
            
            return True
            
        except Exception as e:
            print(f"âŒ æŠ•è³‡çµ„åˆæ‘˜è¦ä¸Šå‚³å¤±æ•—: {e}")
            return False

    def _upload_detailed_report(self, detailed_df: pd.DataFrame) -> bool:
        """ä¸Šå‚³è©³ç´°å ±å‘Š"""
        try:
            try:
                detailed_worksheet = self.spreadsheet.worksheet(self.worksheet_names['detailed'])
            except gspread.WorksheetNotFound:
                print("ğŸ“Š å»ºç«‹è©³ç´°å ±å‘Šå·¥ä½œè¡¨...")
                detailed_worksheet = self.spreadsheet.add_worksheet(title=self.worksheet_names['detailed'], rows=2000, cols=25)
            
            detailed_worksheet.clear()
            time.sleep(self.api_settings['rate_limit_delay'])
            
            detailed_df_clean = detailed_df.copy()
            detailed_df_clean = detailed_df_clean.fillna('')
            
            if 'ä»£è™Ÿ' in detailed_df_clean.columns:
                detailed_df_clean['ä»£è™Ÿ'] = detailed_df_clean['ä»£è™Ÿ'].apply(self._clean_stock_code)
            
            if 'è‚¡ç¥¨ä»£è™Ÿ' in detailed_df_clean.columns:
                detailed_df_clean['è‚¡ç¥¨ä»£è™Ÿ'] = detailed_df_clean['è‚¡ç¥¨ä»£è™Ÿ'].apply(self._clean_stock_code)
            
            numeric_columns = [
                'åˆ†æå¸«æ•¸é‡', 'ç›®æ¨™åƒ¹', 'å“è³ªè©•åˆ†',
                '2025EPSæœ€é«˜å€¼', '2025EPSæœ€ä½å€¼', '2025EPSå¹³å‡å€¼',
                '2026EPSæœ€é«˜å€¼', '2026EPSæœ€ä½å€¼', '2026EPSå¹³å‡å€¼',
                '2027EPSæœ€é«˜å€¼', '2027EPSæœ€ä½å€¼', '2027EPSå¹³å‡å€¼'
            ]
            for col in numeric_columns:
                if col in detailed_df_clean.columns:
                    detailed_df_clean[col] = detailed_df_clean[col].apply(self._format_numeric_value)
            
            headers = detailed_df_clean.columns.tolist()
            data = detailed_df_clean.values.tolist()
            
            data = [[self._ensure_json_compatible(cell) for cell in row] for row in data]
            
            detailed_worksheet.update('A1', [headers])
            time.sleep(self.api_settings['rate_limit_delay'])
            
            if data:
                # åˆ†æ‰¹ä¸Šå‚³å¤§é‡è³‡æ–™
                batch_size = self.api_settings['batch_size']
                for i in range(0, len(data), batch_size):
                    batch_data = data[i:i + batch_size]
                    start_row = i + 2
                    range_name = f'A{start_row}'
                    
                    detailed_worksheet.update(range_name, batch_data)
                    time.sleep(self.api_settings['rate_limit_delay'])
                    
                    if i + batch_size < len(data):
                        print(f"   å·²ä¸Šå‚³ {i + batch_size}/{len(data)} è¡Œ...")
            
            return True
            
        except Exception as e:
            print(f"âŒ è©³ç´°å ±å‘Šä¸Šå‚³å¤±æ•—: {e}")
            return False

    def _upload_keyword_summary(self, keyword_df: pd.DataFrame) -> bool:
        """ä¸Šå‚³é—œéµå­—çµ±è¨ˆå ±å‘Š"""
        try:
            try:
                worksheet = self.spreadsheet.worksheet(self.worksheet_names['keywords'])
            except gspread.WorksheetNotFound:
                print("ğŸ“Š å»ºç«‹é—œéµå­—å·¥ä½œè¡¨...")
                worksheet = self.spreadsheet.add_worksheet(
                    title=self.worksheet_names['keywords'], 
                    rows=1000, 
                    cols=12
                )
            
            worksheet.clear()
            time.sleep(self.api_settings['rate_limit_delay'])
            
            keyword_df_clean = keyword_df.copy()
            keyword_df_clean = keyword_df_clean.fillna('')
            
            numeric_columns = ['ä½¿ç”¨æ¬¡æ•¸', 'å¹³å‡å“è³ªè©•åˆ†', 'æœ€é«˜å“è³ªè©•åˆ†', 'æœ€ä½å“è³ªè©•åˆ†', 'ç›¸é—œå…¬å¸æ•¸é‡']
            for col in numeric_columns:
                if col in keyword_df_clean.columns:
                    keyword_df_clean[col] = keyword_df_clean[col].apply(self._format_numeric_value)
            
            headers = keyword_df_clean.columns.tolist()
            data = keyword_df_clean.values.tolist()
            
            data = [[self._ensure_json_compatible(cell) for cell in row] for row in data]
            
            worksheet.update('A1', [headers])
            time.sleep(self.api_settings['rate_limit_delay'])
            
            if data:
                worksheet.update('A2', data)
                time.sleep(self.api_settings['rate_limit_delay'])
            
            # è¨­å®šåŸºæœ¬æ ¼å¼
            try:
                worksheet.format('A1:J1', {
                    'backgroundColor': {'red': 0.2, 'green': 0.6, 'blue': 0.9},
                    'textFormat': {'bold': True, 'foregroundColor': {'red': 1, 'green': 1, 'blue': 1}}
                })
                time.sleep(self.api_settings['rate_limit_delay'])
            except:
                pass
            
            return True
            
        except Exception as e:
            print(f"âŒ é—œéµå­—å ±å‘Šä¸Šå‚³å¤±æ•—: {e}")
            return False

    def test_connection(self) -> bool:
        """æ¸¬è©¦ Google Sheets é€£ç·š"""
        try:
            if self._setup_connection():
                spreadsheet_info = self.spreadsheet.title
                print(f"âœ… Google Sheets é€£ç·šæˆåŠŸ: {spreadsheet_info}")
                return True
            else:
                print("âŒ Google Sheets é€£ç·šå¤±æ•—")
                return False
        except Exception as e:
            print(f"âŒ Google Sheets é€£ç·šæ¸¬è©¦å¤±æ•—: {e}")
            return False

    # ğŸ†• å…¬ç”¨æ–¹æ³•ï¼šåªç”Ÿæˆ CSVï¼Œä¸ä¸Šå‚³
    def generate_csv_only(self, portfolio_df: pd.DataFrame, detailed_df: pd.DataFrame, 
                         keyword_df: pd.DataFrame = None, watchlist_df: pd.DataFrame = None) -> bool:
        """ğŸ†• å…¬ç”¨æ–¹æ³•ï¼šåªç”Ÿæˆ CSV æª”æ¡ˆï¼Œä¸ä¸Šå‚³åˆ° Google Sheets"""
        return self._csv_only_mode(portfolio_df, detailed_df, keyword_df, watchlist_df)


# æ¸¬è©¦åŠŸèƒ½
if __name__ == "__main__":
    uploader = SheetsUploader()
    
    print("=== ğŸ”§ ä¿®å¾©ç‰ˆ Sheets Uploader v3.6.1 æ¸¬è©¦ ===")
    
    # æ¸¬è©¦è³‡æ–™
    import pandas as pd
    
    test_portfolio = pd.DataFrame([
        {'ä»£è™Ÿ': '2330', 'åç¨±': 'å°ç©é›»', 'å“è³ªè©•åˆ†': 10.0, 'ç‹€æ…‹': 'ğŸŸ¢ å®Œæ•´'},
        {'ä»£è™Ÿ': '6462', 'åç¨±': 'ç¥ç›¾', 'å“è³ªè©•åˆ†': 7.0, 'ç‹€æ…‹': 'ğŸŸ¡ è‰¯å¥½'}
    ])
    
    test_detailed = pd.DataFrame([
        {'ä»£è™Ÿ': '2330', 'åç¨±': 'å°ç©é›»', 'å“è³ªè©•åˆ†': 10.0, 'é©—è­‰ç‹€æ…‹': 'âœ… é€šé'},
        {'ä»£è™Ÿ': '6462', 'åç¨±': 'ç¥ç›¾', 'å“è³ªè©•åˆ†': 7.0, 'é©—è­‰ç‹€æ…‹': 'âš ï¸ é©—è­‰åœç”¨'}
    ])
    
    test_watchlist = pd.DataFrame([
        {
            'å…¬å¸ä»£è™Ÿ': '2330',
            'å…¬å¸åç¨±': 'å°ç©é›»',
            'MDæª”æ¡ˆæ•¸é‡': 2,
            'è™•ç†ç‹€æ…‹': 'âœ… å·²è™•ç†',
            'å¹³å‡å“è³ªè©•åˆ†': 9.2,
            'æœ€é«˜å“è³ªè©•åˆ†': 10.0,
            'æœå°‹é—œéµå­—æ•¸é‡': 4,
            'ä¸»è¦é—œéµå­—': 'å°ç©é›», factset, eps',
            'é—œéµå­—å¹³å‡å“è³ª': 8.5,
            'æœ€æ–°æª”æ¡ˆæ—¥æœŸ': '2025-06-24',
            'é©—è­‰ç‹€æ…‹': 'âœ… é©—è­‰é€šé'
        }
    ])
    
    print("æ¸¬è©¦ 1: CSV-only æ¨¡å¼")
    success = uploader.generate_csv_only(test_portfolio, test_detailed, None, test_watchlist)
    if success:
        print("   âœ… CSV-only æ¨¡å¼æ¸¬è©¦æˆåŠŸ")
    else:
        print("   âŒ CSV-only æ¨¡å¼æ¸¬è©¦å¤±æ•—")
    
    print("\næ¸¬è©¦ 2: ä¸Šå‚³å‰é©—è­‰")
    validation_result = uploader._validate_before_upload_v361(test_portfolio, test_detailed, test_watchlist)
    print(f"   é©—è­‰çµæœ: {validation_result['safe_to_upload']}")
    print(f"   é©—è­‰æ‘˜è¦: {validation_result['summary']}")
    
    print("\næ¸¬è©¦ 3: API å¯ç”¨æ€§æª¢æŸ¥")
    api_available = uploader._check_api_availability()
    print(f"   API å¯ç”¨: {api_available}")
    
    print(f"\nğŸ‰ ä¿®å¾©ç‰ˆ Sheets Uploader v3.6.1 æ¸¬è©¦å®Œæˆ!")
    print(f"âœ… ä¿®å¾© columnWidth API éŒ¯èª¤")
    print(f"âœ… å¢åŠ  API é™åˆ¶ä¿è­·æ©Ÿåˆ¶")
    print(f"âœ… æ”¯æ´ CSV-only æ¨¡å¼")
    print(f"âœ… å¢å¼·éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶")
    print(f"âœ… è‡ªå‹• fallback åˆ° CSV æ¨¡å¼")
    
    print(f"\nğŸ’¡ ä½¿ç”¨å»ºè­°:")
    print(f"   å°æ–¼å¤§é‡è³‡æ–™æˆ– API é™åˆ¶æƒ…æ³ï¼Œä½¿ç”¨ csv_only=True")
    print(f"   ç³»çµ±æœƒè‡ªå‹•æª¢æ¸¬ API å•é¡Œä¸¦åˆ‡æ›åˆ° CSV æ¨¡å¼")
    print(f"   æ‰€æœ‰ CSV æª”æ¡ˆéƒ½æœ‰å®Œæ•´çš„ä½¿ç”¨æŒ‡å—")
