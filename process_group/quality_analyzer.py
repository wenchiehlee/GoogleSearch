#!/usr/bin/env python3
"""
Quality Analyzer - FactSet Pipeline v3.5.0 (Enhanced with Content Validation)
å¼·åŒ–å“è³ªè©•åˆ†ç³»çµ±ï¼Œæ•´åˆå…§å®¹é©—è­‰çµæœ
"""

import re
from datetime import datetime, timedelta
from typing import Dict, Any, List, Tuple

class QualityAnalyzer:
    """å“è³ªåˆ†æå™¨ - å¼·åŒ–é©—è­‰ç‰ˆæœ¬"""
    
    # å“è³ªç¯„åœå®šç¾© - æ­£ç¢ºè™•ç† 8.9 åˆ†
    QUALITY_RANGES = {
        'complete': (9.0, 10.0),      # ğŸŸ¢ å®Œæ•´ (9.0-10.0)
        'good': (8.0, 8.99),          # ğŸŸ¡ è‰¯å¥½ (8.0-8.99)
        'partial': (3.0, 7.99),       # ğŸŸ  éƒ¨åˆ† (3.0-7.99)
        'insufficient': (0.0, 2.99)   # ğŸ”´ ä¸è¶³ (0.0-2.99)
    }
    
    # å“è³ªç‹€æ…‹æŒ‡æ¨™
    QUALITY_INDICATORS = {
        'complete': 'ğŸŸ¢ å®Œæ•´',
        'good': 'ğŸŸ¡ è‰¯å¥½',
        'partial': 'ğŸŸ  éƒ¨åˆ†',
        'insufficient': 'ğŸ”´ ä¸è¶³'
    }
    
    # ğŸ†• æ›´æ–°è©•åˆ†æ¬Šé‡ - åŠ å…¥å…§å®¹é©—è­‰
    QUALITY_WEIGHTS = {
        'data_completeness': 0.30,    # è³‡æ–™å®Œæ•´æ€§ 30% (é™ä½)
        'analyst_coverage': 0.20,     # åˆ†æå¸«è¦†è“‹ 20% (é™ä½)
        'data_freshness': 0.15,       # è³‡æ–™æ–°é®®åº¦ 15% (é™ä½)
        'content_quality': 0.15,      # å…§å®¹å“è³ª 15% (é™ä½)
        'data_consistency': 0.05,     # è³‡æ–™ä¸€è‡´æ€§ 5% (ä¿æŒ)
        'content_validation': 0.15    # ğŸ†• å…§å®¹é©—è­‰ 15% (æ–°å¢)
    }
    
    def __init__(self):
        """åˆå§‹åŒ–å“è³ªåˆ†æå™¨"""
        self.financial_keywords = [
            'eps', 'æ¯è‚¡ç›ˆé¤˜', 'ç‡Ÿæ”¶', 'ç‡Ÿæ¥­æ”¶å…¥', 'æ·¨åˆ©', 'ç²åˆ©',
            'ç›®æ¨™åƒ¹', 'è©•ç­‰', 'åˆ†æå¸«', 'é ä¼°', 'é æ¸¬', 'factset',
            'revenue', 'earnings', 'profit', 'target', 'analyst'
        ]

    def analyze(self, parsed_data: Dict[str, Any]) -> Dict[str, Any]:
        """ä¸»è¦åˆ†ææ–¹æ³• - æ•´åˆå…§å®¹é©—è­‰"""
        try:
            # å„ç¶­åº¦åˆ†æ
            completeness_analysis = self._analyze_data_completeness(parsed_data)
            coverage_analysis = self._analyze_analyst_coverage(parsed_data)
            freshness_analysis = self._analyze_data_freshness(parsed_data)
            content_analysis = self._analyze_content_quality(parsed_data)
            consistency_analysis = self._analyze_data_consistency(parsed_data)
            # ğŸ†• æ–°å¢ï¼šå…§å®¹é©—è­‰åˆ†æ
            validation_analysis = self._analyze_content_validation(parsed_data)
            
            # è¨ˆç®—åŠ æ¬Šç¸½åˆ†
            total_score = (
                completeness_analysis['score'] * self.QUALITY_WEIGHTS['data_completeness'] +
                coverage_analysis['score'] * self.QUALITY_WEIGHTS['analyst_coverage'] +
                freshness_analysis['score'] * self.QUALITY_WEIGHTS['data_freshness'] +
                content_analysis['score'] * self.QUALITY_WEIGHTS['content_quality'] +
                consistency_analysis['score'] * self.QUALITY_WEIGHTS['data_consistency'] +
                validation_analysis['score'] * self.QUALITY_WEIGHTS['content_validation']
            )
            
            # ç¢ºä¿åˆ†æ•¸åœ¨ 0-10 ç¯„åœå…§
            quality_score = round(min(max(total_score, 0), 10), 1)
            
            # ğŸ†• ç‰¹æ®Šè™•ç†ï¼šå¦‚æœå…§å®¹é©—è­‰åš´é‡å¤±æ•—ï¼Œç›´æ¥é™ç´š
            validation_result = parsed_data.get('validation_result', {})
            if validation_result.get('overall_status') == 'error':
                # å¦‚æœæ˜¯åš´é‡çš„é©—è­‰éŒ¯èª¤ï¼ˆå¦‚æ„›æ´¾å¸/æ„›ç«‹ä¿¡å•é¡Œï¼‰ï¼Œç›´æ¥è¨­ç‚ºä½åˆ†
                if any('æ„›ç«‹ä¿¡' in str(error) for error in validation_result.get('errors', [])):
                    quality_score = min(quality_score, 2.0)  # å¼·åˆ¶é™åˆ°ä¸è¶³ç´šåˆ¥
            
            # ç¢ºå®šå“è³ªé¡åˆ¥å’Œç‹€æ…‹
            quality_category = self._determine_quality_category_fixed(quality_score)
            quality_status = self.QUALITY_INDICATORS[quality_category]
            
            # ç”Ÿæˆæ‘˜è¦æŒ‡æ¨™
            summary_metrics = self._generate_summary_metrics(parsed_data)
            
            return {
                'quality_score': quality_score,
                'quality_status': quality_status,
                'quality_category': quality_category,
                'analysis_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                
                'detailed_analysis': {
                    'data_completeness': completeness_analysis,
                    'analyst_coverage': coverage_analysis,
                    'data_freshness': freshness_analysis,
                    'content_quality': content_analysis,
                    'data_consistency': consistency_analysis,
                    'content_validation': validation_analysis  # ğŸ†• æ–°å¢é©—è­‰åˆ†æ
                },
                
                'summary_metrics': summary_metrics,
                
                # ğŸ†• é©—è­‰ç‹€æ…‹æ‘˜è¦
                'validation_summary': {
                    'validation_passed': parsed_data.get('content_validation_passed', True),
                    'validation_warnings': parsed_data.get('validation_warnings', []),
                    'validation_errors': parsed_data.get('validation_errors', []),
                    'has_validation_issues': len(parsed_data.get('validation_errors', [])) > 0
                }
            }
            
        except Exception as e:
            print(f"âŒ å“è³ªåˆ†æå¤±æ•—: {e}")
            return self._create_empty_analysis(str(e))

    def _analyze_content_validation(self, data: Dict) -> Dict:
        """ğŸ†• åˆ†æå…§å®¹é©—è­‰çµæœ (15% æ¬Šé‡)"""
        score = 8.0  # é è¨­é«˜åˆ†ï¼Œæœ‰å•é¡Œæ‰æ‰£åˆ†
        details = []
        metrics = {}
        
        validation_result = data.get('validation_result', {})
        validation_status = validation_result.get('overall_status', 'unknown')
        validation_confidence = validation_result.get('confidence_score', 10.0)
        validation_errors = data.get('validation_errors', [])
        validation_warnings = data.get('validation_warnings', [])
        
        # æ ¹æ“šé©—è­‰ç‹€æ…‹è©•åˆ†
        if validation_status == 'valid':
            score = 10.0
            details.append("âœ… å…§å®¹é©—è­‰é€šé")
        elif validation_status == 'warning':
            score = 6.0
            details.append(f"ğŸŸ¡ å…§å®¹é©—è­‰æœ‰è­¦å‘Š ({len(validation_warnings)} é …)")
        elif validation_status == 'error':
            score = 1.0
            details.append(f"âŒ å…§å®¹é©—è­‰å¤±æ•— ({len(validation_errors)} é …éŒ¯èª¤)")
        else:
            score = 5.0
            details.append("âš ï¸ å…§å®¹é©—è­‰ç‹€æ…‹æœªçŸ¥")
        
        # ğŸš¨ ç‰¹æ®Šè™•ç†ï¼šæ„›æ´¾å¸/æ„›ç«‹ä¿¡å•é¡Œ
        has_company_mismatch = False
        for error in validation_errors:
            if 'æ„›æ´¾å¸' in str(error) and 'æ„›ç«‹ä¿¡' in str(error):
                score = 0.0  # å®Œå…¨é›¶åˆ†
                has_company_mismatch = True
                details.append("ğŸš¨ åš´é‡éŒ¯èª¤ï¼šå…¬å¸è³‡è¨Šå®Œå…¨ä¸ç¬¦")
                break
        
        # æ ¹æ“šä¿¡å¿ƒåˆ†æ•¸èª¿æ•´
        if validation_confidence < 5.0:
            score = min(score, 3.0)
            details.append(f"ğŸ”´ é©—è­‰ä¿¡å¿ƒåº¦æ¥µä½ ({validation_confidence})")
        elif validation_confidence < 7.0:
            score = min(score, 6.0)
            details.append(f"ğŸŸ  é©—è­‰ä¿¡å¿ƒåº¦è¼ƒä½ ({validation_confidence})")
        
        # æª¢æŸ¥å…·é«”çš„é©—è­‰å•é¡Œ
        if validation_result.get('ericsson_detected') and not has_company_mismatch:
            score -= 2.0
            details.append("âš ï¸ åµæ¸¬åˆ°æ„›ç«‹ä¿¡ç›¸é—œå…§å®¹")
        
        detected_regions = validation_result.get('detected_regions', [])
        if 'taiwan_expected' in detected_regions:
            us_codes = validation_result.get('detected_stock_codes', {}).get('us', [])
            if us_codes:
                score -= 1.5
                details.append(f"ğŸŸ  å°è‚¡æª”æ¡ˆåŒ…å«ç¾è‚¡ä»£è™Ÿ: {us_codes}")
        
        # è¨˜éŒ„é©—è­‰æŒ‡æ¨™
        metrics.update({
            'validation_status': validation_status,
            'validation_confidence': validation_confidence,
            'error_count': len(validation_errors),
            'warning_count': len(validation_warnings),
            'has_company_mismatch': has_company_mismatch,
            'ericsson_detected': validation_result.get('ericsson_detected', False)
        })
        
        return {
            'score': round(max(0, min(score, 10)), 2),
            'details': details,
            'metrics': metrics
        }

    def _determine_quality_category_fixed(self, score: float) -> str:
        """ç¢ºå®šå“è³ªé¡åˆ¥ - æ­£ç¢ºè™•ç†é‚Šç•Œå€¼"""
        for category, (min_score, max_score) in self.QUALITY_RANGES.items():
            if min_score <= score <= max_score:
                return category
        
        # å¦‚æœæ²’æœ‰åŒ¹é…åˆ°ï¼ŒæŒ‰ç…§åˆ†æ•¸ç¯„åœåˆ¤æ–·
        if score >= 9.0:
            return 'complete'
        elif score >= 8.0:
            return 'good'
        elif score >= 3.0:
            return 'partial'
        else:
            return 'insufficient'

    # åŸæœ‰çš„åˆ†ææ–¹æ³•ä¿æŒåŸºæœ¬ä¸è®Šï¼Œä½†å¯èƒ½æœƒæ ¹æ“šé©—è­‰çµæœèª¿æ•´

    def _analyze_data_completeness(self, data: Dict) -> Dict:
        """åˆ†æè³‡æ–™å®Œæ•´æ€§ (30% æ¬Šé‡ï¼Œç•¥é™)"""
        score = 0
        details = []
        metrics = {}
        
        # EPS è³‡æ–™å®Œæ•´æ€§ (40% of this dimension)
        eps_years = ['2025', '2026', '2027']
        eps_available = 0
        
        for year in eps_years:
            if data.get(f'eps_{year}_avg') is not None:
                eps_available += 1
        
        eps_completeness = eps_available / len(eps_years)
        score += eps_completeness * 4.0
        
        if eps_available == 3:
            details.append("âœ… EPS é æ¸¬å®Œæ•´ (2025-2027)")
        elif eps_available == 2:
            details.append("ğŸŸ¡ EPS é æ¸¬éƒ¨åˆ†å®Œæ•´")
        elif eps_available == 1:
            details.append("ğŸŸ  EPS é æ¸¬è³‡æ–™æœ‰é™")
        else:
            details.append("âŒ ç¼ºå°‘ EPS é æ¸¬è³‡æ–™")
        
        metrics['eps_years_available'] = eps_available
        
        # ç›®æ¨™åƒ¹æ ¼ (30% of this dimension)
        if data.get('target_price') is not None:
            score += 3.0
            details.append("âœ… ç›®æ¨™åƒ¹æ ¼è³‡è¨Šå®Œæ•´")
            metrics['has_target_price'] = True
        else:
            details.append("âŒ ç¼ºå°‘ç›®æ¨™åƒ¹æ ¼")
            metrics['has_target_price'] = False
        
        # åˆ†æå¸«æ•¸é‡ (20% of this dimension)
        analyst_count = data.get('analyst_count', 0)
        if analyst_count >= 10:
            score += 2.0
            details.append(f"âœ… åˆ†æå¸«è¦†è“‹å……è¶³ ({analyst_count}ä½)")
        elif analyst_count >= 5:
            score += 1.5
            details.append(f"ğŸŸ¡ åˆ†æå¸«è¦†è“‹ä¸€èˆ¬ ({analyst_count}ä½)")
        elif analyst_count > 0:
            score += 1.0
            details.append(f"ğŸŸ  åˆ†æå¸«è¦†è“‹æœ‰é™ ({analyst_count}ä½)")
        else:
            details.append("âŒ ç¼ºå°‘åˆ†æå¸«è³‡è¨Š")
        
        metrics['analyst_count'] = analyst_count
        
        # åŸºæœ¬è³‡è¨Š (10% of this dimension)
        basic_info_score = 0
        if data.get('company_code'):
            basic_info_score += 0.5
        if data.get('company_name'):
            basic_info_score += 0.5
        
        score += basic_info_score * 1.0
        
        if basic_info_score == 1.0:
            details.append("âœ… åŸºæœ¬å…¬å¸è³‡è¨Šå®Œæ•´")
        else:
            details.append("ğŸŸ  åŸºæœ¬å…¬å¸è³‡è¨Šä¸å®Œæ•´")
        
        metrics['basic_info_complete'] = basic_info_score == 1.0
        
        return {
            'score': round(min(score, 10), 2),
            'details': details,
            'metrics': metrics
        }

    def _analyze_analyst_coverage(self, data: Dict) -> Dict:
        """åˆ†æåˆ†æå¸«è¦†è“‹åº¦ (20% æ¬Šé‡ï¼Œç•¥é™)"""
        score = 0
        details = []
        metrics = {}
        
        analyst_count = data.get('analyst_count', 0)
        
        # åˆ†æå¸«æ•¸é‡è©•åˆ† (70% of this dimension)
        if analyst_count >= 30:
            score += 7.0
            details.append(f"ğŸŒŸ å„ªç§€çš„åˆ†æå¸«è¦†è“‹ ({analyst_count}ä½)")
        elif analyst_count >= 20:
            score += 6.0
            details.append(f"âœ… è‰¯å¥½çš„åˆ†æå¸«è¦†è“‹ ({analyst_count}ä½)")
        elif analyst_count >= 15:
            score += 5.0
            details.append(f"ğŸŸ¡ é©ä¸­çš„åˆ†æå¸«è¦†è“‹ ({analyst_count}ä½)")
        elif analyst_count >= 10:
            score += 4.0
            details.append(f"ğŸŸ  åŸºæœ¬çš„åˆ†æå¸«è¦†è“‹ ({analyst_count}ä½)")
        elif analyst_count >= 5:
            score += 2.5
            details.append(f"ğŸ”´ æœ‰é™çš„åˆ†æå¸«è¦†è“‹ ({analyst_count}ä½)")
        elif analyst_count > 0:
            score += 1.0
            details.append(f"âš ï¸ æ¥µå°‘çš„åˆ†æå¸«è¦†è“‹ ({analyst_count}ä½)")
        else:
            details.append("âŒ ç„¡åˆ†æå¸«è¦†è“‹è³‡è¨Š")
        
        metrics['analyst_count'] = analyst_count
        
        # è³‡æ–™ä¾†æºå“è³ª (30% of this dimension)
        data_source = data.get('data_source', '').lower()
        if 'factset' in data_source:
            score += 3.0
            details.append("âœ… é«˜å“è³ªè³‡æ–™ä¾†æº (FactSet)")
            metrics['data_source_quality'] = 'high'
        elif any(source in data_source for source in ['bloomberg', 'refinitiv', 'reuters']):
            score += 2.5
            details.append("ğŸŸ¡ ä¸­ç­‰å“è³ªè³‡æ–™ä¾†æº")
            metrics['data_source_quality'] = 'medium'
        elif data_source:
            score += 1.5
            details.append("ğŸŸ  ä¸€èˆ¬è³‡æ–™ä¾†æº")
            metrics['data_source_quality'] = 'basic'
        else:
            details.append("âŒ æœªçŸ¥è³‡æ–™ä¾†æº")
            metrics['data_source_quality'] = 'unknown'
        
        return {
            'score': round(min(score, 10), 2),
            'details': details,
            'metrics': metrics
        }

    def _analyze_data_freshness(self, data: Dict) -> Dict:
        """åˆ†æè³‡æ–™æ–°é®®åº¦ (15% æ¬Šé‡ï¼Œç•¥é™)"""
        score = 0
        details = []
        metrics = {}
        
        # å–å¾—å…§å®¹æ—¥æœŸæˆ–æª”æ¡ˆä¿®æ”¹æ™‚é–“
        content_date = data.get('content_date')
        file_mtime = data.get('file_mtime')
        
        reference_date = content_date or file_mtime
        
        if reference_date:
            if isinstance(reference_date, str):
                try:
                    # è™•ç†ä¸åŒçš„æ—¥æœŸæ ¼å¼
                    if '/' in reference_date:
                        # 2025/6/6 æ ¼å¼
                        date_parts = reference_date.split('/')
                        if len(date_parts) == 3:
                            year, month, day = date_parts
                            reference_date = datetime(int(year), int(month), int(day))
                    else:
                        reference_date = datetime.fromisoformat(reference_date.replace('Z', '+00:00'))
                except:
                    reference_date = None
            
            if reference_date:
                now = datetime.now()
                if reference_date.tzinfo is None:
                    reference_date = reference_date.replace(tzinfo=None)
                    now = now.replace(tzinfo=None)
                
                age_days = (now - reference_date).days
                
                # è³‡æ–™æ–°é®®åº¦è©•åˆ†
                if age_days <= 7:
                    score += 8
                    details.append(f"âœ… è³‡æ–™éå¸¸æ–°é®® ({age_days} å¤©)")
                elif age_days <= 30:
                    score += 7
                    details.append(f"âœ… è³‡æ–™æ–°é®® ({age_days} å¤©)")
                elif age_days <= 90:
                    score += 5
                    details.append(f"ğŸŸ¡ è³‡æ–™è¼ƒæ–° ({age_days} å¤©)")
                elif age_days <= 180:
                    score += 3
                    details.append(f"ğŸŸ  è³‡æ–™è¼ƒèˆŠ ({age_days} å¤©)")
                else:
                    score += 1
                    details.append(f"ğŸ”´ è³‡æ–™éèˆŠ ({age_days} å¤©)")
                
                metrics['age_days'] = age_days
        else:
            score += 2
            details.append("âš ï¸ ç„¡æ³•ç¢ºå®šè³‡æ–™æ™‚é–“")
        
        return {
            'score': round(min(score, 10), 2),
            'details': details,
            'metrics': metrics
        }

    def _analyze_content_quality(self, data: Dict) -> Dict:
        """åˆ†æå…§å®¹å“è³ª (15% æ¬Šé‡ï¼Œç•¥é™)"""
        score = 0
        details = []
        metrics = {}
        
        content = str(data.get('content', ''))
        content_length = len(content)
        
        # å…§å®¹é•·åº¦è©•åˆ†
        if content_length >= 5000:
            score += 4
            details.append(f"âœ… å…§å®¹è±å¯Œ ({content_length} å­—å…ƒ)")
        elif content_length >= 2000:
            score += 3
            details.append(f"ğŸŸ¡ å…§å®¹é©ä¸­ ({content_length} å­—å…ƒ)")
        elif content_length >= 500:
            score += 2
            details.append(f"ğŸŸ  å…§å®¹è¼ƒå°‘ ({content_length} å­—å…ƒ)")
        else:
            score += 1
            details.append(f"ğŸ”´ å…§å®¹éå°‘ ({content_length} å­—å…ƒ)")
        
        # è²¡å‹™é—œéµå­—æª¢æŸ¥
        keyword_count = sum(1 for keyword in self.financial_keywords if keyword in content.lower())
        
        if keyword_count >= 8:
            score += 4
            details.append(f"âœ… è²¡å‹™é—œéµå­—è±å¯Œ ({keyword_count})")
        elif keyword_count >= 5:
            score += 3
            details.append(f"ğŸŸ¡ è²¡å‹™é—œéµå­—ä¸€èˆ¬ ({keyword_count})")
        elif keyword_count >= 2:
            score += 2
            details.append(f"ğŸŸ  è²¡å‹™é—œéµå­—è¼ƒå°‘ ({keyword_count})")
        else:
            score += 1
            details.append(f"ğŸ”´ è²¡å‹™é—œéµå­—ç¨€å°‘ ({keyword_count})")
        
        # ğŸ”§ ç§»é™¤å° "Oops, something went wrong" çš„æª¢æŸ¥ - é€™å¾ˆå¸¸è¦‹ï¼Œä¸éœ€è¦ç‰¹åˆ¥è™•ç†
        
        metrics['content_length'] = content_length
        metrics['keyword_count'] = keyword_count
        
        return {
            'score': round(min(score, 10), 2),
            'details': details,
            'metrics': metrics
        }

    def _analyze_data_consistency(self, data: Dict) -> Dict:
        """åˆ†æè³‡æ–™ä¸€è‡´æ€§ (5% æ¬Šé‡ï¼Œä¿æŒ)"""
        score = 8  # é è¨­é«˜åˆ†ï¼Œæœ‰å•é¡Œæ‰æ‰£åˆ†
        details = []
        issues = []
        
        # æª¢æŸ¥ EPS è¶¨å‹¢ä¸€è‡´æ€§
        eps_2025 = data.get('eps_2025_avg')
        eps_2026 = data.get('eps_2026_avg')
        eps_2027 = data.get('eps_2027_avg')
        
        if eps_2025 and eps_2026 and eps_2027:
            # æª¢æŸ¥ç•°å¸¸çš„ EPS è®ŠåŒ–
            if abs(eps_2026 - eps_2025) > eps_2025 * 2:
                score -= 2
                issues.append("EPS 2025-2026 è®ŠåŒ–ç•°å¸¸")
            
            if abs(eps_2027 - eps_2026) > eps_2026 * 2:
                score -= 2
                issues.append("EPS 2026-2027 è®ŠåŒ–ç•°å¸¸")
        
        # æª¢æŸ¥å…¬å¸è³‡è¨Šä¸€è‡´æ€§
        company_code = data.get('company_code', '')
        if company_code and not (company_code.isdigit() and len(company_code) == 4):
            score -= 1
            issues.append("å…¬å¸ä»£è™Ÿæ ¼å¼ç•°å¸¸")
        
        # æª¢æŸ¥ç›®æ¨™åƒ¹æ ¼åˆç†æ€§
        target_price = data.get('target_price')
        if target_price and (target_price <= 0 or target_price > 10000):
            score -= 1
            issues.append("ç›®æ¨™åƒ¹æ ¼æ•¸å€¼ç•°å¸¸")
        
        if issues:
            details.append(f"âŒ ç™¼ç¾ {len(issues)} å€‹ä¸€è‡´æ€§å•é¡Œ")
            details.extend([f"  - {issue}" for issue in issues])
        else:
            details.append("âœ… è³‡æ–™ä¸€è‡´æ€§è‰¯å¥½")
        
        return {
            'score': max(0, min(10, score)),
            'details': details,
            'metrics': {'consistency_issues': issues}
        }

    def _generate_summary_metrics(self, data: Dict) -> Dict:
        """ç”Ÿæˆæ‘˜è¦æŒ‡æ¨™ - åŒ…å«é©—è­‰è³‡è¨Š"""
        return {
            'eps_data_available': any(data.get(f'eps_{year}_avg') is not None for year in ['2025', '2026', '2027']),
            'target_price_available': data.get('target_price') is not None,
            'analyst_count': data.get('analyst_count', 0),
            'content_length': len(str(data.get('content', ''))),
            'financial_keywords_found': sum(1 for keyword in self.financial_keywords if keyword in str(data.get('content', '')).lower()),
            # ğŸ†• é©—è­‰ç›¸é—œæŒ‡æ¨™
            'content_validation_passed': data.get('content_validation_passed', True),
            'validation_error_count': len(data.get('validation_errors', [])),
            'validation_warning_count': len(data.get('validation_warnings', []))
        }

    def _create_empty_analysis(self, error_msg: str) -> Dict:
        """å»ºç«‹ç©ºçš„åˆ†æçµæœ"""
        return {
            'quality_score': 0,
            'quality_status': 'ğŸ”´ ä¸è¶³',
            'quality_category': 'insufficient',
            'error': error_msg,
            'analysis_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'validation_summary': {
                'validation_passed': False,
                'validation_errors': [error_msg],
                'has_validation_issues': True
            }
        }


# æ¸¬è©¦åŠŸèƒ½
if __name__ == "__main__":
    analyzer = QualityAnalyzer()
    
    print("=== ğŸ”’ å¼·åŒ–ç‰ˆå“è³ªåˆ†æå™¨æ¸¬è©¦ (å…§å®¹é©—è­‰) ===")
    
    # æ¸¬è©¦æ„›æ´¾å¸ vs æ„›ç«‹ä¿¡å•é¡Œçš„å“è³ªè©•åˆ†
    test_data_error = {
        'company_code': '6918',
        'company_name': 'æ„›æ´¾å¸',
        'analyst_count': 18,
        'target_price': 8.6,
        'eps_2025_avg': 6.00,
        'eps_2026_avg': None,
        'eps_2027_avg': None,
        'content': 'FactSet æ„›ç«‹ä¿¡(ERIC-US) åˆ†æå¸«é ä¼°...',
        'content_date': '2025/6/19',
        # ğŸ†• é©—è­‰çµæœ (æ¨¡æ“¬ md_parser çš„è¼¸å‡º)
        'validation_result': {
            'overall_status': 'error',
            'confidence_score': 0.0,
            'errors': ['æª”æ¡ˆæ¨™ç¤ºç‚ºæ„›æ´¾å¸(6918)ä½†å…§å®¹åŒ…å«æ„›ç«‹ä¿¡ç›¸é—œè³‡è¨Š: [\'æ„›ç«‹ä¿¡\', \'ERIC-US\']'],
            'warnings': [],
            'ericsson_detected': True,
            'mismatch_details': {
                'expected': {'company': 'æ„›æ´¾å¸', 'code': '6918', 'region': 'TW'},
                'detected': {'company': 'æ„›ç«‹ä¿¡', 'code': 'ERIC', 'region': 'US'}
            }
        },
        'content_validation_passed': False,
        'validation_errors': ['æª”æ¡ˆæ¨™ç¤ºç‚ºæ„›æ´¾å¸(6918)ä½†å…§å®¹åŒ…å«æ„›ç«‹ä¿¡ç›¸é—œè³‡è¨Š'],
        'validation_warnings': []
    }
    
    # æ¸¬è©¦æ­£å¸¸è³‡æ–™çš„å“è³ªè©•åˆ†
    test_data_normal = {
        'company_code': '2330',
        'company_name': 'å°ç©é›»',
        'analyst_count': 42,
        'target_price': 650.5,
        'eps_2025_avg': 46.00,
        'eps_2026_avg': 52.00,
        'eps_2027_avg': 58.00,
        'content': 'FactSet å°ç©é›» åˆ†æå¸«é ä¼°...',
        'content_date': '2025/6/24',
        # æ­£å¸¸çš„é©—è­‰çµæœ
        'validation_result': {
            'overall_status': 'valid',
            'confidence_score': 10.0,
            'errors': [],
            'warnings': []
        },
        'content_validation_passed': True,
        'validation_errors': [],
        'validation_warnings': []
    }
    
    print("æ¸¬è©¦ 1: æ„›æ´¾å¸/æ„›ç«‹ä¿¡éŒ¯èª¤è³‡æ–™")
    result_error = analyzer.analyze(test_data_error)
    print(f"  å“è³ªè©•åˆ†: {result_error['quality_score']}")
    print(f"  å“è³ªç‹€æ…‹: {result_error['quality_status']}")
    print(f"  é©—è­‰é€šé: {result_error['validation_summary']['validation_passed']}")
    print(f"  é©—è­‰éŒ¯èª¤: {len(result_error['validation_summary']['validation_errors'])}")
    
    print("\næ¸¬è©¦ 2: æ­£å¸¸å°ç©é›»è³‡æ–™")
    result_normal = analyzer.analyze(test_data_normal)
    print(f"  å“è³ªè©•åˆ†: {result_normal['quality_score']}")
    print(f"  å“è³ªç‹€æ…‹: {result_normal['quality_status']}")
    print(f"  é©—è­‰é€šé: {result_normal['validation_summary']['validation_passed']}")
    
    print(f"\nâœ… é æœŸçµæœ:")
    print(f"  æ„›æ´¾å¸éŒ¯èª¤: è©•åˆ† â‰¤ 2.0 (ğŸ”´ ä¸è¶³)")
    print(f"  å°ç©é›»æ­£å¸¸: è©•åˆ† â‰¥ 8.0 (ğŸŸ¡/ğŸŸ¢)")
    
    print(f"\nğŸ‰ æ¸¬è©¦å®Œæˆ!")
    print(f"æ„›æ´¾å¸è©•åˆ†: {result_error['quality_score']} {'âœ…' if result_error['quality_score'] <= 2.0 else 'âŒ'}")
    print(f"å°ç©é›»è©•åˆ†: {result_normal['quality_score']} {'âœ…' if result_normal['quality_score'] >= 8.0 else 'âŒ'}")