#!/usr/bin/env python3
"""
FactSet Pipeline v3.5.0 - Process CLI (Enhanced with Data Validation)
è™•ç†ç¾¤çµ„çš„å‘½ä»¤åˆ—ä»‹é¢ - è³‡æ–™é©—è­‰å¢å¼·ç‰ˆ
"""

import sys
import os
import re
import argparse
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

# ğŸ”§ è¼‰å…¥ç’°å¢ƒè®Šæ•¸
try:
    from dotenv import load_dotenv
    # è¼‰å…¥ .env æª”æ¡ˆ - å˜—è©¦å¤šå€‹è·¯å¾‘
    env_paths = [
        '.env',
        '../.env', 
        '../../.env',
        os.path.join(os.path.dirname(__file__), '.env'),
        os.path.join(os.path.dirname(__file__), '../.env')
    ]
    
    env_loaded = False
    for env_path in env_paths:
        if os.path.exists(env_path):
            load_dotenv(env_path)
            print(f"âœ… è¼‰å…¥ç’°å¢ƒè®Šæ•¸: {env_path}")
            env_loaded = True
            break
    
    if not env_loaded:
        print("âš ï¸ æœªæ‰¾åˆ° .env æª”æ¡ˆï¼Œä½¿ç”¨ç³»çµ±ç’°å¢ƒè®Šæ•¸")

except ImportError:
    print("âš ï¸ python-dotenv æœªå®‰è£ï¼Œä½¿ç”¨ç³»çµ±ç’°å¢ƒè®Šæ•¸")

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å°å…¥çµ„ä»¶
try:
    from md_scanner import MDScanner
    print("âœ… MDScanner å·²è¼‰å…¥")
except ImportError as e:
    print(f"âŒ MDScanner è¼‰å…¥å¤±æ•—: {e}")
    MDScanner = None

try:
    from md_parser import MDParser
    print("âœ… MDParser å·²è¼‰å…¥")
except ImportError as e:
    print(f"âŒ MDParser è¼‰å…¥å¤±æ•—: {e}")
    MDParser = None

try:
    from quality_analyzer import QualityAnalyzer
    print("âœ… QualityAnalyzer å·²è¼‰å…¥")
except ImportError as e:
    print(f"âŒ QualityAnalyzer è¼‰å…¥å¤±æ•—: {e}")
    QualityAnalyzer = None

try:
    from report_generator import ReportGenerator
    print("âœ… ReportGenerator å·²è¼‰å…¥")
except ImportError as e:
    print(f"âŒ ReportGenerator è¼‰å…¥å¤±æ•—: {e}")
    ReportGenerator = None

try:
    from sheets_uploader import SheetsUploader
    print("âœ… SheetsUploader å·²è¼‰å…¥")
except ImportError as e:
    print(f"âš ï¸ SheetsUploader è¼‰å…¥å¤±æ•—: {e}")
    SheetsUploader = None


class ProcessCLI:
    """è™•ç†å‘½ä»¤åˆ—ä»‹é¢ - è³‡æ–™é©—è­‰å¢å¼·ç‰ˆ"""
    
    def __init__(self):
        print("ğŸ”§ åˆå§‹åŒ– ProcessCLI...")
        
        # æ ¸å¿ƒçµ„ä»¶ - MDScanner æ˜¯å¿…é ˆçš„
        if MDScanner:
            self.md_scanner = MDScanner()
            print("âœ… MDScanner å·²åˆå§‹åŒ–")
        else:
            raise ImportError("MDScanner æ˜¯å¿…é ˆçš„çµ„ä»¶")
        
        # åˆå§‹åŒ–å…¶ä»–çµ„ä»¶
        self.md_parser = None
        self.quality_analyzer = None  
        self.report_generator = None
        self.sheets_uploader = None
        
        self._init_components()
        self._ensure_output_directories()
    
    def _init_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çµ„ä»¶"""
        # MD Parser
        if MDParser:
            try:
                self.md_parser = MDParser()
                print("âœ… MDParser å·²åˆå§‹åŒ–")
            except Exception as e:
                print(f"âŒ MDParser åˆå§‹åŒ–å¤±æ•—: {e}")
        
        # Quality Analyzer
        if QualityAnalyzer:
            try:
                self.quality_analyzer = QualityAnalyzer()
                print("âœ… QualityAnalyzer å·²åˆå§‹åŒ–")
            except Exception as e:
                print(f"âŒ QualityAnalyzer åˆå§‹åŒ–å¤±æ•—: {e}")
        
        # Report Generator
        if ReportGenerator:
            try:
                self.report_generator = ReportGenerator()
                print("âœ… ReportGenerator å·²åˆå§‹åŒ–")
            except Exception as e:
                print(f"âŒ ReportGenerator åˆå§‹åŒ–å¤±æ•—: {e}")
                self.report_generator = None
        
        # Sheets Uploader
        if SheetsUploader:
            try:
                self.sheets_uploader = SheetsUploader()
                print("âœ… SheetsUploader å·²åˆå§‹åŒ–")
            except Exception as e:
                print(f"âš ï¸ SheetsUploader åˆå§‹åŒ–å¤±æ•—: {e}")
                self.sheets_uploader = None

    def validate_data_integrity(self, **kwargs):
        """é©—è­‰è³‡æ–™å®Œæ•´æ€§ - åµæ¸¬æ„›æ´¾å¸/æ„›ç«‹ä¿¡ç­‰å•é¡Œ"""
        print("\nğŸ” é–‹å§‹è³‡æ–™å®Œæ•´æ€§é©—è­‰...")
        print("=" * 60)
        
        # æƒææ‰€æœ‰ MD æª”æ¡ˆ
        md_files = self.md_scanner.scan_all_md_files()
        
        if not md_files:
            print("âŒ æ²’æœ‰æ‰¾åˆ° MD æª”æ¡ˆ")
            return {}
        
        print(f"ğŸ“ é©—è­‰ {len(md_files)} å€‹ MD æª”æ¡ˆ")
        
        validation_results = {
            'total_files': len(md_files),
            'validation_passed': 0,
            'validation_warnings': 0,
            'validation_errors': 0,
            'critical_issues': [],
            'detailed_results': [],
            'summary_by_status': {
                'valid': [],
                'warning': [],
                'error': []
            }
        }
        
        # é€ä¸€é©—è­‰æª”æ¡ˆ
        for i, md_file in enumerate(md_files, 1):
            try:
                print(f"ğŸ” é©—è­‰ {i}/{len(md_files)}: {os.path.basename(md_file)}")
                
                # å–å¾—æª”æ¡ˆåŸºæœ¬è³‡è¨Š
                file_info = self.md_scanner.get_file_info(md_file)
                
                if self.md_parser:
                    # ä½¿ç”¨å¢å¼·ç‰ˆ MD Parser é€²è¡Œé©—è­‰
                    parsed_data = self.md_parser.parse_md_file(md_file)
                    validation_result = parsed_data.get('validation_result', {})
                    
                    # è¨˜éŒ„é©—è­‰çµæœ
                    file_result = {
                        'filename': os.path.basename(md_file),
                        'company_code': file_info.get('company_code'),
                        'company_name': file_info.get('company_name'),
                        'validation_status': validation_result.get('overall_status', 'unknown'),
                        'confidence_score': validation_result.get('confidence_score', 0),
                        'errors': validation_result.get('errors', []),
                        'warnings': validation_result.get('warnings', []),
                        'file_size': file_info.get('file_size', 0),
                        'content_length': parsed_data.get('content_length', 0)
                    }
                    
                    # çµ±è¨ˆé©—è­‰ç‹€æ…‹
                    status = validation_result.get('overall_status', 'unknown')
                    if status == 'valid':
                        validation_results['validation_passed'] += 1
                        validation_results['summary_by_status']['valid'].append(file_result)
                        print(f"   âœ… é©—è­‰é€šé")
                    elif status == 'warning':
                        validation_results['validation_warnings'] += 1
                        validation_results['summary_by_status']['warning'].append(file_result)
                        print(f"   ğŸŸ¡ æœ‰è­¦å‘Š ({len(validation_result.get('warnings', []))} é …)")
                    elif status == 'error':
                        validation_results['validation_errors'] += 1
                        validation_results['summary_by_status']['error'].append(file_result)
                        print(f"   âŒ é©—è­‰å¤±æ•— ({len(validation_result.get('errors', []))} é …éŒ¯èª¤)")
                        
                        # æª¢æŸ¥æ˜¯å¦ç‚ºé—œéµå•é¡Œ
                        for error in validation_result.get('errors', []):
                            # ğŸ†• é—œéµå•é¡Œçš„åµæ¸¬ (ç§»é™¤äº†éŒ¯èª¤è¨Šæ¯æª¢æŸ¥)
                            critical_patterns = [
                                r'æ„›æ´¾å¸.*æ„›ç«‹ä¿¡',      # æ„›æ´¾å¸/æ„›ç«‹ä¿¡å•é¡Œ
                                r'æ„›ç«‹ä¿¡.*æ„›æ´¾å¸',      # æ„›ç«‹ä¿¡/æ„›æ´¾å¸å•é¡Œ  
                                r'å…¬å¸åç¨±ä¸ç¬¦è§€å¯Ÿåå–®',  # è§€å¯Ÿåå–®ä¸ç¬¦
                                r'è§€å¯Ÿåå–®é¡¯ç¤ºæ‡‰ç‚º'      # è§€å¯Ÿåå–®éŒ¯èª¤
                            ]
                            
                            if any(re.search(pattern, str(error), re.IGNORECASE) for pattern in critical_patterns):
                                issue_type = 'company_mismatch'
                                if 'è§€å¯Ÿåå–®' in str(error):
                                    issue_type = 'watchlist_mismatch'
                                
                                validation_results['critical_issues'].append({
                                    'type': issue_type,
                                    'file': os.path.basename(md_file),
                                    'description': error,
                                    'severity': 'critical'
                                })
                    
                    validation_results['detailed_results'].append(file_result)
                
                else:
                    # åŸºæœ¬é©—è­‰ï¼ˆç•¶ MD Parser ä¸å¯ç”¨æ™‚ï¼‰
                    basic_result = self._basic_file_validation(md_file, file_info)
                    validation_results['detailed_results'].append(basic_result)
                
            except Exception as e:
                print(f"   âŒ é©—è­‰ç•°å¸¸: {e}")
                validation_results['validation_errors'] += 1
                continue
        
        # å„²å­˜é©—è­‰çµæœ
        self._save_validation_results(validation_results)
        
        # é¡¯ç¤ºé©—è­‰æ‘˜è¦
        self._display_validation_summary(validation_results)
        
        return validation_results

    def _basic_file_validation(self, md_file: str, file_info: Dict) -> Dict:
        """åŸºæœ¬æª”æ¡ˆé©—è­‰ï¼ˆç•¶ MD Parser ä¸å¯ç”¨æ™‚ï¼‰"""
        # è®€å–æª”æ¡ˆå…§å®¹é€²è¡ŒåŸºæœ¬æª¢æŸ¥
        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            issues = []
            warnings = []
            
            # åŸºæœ¬å…§å®¹æª¢æŸ¥
            if len(content) < 500:
                warnings.append("æª”æ¡ˆå…§å®¹éçŸ­")
            
            # ğŸ”§ ç§»é™¤å° "Oops, something went wrong" çš„æª¢æŸ¥ - é€™å¾ˆå¸¸è¦‹ï¼Œä¸éœ€è¦ç‰¹åˆ¥è™•ç†
            
            # ç°¡å–®çš„å…¬å¸åç¨±æª¢æŸ¥
            expected_name = file_info.get('company_name', '')
            expected_code = file_info.get('company_code', '')
            
            # æ„›æ´¾å¸/æ„›ç«‹ä¿¡æª¢æŸ¥
            if expected_name == 'æ„›æ´¾å¸' and 'æ„›ç«‹ä¿¡' in content:
                issues.append("å¯èƒ½çš„å…¬å¸åç¨±éŒ¯èª¤ï¼šæª”æ¡ˆç‚ºæ„›æ´¾å¸ä½†å…§å®¹æåˆ°æ„›ç«‹ä¿¡")
            
            # ğŸ†• åŸºæœ¬çš„è§€å¯Ÿåå–®æª¢æŸ¥ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
            try:
                import pandas as pd
                # å˜—è©¦è¼‰å…¥è§€å¯Ÿåå–®
                possible_paths = ['è§€å¯Ÿåå–®.csv', '../è§€å¯Ÿåå–®.csv', '../../è§€å¯Ÿåå–®.csv']
                for csv_path in possible_paths:
                    if os.path.exists(csv_path):
                        df = pd.read_csv(csv_path, header=None, names=['code', 'name'])
                        watch_mapping = {}
                        for _, row in df.iterrows():
                            code = str(row['code']).strip()
                            name = str(row['name']).strip()
                            if code and name and code != 'nan' and name != 'nan':
                                watch_mapping[code] = name
                        
                        # æª¢æŸ¥è§€å¯Ÿåå–®ä¸€è‡´æ€§
                        if expected_code in watch_mapping:
                            correct_name = watch_mapping[expected_code]
                            if expected_name != correct_name:
                                issues.append(f"å…¬å¸åç¨±ä¸ç¬¦è§€å¯Ÿåå–®ï¼šæª”æ¡ˆç‚º{expected_name}({expected_code})ï¼Œè§€å¯Ÿåå–®ç‚º{correct_name}({expected_code})")
                        break
            except:
                pass  # å¦‚æœæª¢æŸ¥å¤±æ•—ï¼Œè·³éè§€å¯Ÿåå–®é©—è­‰
            
            status = 'error' if issues else ('warning' if warnings else 'valid')
            
            return {
                'filename': os.path.basename(md_file),
                'company_code': file_info.get('company_code'),
                'company_name': file_info.get('company_name'),
                'validation_status': status,
                'confidence_score': 0 if issues else (5 if warnings else 8),
                'errors': issues,
                'warnings': warnings,
                'file_size': file_info.get('file_size', 0),
                'content_length': len(content),
                'validation_method': 'basic'
            }
            
        except Exception as e:
            return {
                'filename': os.path.basename(md_file),
                'validation_status': 'error',
                'errors': [f"æª”æ¡ˆè®€å–å¤±æ•—: {str(e)}"],
                'validation_method': 'basic'
            }

    def _save_validation_results(self, validation_results: Dict):
        """å„²å­˜é©—è­‰çµæœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = f"data/reports/validation_results_{timestamp}.json"
        latest_file = "data/reports/validation_results_latest.json"
        
        # åŠ å…¥æ™‚é–“æˆ³è¨˜
        validation_results['timestamp'] = datetime.now().isoformat()
        validation_results['version'] = '3.5.0_enhanced'
        
        # å„²å­˜æª”æ¡ˆ
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(validation_results, f, ensure_ascii=False, indent=2, default=str)
        
        with open(latest_file, 'w', encoding='utf-8') as f:
            json.dump(validation_results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ“ é©—è­‰çµæœå·²å„²å­˜: {output_file}")

    def _display_validation_summary(self, results: Dict):
        """é¡¯ç¤ºé©—è­‰æ‘˜è¦"""
        total = results['total_files']
        passed = results['validation_passed']
        warnings = results['validation_warnings'] 
        errors = results['validation_errors']
        critical = len(results['critical_issues'])
        
        print(f"\nğŸ“Š è³‡æ–™é©—è­‰æ‘˜è¦:")
        print(f"=" * 40)
        print(f"ğŸ“ ç¸½æª”æ¡ˆæ•¸: {total}")
        print(f"âœ… é©—è­‰é€šé: {passed} ({passed/total*100:.1f}%)")
        print(f"ğŸŸ¡ æœ‰è­¦å‘Š: {warnings} ({warnings/total*100:.1f}%)")
        print(f"âŒ é©—è­‰å¤±æ•—: {errors} ({errors/total*100:.1f}%)")
        print(f"ğŸš¨ é—œéµå•é¡Œ: {critical}")
        
        # é¡¯ç¤ºé—œéµå•é¡Œè©³æƒ…
        if critical > 0:
            print(f"\nğŸš¨ é—œéµå•é¡Œè©³æƒ…:")
            for issue in results['critical_issues']:
                print(f"  ğŸ“„ {issue['file']}")
                print(f"     é¡å‹: {issue['type']}")
                print(f"     æè¿°: {issue['description']}")
                print(f"     åš´é‡æ€§: {issue['severity']}")
        
        # é¡¯ç¤ºé©—è­‰å¤±æ•—çš„æª”æ¡ˆ
        error_files = results['summary_by_status'].get('error', [])
        if error_files:
            print(f"\nâŒ é©—è­‰å¤±æ•—çš„æª”æ¡ˆ (å‰5å€‹):")
            for file_result in error_files[:5]:
                print(f"  ğŸ“„ {file_result['filename']}")
                print(f"     å…¬å¸: {file_result.get('company_name', 'Unknown')} ({file_result.get('company_code', 'Unknown')})")
                print(f"     éŒ¯èª¤: {file_result.get('errors', [])[:2]}")  # åªé¡¯ç¤ºå‰2å€‹éŒ¯èª¤

    def clean_invalid_data(self, dry_run=True, **kwargs):
        """æ¸…ç†ç„¡æ•ˆè³‡æ–™ - ç§»å‹•æˆ–æ¨™è¨˜æœ‰å•é¡Œçš„æª”æ¡ˆ"""
        print(f"\nğŸ§¹ æ¸…ç†ç„¡æ•ˆè³‡æ–™ ({'é è¦½æ¨¡å¼' if dry_run else 'åŸ·è¡Œæ¨¡å¼'})")
        print("=" * 50)
        
        # å…ˆåŸ·è¡Œé©—è­‰
        validation_results = self.validate_data_integrity()
        
        # æ‰¾å‡ºéœ€è¦æ¸…ç†çš„æª”æ¡ˆ
        error_files = validation_results['summary_by_status'].get('error', [])
        critical_issues = validation_results['critical_issues']
        
        if not error_files and not critical_issues:
            print("âœ… æ²’æœ‰ç™¼ç¾éœ€è¦æ¸…ç†çš„æª”æ¡ˆ")
            return
        
        # å»ºç«‹æ¸…ç†ç›®éŒ„
        quarantine_dir = "data/quarantine"
        if not dry_run:
            os.makedirs(quarantine_dir, exist_ok=True)
        
        cleanup_actions = []
        
        # è™•ç†é—œéµå•é¡Œæª”æ¡ˆ
        for issue in critical_issues:
            filename = issue['file']
            action = {
                'filename': filename,
                'reason': issue['description'],
                'action': 'quarantine',
                'severity': 'critical'
            }
            cleanup_actions.append(action)
            
            if dry_run:
                print(f"ğŸ” [é è¦½] å°‡éš”é›¢: {filename}")
                print(f"    åŸå› : {issue['description']}")
            else:
                # å¯¦éš›ç§»å‹•æª”æ¡ˆ
                src_path = os.path.join("data/md", filename)
                dst_path = os.path.join(quarantine_dir, filename)
                
                try:
                    import shutil
                    shutil.move(src_path, dst_path)
                    print(f"âœ… å·²éš”é›¢: {filename}")
                except Exception as e:
                    print(f"âŒ éš”é›¢å¤±æ•—: {filename} - {e}")
        
        # è™•ç†å…¶ä»–éŒ¯èª¤æª”æ¡ˆ
        for file_result in error_files:
            filename = file_result['filename']
            
            # è·³éå·²ç¶“è™•ç†çš„é—œéµå•é¡Œæª”æ¡ˆ
            if any(issue['file'] == filename for issue in critical_issues):
                continue
            
            action = {
                'filename': filename,
                'reason': ', '.join(file_result.get('errors', [])),
                'action': 'review',
                'severity': 'error'
            }
            cleanup_actions.append(action)
            
            if dry_run:
                print(f"ğŸ” [é è¦½] éœ€è¦æª¢æŸ¥: {filename}")
                print(f"    éŒ¯èª¤: {', '.join(file_result.get('errors', [])[:2])}")
        
        # å„²å­˜æ¸…ç†å ±å‘Š
        cleanup_report = {
            'timestamp': datetime.now().isoformat(),
            'dry_run': dry_run,
            'total_actions': len(cleanup_actions),
            'actions': cleanup_actions,
            'validation_summary': {
                'total_files': validation_results['total_files'],
                'error_files': len(error_files),
                'critical_issues': len(critical_issues)
            }
        }
        
        report_file = f"data/reports/cleanup_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(cleanup_report, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ“Š æ¸…ç†æ‘˜è¦:")
        print(f"ğŸ” æª¢æŸ¥æª”æ¡ˆ: {validation_results['total_files']}")
        print(f"ğŸš¨ é—œéµå•é¡Œ: {len(critical_issues)}")
        print(f"âŒ éŒ¯èª¤æª”æ¡ˆ: {len(error_files)}")
        print(f"ğŸ“„ æ¸…ç†å ±å‘Š: {report_file}")
        
        if dry_run:
            print(f"\nğŸ’¡ åŸ·è¡Œå¯¦éš›æ¸…ç†: python process_cli.py clean-data --execute")

    # åŸæœ‰åŠŸèƒ½ä¿æŒä¸è®Š
    def process_all_md_files(self, upload_sheets=True, **kwargs):
        """è™•ç†æ‰€æœ‰ MD æª”æ¡ˆ - ğŸ†• å¢å¼·ç‰ˆæœ¬çµ±è¨ˆ"""
        print("\nğŸš€ é–‹å§‹è™•ç†æ‰€æœ‰ MD æª”æ¡ˆ...")
        
        # 1. æƒæ MD æª”æ¡ˆ
        md_files = self.md_scanner.scan_all_md_files()
        print(f"ğŸ“ ç™¼ç¾ {len(md_files)} å€‹ MD æª”æ¡ˆ")
        
        if not md_files:
            print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½• MD æª”æ¡ˆ")
            print("ğŸ’¡ è«‹å…ˆåŸ·è¡Œæœå°‹ç¾¤çµ„ä¾†ç”Ÿæˆ MD æª”æ¡ˆ")
            return []
        
        # 2. è™•ç†æª”æ¡ˆ
        processed_companies = self._process_md_file_list(md_files, **kwargs)
        
        # 3. ç”Ÿæˆå ±å‘Šå‰çš„æœ€çµ‚çµ±è¨ˆ
        if processed_companies:
            print(f"\nğŸ¯ å ±å‘Šç”Ÿæˆéšæ®µ:")
            
            # é å…ˆæª¢æŸ¥æœ‰å¤šå°‘å…¬å¸æœƒè¢«åŒ…å«åœ¨å ±å‘Šä¸­
            if self.report_generator:
                companies_for_report = [c for c in processed_companies 
                                      if self.report_generator._should_include_in_report(c)]
                
                excluded_count = len(processed_companies) - len(companies_for_report)
                
                print(f"ğŸ“Š è™•ç†çµæœæ‘˜è¦:")
                print(f"   å·²è™•ç†å…¬å¸: {len(processed_companies)} å®¶")
                print(f"   å°‡ç´å…¥å ±å‘Š: {len(companies_for_report)} å®¶")
                print(f"   å› é©—è­‰å¤±æ•—æ’é™¤: {excluded_count} å®¶")
                
                if excluded_count > 0:
                    print(f"   âœ… æˆåŠŸéæ¿¾äº† {excluded_count} å®¶æœ‰å•é¡Œçš„å…¬å¸")
            
            # 4. ç”Ÿæˆå’Œä¸Šå‚³å ±å‘Š
            self._generate_and_upload_reports_fixed(processed_companies, upload_sheets, 
                                                   force_upload=kwargs.get('force_upload', False))
            
            print(f"âœ… è™•ç†å®Œæˆ")
            
            # é¡¯ç¤ºæœ€çµ‚é©—è­‰æ‘˜è¦
            self._display_processing_validation_summary(processed_companies)
        else:
            print("âŒ æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•æª”æ¡ˆ")
        
        return processed_companies

    def _display_processing_validation_summary(self, processed_companies: List):
        """é¡¯ç¤ºè™•ç†éç¨‹ä¸­çš„é©—è­‰æ‘˜è¦ - ğŸ†• å¢å¼·ç‰ˆæœ¬"""
        validation_passed = sum(1 for c in processed_companies if c.get('content_validation_passed', True))
        validation_failed = len(processed_companies) - validation_passed
        
        if validation_failed > 0:
            print(f"\nâš ï¸ é©—è­‰æ‘˜è¦:")
            print(f"âœ… é©—è­‰é€šé: {validation_passed}")
            print(f"âŒ é©—è­‰å¤±æ•—: {validation_failed}")
            
            # ğŸ†• è©³ç´°åˆ†æé©—è­‰å¤±æ•—çš„åŸå› 
            failed_companies = [c for c in processed_companies if not c.get('content_validation_passed', True)]
            if failed_companies:
                print(f"\nâŒ é©—è­‰å¤±æ•—çš„å…¬å¸åˆ†æ:")
                
                error_summary = {}
                for company in failed_companies:
                    errors = company.get('validation_errors', [])
                    if errors:
                        main_error = str(errors[0])
                        if "ä¸åœ¨è§€å¯Ÿåå–®" in main_error:
                            error_summary['ä¸åœ¨è§€å¯Ÿåå–®'] = error_summary.get('ä¸åœ¨è§€å¯Ÿåå–®', []) + [company]
                        elif any(keyword in main_error for keyword in ['æ„›æ´¾å¸', 'æ„›ç«‹ä¿¡']):
                            error_summary['å…¬å¸åç¨±éŒ¯èª¤'] = error_summary.get('å…¬å¸åç¨±éŒ¯èª¤', []) + [company]
                        elif "è§€å¯Ÿåå–®é¡¯ç¤ºæ‡‰ç‚º" in main_error:
                            error_summary['è§€å¯Ÿåå–®åç¨±ä¸ç¬¦'] = error_summary.get('è§€å¯Ÿåå–®åç¨±ä¸ç¬¦', []) + [company]
                        else:
                            error_summary['å…¶ä»–'] = error_summary.get('å…¶ä»–', []) + [company]
                
                for error_type, companies in error_summary.items():
                    print(f"   {error_type} ({len(companies)} å®¶):")
                    for company in companies[:3]:  # åªé¡¯ç¤ºå‰3å€‹
                        print(f"     - {company.get('company_name', 'Unknown')} ({company.get('company_code', 'Unknown')})")
                    if len(companies) > 3:
                        print(f"     - ... é‚„æœ‰ {len(companies) - 3} å®¶")
                
                print(f"\nğŸ’¡ é€™äº›å…¬å¸å·²è‡ªå‹•æ’é™¤ï¼Œä¸æœƒå‡ºç¾åœ¨å ±å‘Šå’Œ Google Sheets ä¸­")

    def process_recent_md_files(self, hours=24, upload_sheets=True, **kwargs):
        """è™•ç†æœ€è¿‘ N å°æ™‚çš„ MD æª”æ¡ˆ"""
        print(f"\nğŸš€ è™•ç†æœ€è¿‘ {hours} å°æ™‚çš„ MD æª”æ¡ˆ...")
        
        recent_files = self.md_scanner.scan_recent_files(hours)
        print(f"ğŸ“ ç™¼ç¾ {len(recent_files)} å€‹æœ€è¿‘çš„ MD æª”æ¡ˆ")
        
        if not recent_files:
            print(f"ğŸ“ æœ€è¿‘ {hours} å°æ™‚å…§æ²’æœ‰æ–°çš„ MD æª”æ¡ˆ")
            return []
        
        processed_companies = self._process_md_file_list(recent_files, **kwargs)
        
        if processed_companies:
            self._generate_and_upload_reports_fixed(processed_companies, upload_sheets)
            print(f"âœ… è™•ç†å®Œæˆ: {len(processed_companies)} å®¶å…¬å¸")
            self._display_processing_validation_summary(processed_companies)
        
        return processed_companies

    def process_single_company(self, company_code, upload_sheets=True, **kwargs):
        """è™•ç†å–®ä¸€å…¬å¸çš„ MD æª”æ¡ˆ"""
        print(f"\nğŸš€ è™•ç†å–®ä¸€å…¬å¸: {company_code}")
        
        company_files = self.md_scanner.find_company_files(company_code)
        
        if not company_files:
            print(f"âŒ æ‰¾ä¸åˆ°å…¬å¸ {company_code} çš„ MD æª”æ¡ˆ")
            return None
        
        latest_file = company_files[0]
        print(f"ğŸ“ æ‰¾åˆ° {len(company_files)} å€‹æª”æ¡ˆï¼Œä½¿ç”¨æœ€æ–°çš„: {os.path.basename(latest_file)}")
        
        processed_companies = self._process_md_file_list([latest_file], **kwargs)
        
        if processed_companies:
            if upload_sheets:
                self._generate_and_upload_reports_fixed(processed_companies, upload_sheets)
            print(f"âœ… è™•ç†å®Œæˆ: {company_code}")
            return processed_companies[0]
        else:
            print(f"âŒ è™•ç†å¤±æ•—: {company_code}")
            return None

    def analyze_quality_only(self, **kwargs):
        """åªé€²è¡Œå“è³ªåˆ†æ"""
        print("\nğŸ“Š åŸ·è¡Œå“è³ªåˆ†æ...")
        
        md_files = self.md_scanner.scan_all_md_files()
        
        if not md_files:
            print("âŒ æ²’æœ‰æ‰¾åˆ° MD æª”æ¡ˆ")
            return []
        
        print(f"ğŸ“Š åˆ†æ {len(md_files)} å€‹ MD æª”æ¡ˆçš„å“è³ª")
        
        quality_results = []
        success_count = 0
        
        for md_file in md_files:
            try:
                file_info = self.md_scanner.get_file_info(md_file)
                
                if self.quality_analyzer and self.md_parser:
                    parsed_data = self.md_parser.parse_md_file(md_file)
                    quality_data = self.quality_analyzer.analyze(parsed_data)
                    
                    quality_result = {
                        'company_code': file_info['company_code'],
                        'company_name': file_info['company_name'],
                        'quality_score': quality_data.get('quality_score', 0),
                        'quality_status': quality_data.get('quality_status', 'ğŸ”´ ä¸è¶³'),
                        'md_file': os.path.basename(md_file),
                        'file_size': file_info['file_size'],
                        'file_time': file_info['file_mtime'],
                        # åŠ å…¥é©—è­‰è³‡è¨Š
                        'validation_passed': parsed_data.get('content_validation_passed', True),
                        'validation_errors': len(parsed_data.get('validation_errors', []))
                    }
                else:
                    basic_score = self._calculate_basic_quality_score(file_info)
                    quality_result = {
                        'company_code': file_info['company_code'],
                        'company_name': file_info['company_name'],
                        'quality_score': basic_score,
                        'quality_status': self._get_quality_status(basic_score),
                        'md_file': os.path.basename(md_file),
                        'file_size': file_info['file_size'],
                        'file_time': file_info['file_mtime'],
                        'validation_passed': True,
                        'validation_errors': 0
                    }
                
                quality_results.append(quality_result)
                success_count += 1
                
            except Exception as e:
                print(f"âŒ å“è³ªåˆ†æå¤±æ•—: {os.path.basename(md_file)} - {e}")
                continue
        
        self._save_quality_analysis(quality_results)
        self._display_quality_summary(quality_results)
        
        print(f"âœ… å“è³ªåˆ†æå®Œæˆ: {success_count}/{len(md_files)} æˆåŠŸ")
        return quality_results

    def show_md_stats(self, **kwargs):
        """é¡¯ç¤º MD æª”æ¡ˆçµ±è¨ˆè³‡è¨Š"""
        print("\nğŸ“Š MD æª”æ¡ˆçµ±è¨ˆè³‡è¨Š")
        print("=" * 50)
        
        stats = self.md_scanner.get_stats()
        
        print(f"ğŸ“ ç¸½æª”æ¡ˆæ•¸: {stats['total_files']}")
        print(f"ğŸ• æœ€è¿‘ 24h: {stats['recent_files_24h']}")
        print(f"ğŸ¢ å…¬å¸æ•¸é‡: {stats['unique_companies']}")
        print(f"ğŸ’¾ ç¸½å¤§å°: {stats['total_size_mb']} MB")
        
        if stats['oldest_file']:
            oldest_info = self.md_scanner.get_file_info(stats['oldest_file'])
            print(f"ğŸ“… æœ€èˆŠæª”æ¡ˆ: {oldest_info['filename']} ({oldest_info['file_mtime']})")
        
        if stats['newest_file']:
            newest_info = self.md_scanner.get_file_info(stats['newest_file'])
            print(f"ğŸ†• æœ€æ–°æª”æ¡ˆ: {newest_info['filename']} ({newest_info['file_mtime']})")
        
        if stats['companies_with_files']:
            print("\nğŸ“ˆ æª”æ¡ˆæœ€å¤šçš„å…¬å¸ (å‰ 10 å):")
            sorted_companies = sorted(stats['companies_with_files'].items(), 
                                    key=lambda x: x[1], reverse=True)
            for i, (company, count) in enumerate(sorted_companies[:10], 1):
                print(f"  {i:2}. {company}: {count} å€‹æª”æ¡ˆ")
        
        return stats

    def validate_setup(self, **kwargs):
        """é©—è­‰è™•ç†ç’°å¢ƒè¨­å®š"""
        print("\nğŸ”§ é©—è­‰è™•ç†ç’°å¢ƒè¨­å®š")
        print("=" * 50)
        
        validation_results = {}
        
        # æª¢æŸ¥ MD ç›®éŒ„
        try:
            md_files = self.md_scanner.scan_all_md_files()
            validation_results['md_scanner'] = {
                'status': 'âœ… æ­£å¸¸',
                'details': f'æ‰¾åˆ° {len(md_files)} å€‹ MD æª”æ¡ˆ'
            }
        except Exception as e:
            validation_results['md_scanner'] = {
                'status': 'âŒ éŒ¯èª¤',
                'details': str(e)
            }
        
        # æª¢æŸ¥å…¶ä»–çµ„ä»¶
        components = [
            ('md_parser', self.md_parser),
            ('quality_analyzer', self.quality_analyzer),
            ('report_generator', self.report_generator),
            ('sheets_uploader', self.sheets_uploader)
        ]
        
        for name, component in components:
            if component:
                try:
                    if hasattr(component, 'test_connection'):
                        result = component.test_connection()
                        validation_results[name] = {
                            'status': 'âœ… æ­£å¸¸' if result else 'âš ï¸ è­¦å‘Š',
                            'details': 'é€£ç·šæ¸¬è©¦æˆåŠŸ' if result else 'é€£ç·šæ¸¬è©¦å¤±æ•—'
                        }
                    else:
                        validation_results[name] = {
                            'status': 'âœ… å·²è¼‰å…¥',
                            'details': 'æ¨¡çµ„å·²æˆåŠŸè¼‰å…¥'
                        }
                except Exception as e:
                    validation_results[name] = {
                        'status': 'âŒ éŒ¯èª¤',
                        'details': str(e)
                    }
            else:
                validation_results[name] = {
                    'status': 'âš ï¸ æœªè¼‰å…¥',
                    'details': 'æ¨¡çµ„æœªå®‰è£æˆ–è¼‰å…¥å¤±æ•—'
                }
        
        # é¡¯ç¤ºçµæœ
        for component, result in validation_results.items():
            print(f"{component:15}: {result['status']} - {result['details']}")
        
        return validation_results

    # ç§æœ‰æ–¹æ³•
    def _ensure_output_directories(self):
        """ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨"""
        directories = [
            "data/reports",
            "data/quarantine",
            "logs/process"
        ]
        
        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)

    def _process_md_file_list(self, md_files, **kwargs):
        """è™•ç† MD æª”æ¡ˆæ¸…å–® - ğŸ†• å¢å¼·é©—è­‰æ—¥èªŒå’Œçµ±è¨ˆ"""
        processed_companies = []
        validation_stats = {
            'total_processed': 0,
            'validation_passed': 0,
            'validation_failed': 0,
            'not_in_watchlist': 0,
            'name_mismatch': 0,
            'other_errors': 0
        }
        
        print(f"\nğŸ“„ é–‹å§‹è™•ç† {len(md_files)} å€‹ MD æª”æ¡ˆ...")
        
        for i, md_file in enumerate(md_files, 1):
            try:
                print(f"ğŸ“„ è™•ç† {i}/{len(md_files)}: {os.path.basename(md_file)}")
                
                file_info = self.md_scanner.get_file_info(md_file)
                validation_stats['total_processed'] += 1
                
                if self.md_parser:
                    parsed_data = self.md_parser.parse_md_file(md_file)
                    
                    # ğŸ†• è©³ç´°çš„é©—è­‰ç‹€æ…‹åˆ†æ
                    validation_passed = parsed_data.get('content_validation_passed', True)
                    validation_errors = parsed_data.get('validation_errors', [])
                    validation_result = parsed_data.get('validation_result', {})
                    
                    if validation_passed:
                        validation_stats['validation_passed'] += 1
                        status_icon = "âœ…"
                        status_msg = "é©—è­‰é€šé"
                    else:
                        validation_stats['validation_failed'] += 1
                        status_icon = "âŒ"
                        
                        # åˆ†æå¤±æ•—åŸå› 
                        main_error = validation_errors[0] if validation_errors else "æœªçŸ¥éŒ¯èª¤"
                        if "ä¸åœ¨è§€å¯Ÿåå–®" in str(main_error):
                            validation_stats['not_in_watchlist'] += 1
                            status_msg = "ä¸åœ¨è§€å¯Ÿåå–®"
                        elif any(keyword in str(main_error) for keyword in ['æ„›æ´¾å¸', 'æ„›ç«‹ä¿¡']):
                            validation_stats['other_errors'] += 1
                            status_msg = "å…¬å¸åç¨±éŒ¯èª¤"
                        elif "è§€å¯Ÿåå–®é¡¯ç¤ºæ‡‰ç‚º" in str(main_error):
                            validation_stats['name_mismatch'] += 1
                            status_msg = "è§€å¯Ÿåå–®åç¨±ä¸ç¬¦"
                        else:
                            validation_stats['other_errors'] += 1
                            status_msg = "å…¶ä»–é©—è­‰éŒ¯èª¤"
                    
                    if self.quality_analyzer:
                        quality_data = self.quality_analyzer.analyze(parsed_data)
                        
                        company_data = {
                            **parsed_data,
                            'quality_score': quality_data.get('quality_score', 0),
                            'quality_status': quality_data.get('quality_status', 'ğŸ”´ ä¸è¶³'),
                            'quality_category': quality_data.get('quality_category', 'insufficient'),
                            'processed_at': datetime.now()
                        }
                    else:
                        company_data = {
                            **parsed_data,
                            'quality_score': parsed_data.get('data_richness_score', 0),
                            'quality_status': self._get_quality_status(parsed_data.get('data_richness_score', 0)),
                            'quality_category': 'partial',
                            'processed_at': datetime.now()
                        }
                else:
                    company_data = self._basic_process_md_file(md_file, file_info)
                    validation_stats['validation_passed'] += 1  # åŸºæœ¬è™•ç†å‡è¨­é€šéé©—è­‰
                    status_icon = "âœ…"
                    status_msg = "åŸºæœ¬è™•ç†"
                
                processed_companies.append(company_data)
                
                # ğŸ†• è©³ç´°çš„è™•ç†çµæœé¡¯ç¤º
                company_name = company_data.get('company_name', 'Unknown')
                company_code = company_data.get('company_code', 'Unknown')
                quality_score = company_data.get('quality_score', 0)
                quality_status = company_data.get('quality_status', 'ğŸ”´ ä¸è¶³')
                
                print(f"   {status_icon} {company_name} ({company_code}) - å“è³ª: {quality_score:.1f} {quality_status} - {status_msg}")
                
                # å¦‚æœé©—è­‰å¤±æ•—ï¼Œé¡¯ç¤ºè©³ç´°åŸå› 
                if not validation_passed and validation_errors:
                    print(f"      ğŸ” é©—è­‰å•é¡Œ: {str(validation_errors[0])[:80]}...")
                    
                    # å¦‚æœæ˜¯è§€å¯Ÿåå–®å•é¡Œï¼Œæä¾›æ›´å¤šè³‡è¨Š
                    if "ä¸åœ¨è§€å¯Ÿåå–®" in str(validation_errors[0]):
                        print(f"      ğŸ’¡ æ­¤å…¬å¸å°‡è¢«æ’é™¤åœ¨æœ€çµ‚å ±å‘Šä¹‹å¤–")
                
            except Exception as e:
                print(f"   âŒ è™•ç†å¤±æ•—: {os.path.basename(md_file)} - {e}")
                continue
        
        # ğŸ†• è™•ç†å®Œæˆå¾Œé¡¯ç¤ºè©³ç´°çµ±è¨ˆ
        self._display_processing_statistics(validation_stats)
        
        return processed_companies

    def _display_processing_statistics(self, validation_stats: Dict):
        """ğŸ†• é¡¯ç¤ºè™•ç†çµ±è¨ˆè³‡è¨Š"""
        total = validation_stats['total_processed']
        passed = validation_stats['validation_passed']
        failed = validation_stats['validation_failed']
        
        print(f"\nğŸ“Š è™•ç†çµ±è¨ˆæ‘˜è¦:")
        print(f"=" * 40)
        print(f"ğŸ“ ç¸½è™•ç†æª”æ¡ˆ: {total}")
        print(f"âœ… é©—è­‰é€šé: {passed} ({passed/total*100:.1f}%)")
        print(f"âŒ é©—è­‰å¤±æ•—: {failed} ({failed/total*100:.1f}%)")
        
        if failed > 0:
            print(f"\nâŒ é©—è­‰å¤±æ•—è©³ç´°åˆ†é¡:")
            not_in_watchlist = validation_stats['not_in_watchlist']
            name_mismatch = validation_stats['name_mismatch']
            other_errors = validation_stats['other_errors']
            
            if not_in_watchlist > 0:
                print(f"   ğŸš« ä¸åœ¨è§€å¯Ÿåå–®: {not_in_watchlist} å€‹")
            if name_mismatch > 0:
                print(f"   ğŸ“ è§€å¯Ÿåå–®åç¨±ä¸ç¬¦: {name_mismatch} å€‹")
            if other_errors > 0:
                print(f"   âš ï¸ å…¶ä»–éŒ¯èª¤: {other_errors} å€‹")
            
            print(f"\nğŸ’¡ é€™äº›é©—è­‰å¤±æ•—çš„å…¬å¸å°‡ä¸æœƒå‡ºç¾åœ¨æœ€çµ‚å ±å‘Šä¸­")

    def _basic_process_md_file(self, md_file, file_info):
        """åŸºæœ¬çš„ MD æª”æ¡ˆè™•ç†ï¼ˆç•¶å…¶ä»–æ¨¡çµ„ä¸å¯ç”¨æ™‚ï¼‰"""
        with open(md_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        basic_score = self._calculate_basic_quality_score(file_info)
        
        company_data = {
            **file_info,
            'content': content,
            'content_length': len(content),
            'quality_score': basic_score,
            'quality_status': self._get_quality_status(basic_score),
            'quality_category': 'partial',
            'processed_at': datetime.now(),
            'processing_method': 'basic',
            'content_validation_passed': True,
            'validation_errors': [],
            'validation_warnings': []
        }
        
        return company_data

    def _calculate_basic_quality_score(self, file_info):
        """åŸºæ–¼æª”æ¡ˆè³‡è¨Šè¨ˆç®—åŸºæœ¬å“è³ªè©•åˆ†"""
        score = 0
        
        file_size = file_info.get('file_size', 0)
        if file_size > 5000:
            score += 3
        elif file_size > 2000:
            score += 2
        elif file_size > 500:
            score += 1
        
        if file_info.get('company_code', 'Unknown') != 'Unknown':
            score += 2
        if file_info.get('company_name', 'Unknown') != 'Unknown':
            score += 2
        
        source = file_info.get('data_source', 'Unknown').lower()
        if 'factset' in source:
            score += 3
        elif source in ['yahoo', 'reuters', 'bloomberg']:
            score += 2
        elif source != 'unknown':
            score += 1
        
        return min(10, max(0, score))

    def _get_quality_status(self, score):
        """æ ¹æ“šè©•åˆ†å–å¾—å“è³ªç‹€æ…‹"""
        if score >= 9:
            return "ğŸŸ¢ å®Œæ•´"
        elif score >= 8:
            return "ğŸŸ¡ è‰¯å¥½"
        elif score >= 3:
            return "ğŸŸ  éƒ¨åˆ†"
        else:
            return "ğŸ”´ ä¸è¶³"

    def _generate_and_upload_reports_fixed(self, processed_companies, upload_sheets=True, force_upload=False):
        """ç”Ÿæˆå ±å‘Šä¸¦ä¸Šå‚³ - ğŸ†• æ”¯æ´å¼·åˆ¶ä¸Šå‚³"""
        try:
            if self.report_generator:
                print("ğŸ“Š ä½¿ç”¨ ReportGenerator ç”Ÿæˆå ±å‘Š...")
                
                portfolio_summary = self.report_generator.generate_portfolio_summary(processed_companies)
                detailed_report = self.report_generator.generate_detailed_report(processed_companies)
                
                saved_files = self.report_generator.save_reports(portfolio_summary, detailed_report)
                
                if saved_files:
                    print("ğŸ“ å ±å‘Šå·²æˆåŠŸå„²å­˜:")
                    for report_type, file_path in saved_files.items():
                        if 'latest' in report_type:
                            print(f"   âœ… {report_type}: {file_path}")
                
                try:
                    statistics = self.report_generator.generate_statistics_report(processed_companies)
                    stats_file = self.report_generator.save_statistics_report(statistics)
                    if stats_file:
                        print(f"   ğŸ“Š çµ±è¨ˆå ±å‘Š: {stats_file}")
                except Exception as e:
                    print(f"   âš ï¸ çµ±è¨ˆå ±å‘Šç”Ÿæˆå¤±æ•—: {e}")
                
                if upload_sheets and self.sheets_uploader:
                    try:
                        print("â˜ï¸ ä¸Šå‚³åˆ° Google Sheets...")
                        
                        # ğŸ†• å¦‚æœå¼·åˆ¶ä¸Šå‚³ï¼Œèª¿æ•´é©—è­‰è¨­å®š
                        if force_upload:
                            print("âš ï¸ å¼·åˆ¶ä¸Šå‚³æ¨¡å¼ï¼šå¿½ç•¥é©—è­‰éŒ¯èª¤")
                            original_settings = self.sheets_uploader.validation_settings.copy()
                            self.sheets_uploader.validation_settings.update({
                                'allow_error_data': True,
                                'max_validation_errors': 10000,
                                'check_before_upload': False  # å®Œå…¨è·³éé©—è­‰æª¢æŸ¥
                            })
                        
                        success = self.sheets_uploader.upload_reports(portfolio_summary, detailed_report)
                        
                        # ğŸ†• æ¢å¾©åŸå§‹è¨­å®š
                        if force_upload:
                            self.sheets_uploader.validation_settings = original_settings
                        
                        if success:
                            print("   âœ… Google Sheets ä¸Šå‚³æˆåŠŸ")
                        else:
                            print("   âŒ Google Sheets ä¸Šå‚³å¤±æ•—")
                    except Exception as e:
                        print(f"   âŒ Google Sheets ä¸Šå‚³éŒ¯èª¤: {e}")
                
            else:
                print("âŒ ReportGenerator æœªè¼‰å…¥ï¼Œç„¡æ³•ç”Ÿæˆæ¨™æº–å ±å‘Š")
                self._generate_minimal_reports(processed_companies)
        
        except Exception as e:
            print(f"âŒ å ±å‘Šç”Ÿæˆæˆ–ä¸Šå‚³å¤±æ•—: {e}")

    def _generate_minimal_reports(self, processed_companies):
        """ç”Ÿæˆæœ€å°åŒ–å ±å‘Šï¼ˆç•¶ ReportGenerator ä¸å¯ç”¨æ™‚ï¼‰"""
        import pandas as pd
        
        print("ğŸ“„ ç”Ÿæˆæœ€å°åŒ–å ±å‘Š...")
        
        summary_data = []
        for company in processed_companies:
            summary_data.append({
                'ä»£è™Ÿ': company.get('company_code', ''),
                'åç¨±': company.get('company_name', ''),
                'å“è³ªè©•åˆ†': company.get('quality_score', 0),
                'ç‹€æ…‹': company.get('quality_status', ''),
                'é©—è­‰é€šé': company.get('content_validation_passed', True),
                'è™•ç†æ™‚é–“': company.get('processed_at', '')
            })
        
        df = pd.DataFrame(summary_data)
        emergency_file = "data/reports/emergency_summary.csv"
        df.to_csv(emergency_file, index=False, encoding='utf-8-sig')
        print(f"ğŸ“ ç·Šæ€¥å ±å‘Šå·²å„²å­˜: {emergency_file}")

    def _save_quality_analysis(self, quality_results):
        """å„²å­˜å“è³ªåˆ†æçµæœ"""
        output_file = "data/reports/quality_analysis.json"
        
        analysis_data = {
            'timestamp': datetime.now().isoformat(),
            'total_files': len(quality_results),
            'results': quality_results
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ“ å“è³ªåˆ†æçµæœå·²å„²å­˜: {output_file}")

    def _display_quality_summary(self, quality_results):
        """é¡¯ç¤ºå“è³ªåˆ†ææ‘˜è¦"""
        if not quality_results:
            return
        
        quality_counts = {}
        total_score = 0
        validation_failed = 0
        
        for result in quality_results:
            status = result['quality_status']
            quality_counts[status] = quality_counts.get(status, 0) + 1
            total_score += result.get('quality_score', 0)
            if not result.get('validation_passed', True):
                validation_failed += 1
        
        avg_score = total_score / len(quality_results)
        
        print("\nğŸ“Š å“è³ªåˆ†ææ‘˜è¦:")
        print(f"å¹³å‡å“è³ªè©•åˆ†: {avg_score:.1f}/10")
        print(f"é©—è­‰å¤±æ•—æª”æ¡ˆ: {validation_failed}/{len(quality_results)}")
        print("å“è³ªåˆ†ä½ˆ:")
        for status, count in quality_counts.items():
            percentage = (count / len(quality_results)) * 100
            print(f"  {status}: {count} å€‹ ({percentage:.1f}%)")


def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    parser = argparse.ArgumentParser(description='FactSet è™•ç†ç³»çµ± v3.5.0 (è³‡æ–™é©—è­‰å¢å¼·ç‰ˆ)')
    parser.add_argument('command', choices=[
        'process',           # è™•ç†æ‰€æœ‰ MD æª”æ¡ˆ
        'process-recent',    # è™•ç†æœ€è¿‘çš„ MD æª”æ¡ˆ
        'process-single',    # è™•ç†å–®ä¸€å…¬å¸
        'analyze-quality',   # å“è³ªåˆ†æ
        'validate-data',     # ğŸ†• è³‡æ–™é©—è­‰
        'clean-data',        # ğŸ†• æ¸…ç†ç„¡æ•ˆè³‡æ–™
        'stats',            # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
        'validate'          # é©—è­‰ç’°å¢ƒè¨­å®š
    ])
    parser.add_argument('--company', help='å–®ä¸€å…¬å¸ä»£è™Ÿ (ç”¨æ–¼ process-single)')
    parser.add_argument('--hours', type=int, default=24, help='æœ€è¿‘å°æ™‚æ•¸ (ç”¨æ–¼ process-recent)')
    parser.add_argument('--no-upload', action='store_true', help='ä¸ä¸Šå‚³åˆ° Google Sheets')
    parser.add_argument('--force-upload', action='store_true', help='å¼·åˆ¶ä¸Šå‚³ï¼Œå¿½ç•¥é©—è­‰éŒ¯èª¤')
    parser.add_argument('--execute', action='store_true', help='å¯¦éš›åŸ·è¡Œæ¸…ç† (ç”¨æ–¼ clean-data)')
    
    args = parser.parse_args()
    
    # å»ºç«‹ CLI å¯¦ä¾‹
    try:
        cli = ProcessCLI()
    except Exception as e:
        print(f"âŒ ProcessCLI åˆå§‹åŒ–å¤±æ•—: {e}")
        sys.exit(1)
    
    # åŸ·è¡Œå°æ‡‰å‘½ä»¤
    try:
        if args.command == 'process':
            cli.process_all_md_files(upload_sheets=not args.no_upload, force_upload=args.force_upload if hasattr(args, 'force_upload') else False)
        
        elif args.command == 'process-recent':
            cli.process_recent_md_files(args.hours, upload_sheets=not args.no_upload)
        
        elif args.command == 'process-single':
            if not args.company:
                print("âŒ è«‹æä¾› --company åƒæ•¸")
                sys.exit(1)
            cli.process_single_company(args.company, upload_sheets=not args.no_upload)
        
        elif args.command == 'analyze-quality':
            cli.analyze_quality_only()
        
        elif args.command == 'validate-data':
            cli.validate_data_integrity()
        
        elif args.command == 'clean-data':
            cli.clean_invalid_data(dry_run=not args.execute)
        
        elif args.command == 'stats':
            cli.show_md_stats()
        
        elif args.command == 'validate':
            cli.validate_setup()
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ä½¿ç”¨è€…ä¸­æ–·æ“ä½œ")
        sys.exit(0)
    except Exception as e:
        print(f"âŒ åŸ·è¡ŒéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()