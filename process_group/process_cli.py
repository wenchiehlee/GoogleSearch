#!/usr/bin/env python3
"""
Process CLI - FactSet Pipeline v3.6.1 (Modified)
å‘½ä»¤åˆ—ä»‹é¢ - æ•´åˆå¢å¼·å…§å®¹æ—¥æœŸè™•ç†é‚è¼¯
æ”¯æ´ç¼ºå°‘å…§å®¹æ—¥æœŸæª”æ¡ˆçš„ä½å“è³ªè©•åˆ†è€Œéæ’é™¤
"""

import argparse
import json
import sys
import os
from datetime import datetime
from typing import List, Dict, Any, Optional
import traceback

class ProcessCLI:
    """Process CLI v3.6.1-modified - å¢å¼·å…§å®¹æ—¥æœŸè™•ç†æ•´åˆ"""
    
    def __init__(self):
        self.version = "3.6.1-modified"
        
        # åˆå§‹åŒ–æ‰€æœ‰æ¨¡çµ„ (graceful degradation)
        self.md_scanner = None
        self.md_parser = None
        self.quality_analyzer = None
        self.keyword_analyzer = None
        self.watchlist_analyzer = None
        self.report_generator = None
        self.sheets_uploader = None
        
        self._init_components()
    
    def _init_components(self):
        """Initialize components - supports graceful degradation"""
        print(f"ProcessCLI v{self.version} initializing...")

        # 1. MD Scanner (required)
        try:
            from md_scanner import MDScanner
            self.md_scanner = MDScanner(md_dir="data/md")
            print("[OK] MDScanner loaded")
        except ImportError as e:
            print(f"[ERROR] MDScanner failed: {e}")
            sys.exit(1)
        
        # 2. MD Parser (required) - Modified version
        try:
            from md_parser import MDParser
            self.md_parser = MDParser()
            print(f"[OK] MDParser v{self.md_parser.version} loaded")
        except ImportError as e:
            print(f"[ERROR] MDParser failed: {e}")
            sys.exit(1)

        # 3. Quality Analyzer (optional)
        try:
            from quality_analyzer import QualityAnalyzer
            self.quality_analyzer = QualityAnalyzer()
            print("[OK] QualityAnalyzer loaded")
        except ImportError as e:
            print(f"[WARN] QualityAnalyzer failed: {e} (will use basic analysis)")
            self.quality_analyzer = None
        
        # 4. Keyword Analyzer (å¯é¸) - v3.6.1
        try:
            from keyword_analyzer import KeywordAnalyzer
            self.keyword_analyzer = KeywordAnalyzer()
            print("âœ… KeywordAnalyzer v3.6.1 è¼‰å…¥æˆåŠŸ")
        except ImportError as e:
            print(f"âš ï¸ KeywordAnalyzer è¼‰å…¥å¤±æ•—: {e} (å°‡è·³éæŸ¥è©¢æ¨¡å¼åˆ†æ)")
            self.keyword_analyzer = None
        
        # 5. Watchlist Analyzer (å¯é¸) - v3.6.1 æ–°å¢
        try:
            from watchlist_analyzer import WatchlistAnalyzer
            self.watchlist_analyzer = WatchlistAnalyzer()
            print("âœ… WatchlistAnalyzer v3.6.1 è¼‰å…¥æˆåŠŸ")
        except ImportError as e:
            print(f"âš ï¸ WatchlistAnalyzer è¼‰å…¥å¤±æ•—: {e} (å°‡è·³éè§€å¯Ÿåå–®åˆ†æ)")
            self.watchlist_analyzer = None
        
        # 6. Report Generator (å¿…éœ€) - ä½¿ç”¨ä¿®æ”¹ç‰ˆ
        try:
            from report_generator import ReportGenerator
            self.report_generator = ReportGenerator()
            print("âœ… ReportGenerator v3.6.1-modified è¼‰å…¥æˆåŠŸ")
        except ImportError as e:
            print(f"âŒ ReportGenerator è¼‰å…¥å¤±æ•—: {e}")
            sys.exit(1)
        
        # 7. Sheets Uploader (å¯é¸)
        try:
            from sheets_uploader import SheetsUploader
            self.sheets_uploader = SheetsUploader()
            print("âœ… SheetsUploader è¼‰å…¥æˆåŠŸ")
        except ImportError as e:
            print(f"âš ï¸ SheetsUploader è¼‰å…¥å¤±æ•—: {e} (å°‡è·³é Google Sheets ä¸Šå‚³)")
            self.sheets_uploader = None
        
        print(f"ğŸ‰ ProcessCLI v{self.version} åˆå§‹åŒ–å®Œæˆ")

    def validate_setup(self) -> bool:
        """é©—è­‰ç³»çµ±è¨­å®š - å¢å¼·ç‰ˆå…§å®¹æ—¥æœŸæª¢æŸ¥"""
        print(f"\n=== ProcessCLI v{self.version} ç³»çµ±é©—è­‰ ===")
        
        validation_results = {}
        overall_status = True
        
        # 1. MD Scanner é©—è­‰
        try:
            md_files = self.md_scanner.scan_all_md_files()
            validation_results['md_scanner'] = {
                'status': 'success',
                'message': f"æ‰¾åˆ° {len(md_files)} å€‹ MD æª”æ¡ˆ",
                'files_count': len(md_files)
            }
            print(f"âœ… MD Scanner: æ‰¾åˆ° {len(md_files)} å€‹æª”æ¡ˆ")
        except Exception as e:
            validation_results['md_scanner'] = {
                'status': 'error',
                'message': f"MD Scanner éŒ¯èª¤: {e}",
                'files_count': 0
            }
            print(f"âŒ MD Scanner éŒ¯èª¤: {e}")
            overall_status = False
        
        # 2. MD Parser é©—è­‰ (åŒ…å«å…§å®¹æ—¥æœŸæª¢æŸ¥)
        try:
            if validation_results['md_scanner']['files_count'] > 0:
                # æ¸¬è©¦è§£æä¸€å€‹æª”æ¡ˆ
                test_file = md_files[0] if md_files else None
                if test_file:
                    test_result = self.md_parser.parse_md_file(test_file)
                    
                    # æª¢æŸ¥å…§å®¹æ—¥æœŸæå–
                    content_date = test_result.get('content_date', '')
                    quality_score = test_result.get('quality_score', 0)
                    
                    validation_results['md_parser'] = {
                        'status': 'success',
                        'message': f"MD Parser v{self.md_parser.version} é‹è¡Œæ­£å¸¸",
                        'test_file': os.path.basename(test_file),
                        'content_date_extracted': bool(content_date),
                        'quality_score': quality_score,
                        'enhanced_scoring': quality_score <= 1 if not content_date else False
                    }
                    
                    if content_date:
                        print(f"âœ… MD Parser: æ¸¬è©¦æˆåŠŸ - å…§å®¹æ—¥æœŸ: {content_date}")
                    else:
                        print(f"âœ… MD Parser: æ¸¬è©¦æˆåŠŸ - ç„¡å…§å®¹æ—¥æœŸï¼Œå“è³ªè©•åˆ†: {quality_score}")
                        if quality_score <= 1:
                            print(f"   ğŸ” å¢å¼·å“è³ªè©•åˆ†: ç¼ºå°‘å…§å®¹æ—¥æœŸå·²æ­£ç¢ºæ‡²ç½°")
                else:
                    validation_results['md_parser'] = {
                        'status': 'warning',
                        'message': "ç„¡å¯æ¸¬è©¦çš„ MD æª”æ¡ˆ"
                    }
                    print(f"âš ï¸ MD Parser: ç„¡æª”æ¡ˆå¯æ¸¬è©¦")
            else:
                validation_results['md_parser'] = {
                    'status': 'warning',
                    'message': "ç„¡ MD æª”æ¡ˆå¯æ¸¬è©¦"
                }
                print(f"âš ï¸ MD Parser: ç„¡æª”æ¡ˆå¯æ¸¬è©¦")
        except Exception as e:
            validation_results['md_parser'] = {
                'status': 'error',
                'message': f"MD Parser éŒ¯èª¤: {e}"
            }
            print(f"âŒ MD Parser éŒ¯èª¤: {e}")
            overall_status = False
        
        # 3. æŸ¥è©¢æ¨¡å¼åˆ†æå™¨é©—è­‰ (v3.6.1)
        if self.keyword_analyzer:
            try:
                validation_results['keyword_analyzer'] = {
                    'status': 'success',
                    'message': "KeywordAnalyzer v3.6.1 å·²è¼‰å…¥ï¼Œæ”¯æ´æ¨™æº–åŒ–æŸ¥è©¢æ¨¡å¼åˆ†æ"
                }
                print("âœ… KeywordAnalyzer: æŸ¥è©¢æ¨¡å¼åˆ†æåŠŸèƒ½å¯ç”¨")
            except Exception as e:
                validation_results['keyword_analyzer'] = {
                    'status': 'error',
                    'message': f"KeywordAnalyzer éŒ¯èª¤: {e}"
                }
                print(f"âŒ KeywordAnalyzer éŒ¯èª¤: {e}")
        else:
            validation_results['keyword_analyzer'] = {
                'status': 'disabled',
                'message': "KeywordAnalyzer æœªè¼‰å…¥"
            }
            print("âš ï¸ KeywordAnalyzer: åŠŸèƒ½åœç”¨")
        
        # 4. è§€å¯Ÿåå–®åˆ†æå™¨é©—è­‰ (v3.6.1)
        if self.watchlist_analyzer:
            try:
                watchlist_size = len(self.watchlist_analyzer.watchlist_mapping)
                validation_results['watchlist_analyzer'] = {
                    'status': 'success',
                    'message': f"WatchlistAnalyzer v3.6.1 å·²è¼‰å…¥ï¼Œè§€å¯Ÿåå–®: {watchlist_size} å®¶å…¬å¸",
                    'watchlist_size': watchlist_size
                }
                print(f"âœ… WatchlistAnalyzer: è§€å¯Ÿåå–®è¼‰å…¥ {watchlist_size} å®¶å…¬å¸")
            except Exception as e:
                validation_results['watchlist_analyzer'] = {
                    'status': 'error',
                    'message': f"WatchlistAnalyzer éŒ¯èª¤: {e}"
                }
                print(f"âŒ WatchlistAnalyzer éŒ¯èª¤: {e}")
        else:
            validation_results['watchlist_analyzer'] = {
                'status': 'disabled',
                'message': "WatchlistAnalyzer æœªè¼‰å…¥"
            }
            print("âš ï¸ WatchlistAnalyzer: åŠŸèƒ½åœç”¨")
        
        # 5. Report Generator é©—è­‰
        try:
            validation_results['report_generator'] = {
                'status': 'success',
                'message': "ReportGenerator v3.6.1-modified å·²è¼‰å…¥ï¼Œæ”¯æ´å¢å¼·å…§å®¹æ—¥æœŸè™•ç†"
            }
            print("âœ… ReportGenerator: å¢å¼·å…§å®¹æ—¥æœŸè™•ç†åŠŸèƒ½å¯ç”¨")
        except Exception as e:
            validation_results['report_generator'] = {
                'status': 'error',
                'message': f"ReportGenerator éŒ¯èª¤: {e}"
            }
            print(f"âŒ ReportGenerator éŒ¯èª¤: {e}")
            overall_status = False
        
        # 6. Sheets Uploader é©—è­‰
        if self.sheets_uploader:
            try:
                # é€™è£¡å¯ä»¥åŠ å…¥å¯¦éš›çš„é€£ç·šæ¸¬è©¦
                validation_results['sheets_uploader'] = {
                    'status': 'success',
                    'message': "SheetsUploader å·²è¼‰å…¥"
                }
                print("âœ… SheetsUploader: Google Sheets ä¸Šå‚³åŠŸèƒ½å¯ç”¨")
            except Exception as e:
                validation_results['sheets_uploader'] = {
                    'status': 'error',
                    'message': f"SheetsUploader éŒ¯èª¤: {e}"
                }
                print(f"âŒ SheetsUploader éŒ¯èª¤: {e}")
        else:
            validation_results['sheets_uploader'] = {
                'status': 'disabled',
                'message': "SheetsUploader æœªè¼‰å…¥"
            }
            print("âš ï¸ SheetsUploader: åŠŸèƒ½åœç”¨")
        
        # ç¸½çµ
        print(f"\n=== é©—è­‰çµæœç¸½çµ ===")
        if overall_status:
            print("ğŸ‰ ç³»çµ±é©—è­‰é€šéï¼æ ¸å¿ƒåŠŸèƒ½å¯ç”¨")
        else:
            print("âš ï¸ ç³»çµ±é©—è­‰éƒ¨åˆ†å¤±æ•—ï¼Œä½†å¯èƒ½ä»å¯é‹è¡Œ")
        
        print(f"ğŸ“‹ å¢å¼·åŠŸèƒ½ç‹€æ…‹:")
        print(f"   å…§å®¹æ—¥æœŸå¢å¼·è™•ç†: âœ… å·²å•Ÿç”¨")
        print(f"   æŸ¥è©¢æ¨¡å¼åˆ†æ: {'âœ… å¯ç”¨' if self.keyword_analyzer else 'âŒ åœç”¨'}")
        print(f"   è§€å¯Ÿåå–®åˆ†æ: {'âœ… å¯ç”¨' if self.watchlist_analyzer else 'âŒ åœç”¨'}")
        print(f"   Google Sheets ä¸Šå‚³: {'âœ… å¯ç”¨' if self.sheets_uploader else 'âŒ åœç”¨'}")
        
        self._save_validation_results(validation_results)
        return overall_status

    def _save_validation_results(self, validation_results: Dict[str, Any]) -> None:
        """å„²å­˜é©—è­‰çµæœï¼ˆåƒ…ä¿ç•™æœ€æ–°æª”ï¼‰"""
        reports_dir = os.path.join("data", "reports")
        os.makedirs(reports_dir, exist_ok=True)
        output_path = os.path.join(reports_dir, "validation_results_latest.json")
        payload = {
            'version': self.version,
            'timestamp': datetime.now().isoformat(),
            'results': validation_results
        }
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(payload, f, ensure_ascii=True, indent=2)

    def process_all_md_files(self, upload_sheets=True, **kwargs) -> bool:
        """MODIFIED: è™•ç†æ‰€æœ‰ MD æª”æ¡ˆ - å¢å¼·å…§å®¹æ—¥æœŸçµ±è¨ˆ"""
        print(f"\n=== å®Œæ•´è™•ç†æ‰€æœ‰ MD æª”æ¡ˆ (v{self.version}) ===")
        
        try:
            # 1. æƒææ‰€æœ‰ MD æª”æ¡ˆ
            print("ğŸ” æƒæ MD æª”æ¡ˆ...")
            md_files = self.md_scanner.scan_all_md_files()
            
            if not md_files:
                print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½• MD æª”æ¡ˆ - ç„¡éœ€è™•ç† (è¦–ç‚ºæˆåŠŸ)")
                return True
            
            print(f"ğŸ“ æ‰¾åˆ° {len(md_files)} å€‹ MD æª”æ¡ˆ")
            
            # 2. è§£ææ¯å€‹æª”æ¡ˆ
            print("ğŸ“– è§£æ MD æª”æ¡ˆ...")
            processed_companies = []
            content_date_stats = {
                'total_processed': 0,
                'successful_date_extraction': 0,
                'failed_date_extraction': 0,
                'low_quality_due_to_missing_date': 0
            }
            
            for i, md_file in enumerate(md_files, 1):
                try:
                    print(f"   è™•ç†ä¸­ ({i}/{len(md_files)}): {os.path.basename(md_file)}")
                    
                    # ä½¿ç”¨ä¿®æ”¹ç‰ˆ MD Parser
                    parsed_data = self.md_parser.parse_md_file(md_file)
                    
                    # ENHANCED: çµ±è¨ˆå…§å®¹æ—¥æœŸæå–æƒ…æ³
                    content_date = parsed_data.get('content_date', '')
                    quality_score = parsed_data.get('quality_score', 0)
                    
                    content_date_stats['total_processed'] += 1
                    
                    if content_date and content_date.strip():
                        content_date_stats['successful_date_extraction'] += 1
                    else:
                        content_date_stats['failed_date_extraction'] += 1
                        if quality_score <= 1:
                            content_date_stats['low_quality_due_to_missing_date'] += 1
                    
                    # å“è³ªåˆ†æ (å¦‚æœå¯ç”¨)
                    if self.quality_analyzer:
                        quality_data = self.quality_analyzer.analyze(parsed_data)
                        parsed_data.update(quality_data)
                    
                    processed_companies.append(parsed_data)
                    
                except Exception as e:
                    print(f"   âš ï¸ è§£æå¤±æ•—: {os.path.basename(md_file)} - {e}")
                    continue
            
            # ENHANCED: é¡¯ç¤ºå…§å®¹æ—¥æœŸçµ±è¨ˆ
            success_rate = (content_date_stats['successful_date_extraction'] / content_date_stats['total_processed'] * 100) if content_date_stats['total_processed'] > 0 else 0
            
            print(f"\nğŸ“Š å…§å®¹æ—¥æœŸæå–çµ±è¨ˆ:")
            print(f"   ç¸½è™•ç†æª”æ¡ˆ: {content_date_stats['total_processed']}")
            print(f"   æˆåŠŸæå–æ—¥æœŸ: {content_date_stats['successful_date_extraction']}")
            print(f"   å¤±æ•—æå–æ—¥æœŸ: {content_date_stats['failed_date_extraction']}")
            print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
            print(f"   ä½å“è³ª(ç¼ºæ—¥æœŸ): {content_date_stats['low_quality_due_to_missing_date']}")
            
            if not processed_companies:
                print("âŒ æ²’æœ‰æˆåŠŸè™•ç†çš„å…¬å¸è³‡æ–™")
                return False
            
            print(f"âœ… æˆåŠŸè™•ç† {len(processed_companies)} å®¶å…¬å¸")
            
            # 3. æŸ¥è©¢æ¨¡å¼åˆ†æ (v3.6.1)
            keyword_analysis = None
            if self.keyword_analyzer:
                print("ğŸ” é€²è¡ŒæŸ¥è©¢æ¨¡å¼åˆ†æ...")
                try:
                    keyword_analysis = self.keyword_analyzer.analyze_query_patterns(processed_companies)
                    pattern_count = len(keyword_analysis.get('pattern_stats', {}))
                    print(f"âœ… æŸ¥è©¢æ¨¡å¼åˆ†æå®Œæˆ: {pattern_count} å€‹æ¨¡å¼")
                except Exception as e:
                    print(f"âš ï¸ æŸ¥è©¢æ¨¡å¼åˆ†æå¤±æ•—: {e}")
                    keyword_analysis = None
            
            # 4. è§€å¯Ÿåå–®åˆ†æ (v3.6.1)
            watchlist_analysis = None
            if self.watchlist_analyzer:
                print("ğŸ” é€²è¡Œè§€å¯Ÿåå–®åˆ†æ...")
                try:
                    watchlist_analysis = self.watchlist_analyzer.analyze_watchlist_coverage(processed_companies)
                    coverage_rate = watchlist_analysis.get('coverage_rate', 0)
                    print(f"âœ… è§€å¯Ÿåå–®åˆ†æå®Œæˆ: è¦†è“‹ç‡ {coverage_rate:.1f}%")
                except Exception as e:
                    print(f"âš ï¸ è§€å¯Ÿåå–®åˆ†æå¤±æ•—: {e}")
                    watchlist_analysis = None
            
            # 5. ç”Ÿæˆå ±å‘Š
            print("ğŸ“‹ ç”Ÿæˆå ±å‘Š...")
            
            # Portfolio Summary
            portfolio_summary = self.report_generator.generate_portfolio_summary(processed_companies)
            print(f"âœ… æŠ•è³‡çµ„åˆæ‘˜è¦: {len(portfolio_summary)} å®¶å…¬å¸")
            
            # Detailed Report  
            detailed_report = self.report_generator.generate_detailed_report(processed_companies)
            print(f"âœ… è©³ç´°å ±å‘Š: {len(detailed_report)} ç­†è¨˜éŒ„")
            
            # Query Pattern Summary (v3.6.1)
            pattern_summary = None
            if keyword_analysis:
                pattern_summary = self.report_generator.generate_keyword_summary(keyword_analysis)
                if pattern_summary is not None:
                    print(f"âœ… æŸ¥è©¢æ¨¡å¼å ±å‘Š: {len(pattern_summary)} å€‹æ¨¡å¼")
                else:
                    print("âš ï¸ æŸ¥è©¢æ¨¡å¼å ±å‘Šç”Ÿæˆå¤±æ•—")

            # Watchlist Summary (v3.6.1)
            watchlist_summary = None
            if watchlist_analysis:
                watchlist_summary = self.report_generator.generate_watchlist_summary(watchlist_analysis)
                if watchlist_summary is not None:
                    print(f"âœ… è§€å¯Ÿåå–®å ±å‘Š: {len(watchlist_summary)} å®¶å…¬å¸")
                else:
                    print("âš ï¸ è§€å¯Ÿåå–®å ±å‘Šç”Ÿæˆå¤±æ•—")
            
            # 6. å„²å­˜å ±å‘Š
            saved_files = self.report_generator.save_all_reports(
                portfolio_summary, 
                detailed_report, 
                pattern_summary if pattern_summary is not None else None,
                watchlist_summary if watchlist_summary is not None else None
            )
            
            # 7. ç”Ÿæˆçµ±è¨ˆå ±å‘Š
            statistics = self.report_generator.generate_statistics_report(processed_companies)
            self.report_generator.save_statistics_report(statistics)
            
            # 8. ä¸Šå‚³åˆ° Google Sheets (å¦‚æœå¯ç”¨ä¸”è¦æ±‚)
            if upload_sheets and self.sheets_uploader:
                print("â˜ï¸ ä¸Šå‚³åˆ° Google Sheets...")
                try:
                    upload_success = self.sheets_uploader.upload_all_reports(
                        portfolio_summary, detailed_report, pattern_summary, watchlist_summary
                    )
                    if upload_success:
                        print("âœ… Google Sheets ä¸Šå‚³æˆåŠŸ")
                    else:
                        print("âš ï¸ Google Sheets ä¸Šå‚³å¤±æ•—")
                except Exception as e:
                    print(f"âš ï¸ Google Sheets ä¸Šå‚³éŒ¯èª¤: {e}")
            elif upload_sheets:
                print("âš ï¸ SheetsUploader æœªè¼‰å…¥ï¼Œè·³éä¸Šå‚³")
            
            print(f"\nğŸ‰ å®Œæ•´è™•ç†æˆåŠŸå®Œæˆï¼")
            print(f"ğŸ“ˆ é—œéµçµ±è¨ˆ:")
            content_date_stats_final = statistics.get('content_date_extraction', {})
            print(f"   å…§å®¹æ—¥æœŸæˆåŠŸç‡: {content_date_stats_final.get('success_rate_percentage', 0)}%")
            print(f"   ç¼ºå°‘æ—¥æœŸä½†åŒ…å«: {content_date_stats_final.get('files_included_despite_missing_date', 0)} æª”æ¡ˆ")
            print(f"   å ±å‘Šè¦†è“‹ç‡: {statistics.get('validation_statistics', {}).get('inclusion_rate', 0)}%")
            
            return True
            
        except Exception as e:
            print(f"âŒ è™•ç†éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            traceback.print_exc()
            return False

    def analyze_content_date_extraction(self, **kwargs) -> bool:
        """ENHANCED: å°ˆé–€åˆ†æå…§å®¹æ—¥æœŸæå–æƒ…æ³"""
        print(f"\n=== å…§å®¹æ—¥æœŸæå–åˆ†æ (v{self.version}) ===")
        
        try:
            # æƒæ MD æª”æ¡ˆ
            md_files = self.md_scanner.scan_all_md_files()
            
            if not md_files:
                print("âŒ æœªæ‰¾åˆ°ä»»ä½• MD æª”æ¡ˆ")
                return False
            
            print(f"ğŸ“ åˆ†æ {len(md_files)} å€‹ MD æª”æ¡ˆçš„å…§å®¹æ—¥æœŸæå–æƒ…æ³")
            
            # è©³ç´°åˆ†æ
            extraction_analysis = {
                'total_files': len(md_files),
                'successful_extractions': 0,
                'failed_extractions': 0,
                'extraction_methods': {},
                'quality_impact': {
                    'high_quality_with_date': 0,
                    'low_quality_no_date': 0,
                    'patterns_successful': [],
                    'patterns_failed': []
                }
            }
            
            print("\nğŸ“Š é€æª”æ¡ˆåˆ†æ:")
            
            for i, md_file in enumerate(md_files, 1):
                try:
                    filename = os.path.basename(md_file)
                    parsed_data = self.md_parser.parse_md_file(md_file)
                    
                    content_date = parsed_data.get('content_date', '')
                    quality_score = parsed_data.get('quality_score', 0)
                    extraction_method = parsed_data.get('date_extraction_method', 'unknown')
                    
                    if content_date and content_date.strip():
                        extraction_analysis['successful_extractions'] += 1
                        if quality_score >= 7:
                            extraction_analysis['quality_impact']['high_quality_with_date'] += 1
                        
                        # è¨˜éŒ„æˆåŠŸçš„æå–æ–¹æ³•
                        if extraction_method not in extraction_analysis['extraction_methods']:
                            extraction_analysis['extraction_methods'][extraction_method] = {'success': 0, 'total': 0}
                        extraction_analysis['extraction_methods'][extraction_method]['success'] += 1
                        
                        print(f"   âœ… {filename}: {content_date} (å“è³ª: {quality_score}, æ–¹æ³•: {extraction_method})")
                    else:
                        extraction_analysis['failed_extractions'] += 1
                        if quality_score <= 1:
                            extraction_analysis['quality_impact']['low_quality_no_date'] += 1
                        
                        print(f"   âŒ {filename}: ç„¡æ—¥æœŸ (å“è³ª: {quality_score}, æ–¹æ³•: {extraction_method})")
                    
                    # è¨˜éŒ„ç¸½æ•¸
                    if extraction_method not in extraction_analysis['extraction_methods']:
                        extraction_analysis['extraction_methods'][extraction_method] = {'success': 0, 'total': 0}
                    extraction_analysis['extraction_methods'][extraction_method]['total'] += 1
                    
                except Exception as e:
                    print(f"   âš ï¸ {filename}: è§£æéŒ¯èª¤ - {e}")
                    extraction_analysis['failed_extractions'] += 1
            
            # çµ±è¨ˆç¸½çµ
            success_rate = (extraction_analysis['successful_extractions'] / extraction_analysis['total_files'] * 100)
            
            print(f"\nğŸ“ˆ æå–çµ±è¨ˆç¸½çµ:")
            print(f"   ç¸½æª”æ¡ˆæ•¸: {extraction_analysis['total_files']}")
            print(f"   æˆåŠŸæå–: {extraction_analysis['successful_extractions']}")
            print(f"   æå–å¤±æ•—: {extraction_analysis['failed_extractions']}")
            print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
            
            print(f"\nğŸ¯ å“è³ªå½±éŸ¿åˆ†æ:")
            print(f"   é«˜å“è³ªä¸”æœ‰æ—¥æœŸ: {extraction_analysis['quality_impact']['high_quality_with_date']}")
            print(f"   ä½å“è³ªå› ç¼ºæ—¥æœŸ: {extraction_analysis['quality_impact']['low_quality_no_date']}")
            
            print(f"\nğŸ”§ æå–æ–¹æ³•æ•ˆæœ:")
            for method, stats in extraction_analysis['extraction_methods'].items():
                method_success_rate = (stats['success'] / stats['total'] * 100) if stats['total'] > 0 else 0
                print(f"   {method}: {stats['success']}/{stats['total']} ({method_success_rate:.1f}%)")
            
            return True
            
        except Exception as e:
            print(f"âŒ å…§å®¹æ—¥æœŸåˆ†æå¤±æ•—: {e}")
            traceback.print_exc()
            return False

    def analyze_keywords_only(self, **kwargs) -> bool:
        """åªé€²è¡ŒæŸ¥è©¢æ¨¡å¼åˆ†æ (v3.6.1)"""
        print(f"\n=== æŸ¥è©¢æ¨¡å¼åˆ†æ (v{self.version}) ===")
        
        if not self.keyword_analyzer:
            print("âŒ KeywordAnalyzer æœªè¼‰å…¥ï¼Œç„¡æ³•é€²è¡Œåˆ†æ")
            return False
        
        try:
            # æƒæå’Œè§£ææª”æ¡ˆ
            md_files = self.md_scanner.scan_all_md_files()
            if not md_files:
                print("âŒ æœªæ‰¾åˆ°ä»»ä½• MD æª”æ¡ˆ")
                return False
            
            print(f"ğŸ“ è™•ç† {len(md_files)} å€‹æª”æ¡ˆé€²è¡ŒæŸ¥è©¢æ¨¡å¼åˆ†æ")
            
            processed_companies = []
            for md_file in md_files:
                try:
                    parsed_data = self.md_parser.parse_md_file(md_file)
                    processed_companies.append(parsed_data)
                except Exception as e:
                    print(f"âš ï¸ è§£æå¤±æ•—: {os.path.basename(md_file)}")
                    continue
            
            # é€²è¡ŒæŸ¥è©¢æ¨¡å¼åˆ†æ
            keyword_analysis = self.keyword_analyzer.analyze_query_patterns(processed_companies)
            
            pattern_count = len(keyword_analysis.get('pattern_stats', {}))
            print(f"âœ… åˆ†æå®Œæˆ: ç™¼ç¾ {pattern_count} å€‹æŸ¥è©¢æ¨¡å¼")
            
            # é¡¯ç¤ºå‰å¹¾å€‹æ¨¡å¼
            pattern_stats = keyword_analysis.get('pattern_stats', {})
            if pattern_stats:
                print(f"\nğŸ” å‰5å€‹æœ€æœ‰æ•ˆçš„æŸ¥è©¢æ¨¡å¼:")
                sorted_patterns = sorted(pattern_stats.items(), 
                                       key=lambda x: x[1]['avg_quality_score'], 
                                       reverse=True)
                
                for i, (pattern, stats) in enumerate(sorted_patterns[:5], 1):
                    print(f"   {i}. {pattern}")
                    print(f"      ä½¿ç”¨æ¬¡æ•¸: {stats['usage_count']}, å¹³å‡å“è³ª: {stats['avg_quality_score']:.1f}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æŸ¥è©¢æ¨¡å¼åˆ†æå¤±æ•—: {e}")
            traceback.print_exc()
            return False

    def analyze_watchlist_only(self, **kwargs) -> bool:
        """åªé€²è¡Œè§€å¯Ÿåå–®åˆ†æ (v3.6.1)"""
        print(f"\n=== è§€å¯Ÿåå–®åˆ†æ (v{self.version}) ===")
        
        if not self.watchlist_analyzer:
            print("âŒ WatchlistAnalyzer æœªè¼‰å…¥ï¼Œç„¡æ³•é€²è¡Œåˆ†æ")
            return False
        
        try:
            # æƒæå’Œè§£ææª”æ¡ˆ
            md_files = self.md_scanner.scan_all_md_files()
            if not md_files:
                print("âŒ æœªæ‰¾åˆ°ä»»ä½• MD æª”æ¡ˆ")
                return False
            
            print(f"ğŸ“ è™•ç† {len(md_files)} å€‹æª”æ¡ˆé€²è¡Œè§€å¯Ÿåå–®åˆ†æ")
            
            processed_companies = []
            for md_file in md_files:
                try:
                    parsed_data = self.md_parser.parse_md_file(md_file)
                    processed_companies.append(parsed_data)
                except Exception as e:
                    print(f"âš ï¸ è§£æå¤±æ•—: {os.path.basename(md_file)}")
                    continue
            
            # é€²è¡Œè§€å¯Ÿåå–®åˆ†æ
            watchlist_analysis = self.watchlist_analyzer.analyze_watchlist_coverage(processed_companies)
            
            coverage_rate = watchlist_analysis.get('coverage_rate', 0)
            total_watchlist = watchlist_analysis.get('total_watchlist_companies', 0)
            processed_count = watchlist_analysis.get('companies_with_md_files', 0)
            
            print(f"âœ… åˆ†æå®Œæˆ:")
            print(f"   è§€å¯Ÿåå–®ç¸½æ•¸: {total_watchlist} å®¶å…¬å¸")
            print(f"   å·²è™•ç†å…¬å¸: {processed_count} å®¶å…¬å¸")
            print(f"   è¦†è“‹ç‡: {coverage_rate:.1f}%")
            
            # é¡¯ç¤ºç‹€æ…‹åˆ†å¸ƒ
            status_summary = watchlist_analysis.get('company_status_summary', {})
            if status_summary:
                print(f"\nğŸ“Š è™•ç†ç‹€æ…‹åˆ†å¸ƒ:")
                for status, count in status_summary.items():
                    status_name = {
                        'processed': 'âœ… å·²è™•ç†',
                        'not_found': 'âŒ æœªæ‰¾åˆ°',
                        'validation_failed': 'ğŸš« é©—è­‰å¤±æ•—',
                        'low_quality': 'ğŸ”´ å“è³ªéä½',
                        'multiple_files': 'ğŸ“„ å¤šå€‹æª”æ¡ˆ'
                    }.get(status, status)
                    print(f"   {status_name}: {count} å®¶")
            
            return True
            
        except Exception as e:
            print(f"âŒ è§€å¯Ÿåå–®åˆ†æå¤±æ•—: {e}")
            traceback.print_exc()
            return False

    def generate_keyword_summary(self, upload_sheets=True, **kwargs) -> bool:
        """ç”ŸæˆæŸ¥è©¢æ¨¡å¼çµ±è¨ˆå ±å‘Š (v3.6.1)"""
        print(f"\n=== ç”ŸæˆæŸ¥è©¢æ¨¡å¼çµ±è¨ˆå ±å‘Š (v{self.version}) ===")
        
        if not self.keyword_analyzer:
            print("âŒ KeywordAnalyzer æœªè¼‰å…¥ï¼Œç„¡æ³•ç”Ÿæˆå ±å‘Š")
            return False
        
        try:
            # ç²å–åˆ†æçµæœ
            md_files = self.md_scanner.scan_all_md_files()
            processed_companies = []
            
            for md_file in md_files:
                try:
                    parsed_data = self.md_parser.parse_md_file(md_file)
                    processed_companies.append(parsed_data)
                except:
                    continue
            
            keyword_analysis = self.keyword_analyzer.analyze_query_patterns(processed_companies)
            
            # ç”Ÿæˆå ±å‘Š
            keyword_summary = self.report_generator.generate_keyword_summary(keyword_analysis)
            
            # å„²å­˜å ±å‘Š
            saved_path = self.report_generator.save_keyword_summary(keyword_summary)
            
            # ä¸Šå‚³ (å¦‚æœéœ€è¦)
            if upload_sheets and self.sheets_uploader:
                try:
                    self.sheets_uploader.upload_keyword_summary(keyword_summary)
                    print("âœ… å·²ä¸Šå‚³åˆ° Google Sheets")
                except Exception as e:
                    print(f"âš ï¸ Google Sheets ä¸Šå‚³å¤±æ•—: {e}")
            
            print(f"âœ… æŸ¥è©¢æ¨¡å¼çµ±è¨ˆå ±å‘Šç”Ÿæˆå®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ ç”ŸæˆæŸ¥è©¢æ¨¡å¼å ±å‘Šå¤±æ•—: {e}")
            traceback.print_exc()
            return False

    def generate_watchlist_summary(self, upload_sheets=True, **kwargs) -> bool:
        """ç”Ÿæˆè§€å¯Ÿåå–®çµ±è¨ˆå ±å‘Š (v3.6.1)"""
        print(f"\n=== ç”Ÿæˆè§€å¯Ÿåå–®çµ±è¨ˆå ±å‘Š (v{self.version}) ===")
        
        if not self.watchlist_analyzer:
            print("âŒ WatchlistAnalyzer æœªè¼‰å…¥ï¼Œç„¡æ³•ç”Ÿæˆå ±å‘Š")
            return False
        
        try:
            # ç²å–åˆ†æçµæœ
            md_files = self.md_scanner.scan_all_md_files()
            processed_companies = []
            
            for md_file in md_files:
                try:
                    parsed_data = self.md_parser.parse_md_file(md_file)
                    processed_companies.append(parsed_data)
                except:
                    continue
            
            watchlist_analysis = self.watchlist_analyzer.analyze_watchlist_coverage(processed_companies)
            
            # ç”Ÿæˆå ±å‘Š
            watchlist_summary = self.report_generator.generate_watchlist_summary(watchlist_analysis)
            
            # å„²å­˜å ±å‘Š
            saved_path = self.report_generator.save_watchlist_summary(watchlist_summary)
            
            # ä¸Šå‚³ (å¦‚æœéœ€è¦)
            if upload_sheets and self.sheets_uploader:
                try:
                    self.sheets_uploader.upload_watchlist_summary(watchlist_summary)
                    print("âœ… å·²ä¸Šå‚³åˆ° Google Sheets")
                except Exception as e:
                    print(f"âš ï¸ Google Sheets ä¸Šå‚³å¤±æ•—: {e}")
            
            print(f"âœ… è§€å¯Ÿåå–®çµ±è¨ˆå ±å‘Šç”Ÿæˆå®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆè§€å¯Ÿåå–®å ±å‘Šå¤±æ•—: {e}")
            traceback.print_exc()
            return False

    def generate_csv_only(self) -> bool:
        """åƒ…ç”Ÿæˆ CSV å ±å‘Š (ç”¨æ–¼ Quarantine åµæ¸¬)

        è¼•é‡ç´šè™•ç†:
        - æƒæå’Œè§£æ MD æª”æ¡ˆ
        - ç”Ÿæˆ factset_detailed_report_latest.csv
        - ä¸ä¸Šå‚³ã€ä¸ç”Ÿæˆå…¶ä»–å ±å‘Š
        - ç”¨æ–¼ Quarantine workflow çš„åµæ¸¬ä¾†æº
        """
        print(f"\n=== ç”Ÿæˆ CSV å ±å‘Š (v{self.version}) ===")

        try:
            # 1. æƒæ MD æª”æ¡ˆ
            print("ğŸ“ æƒæ MD æª”æ¡ˆ...")
            md_files = self.md_scanner.scan_all_md_files()

            if not md_files:
                print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½• MD æª”æ¡ˆ - ç„¡éœ€ç”Ÿæˆ CSV (è¦–ç‚ºæˆåŠŸ)")
                return True

            print(f"âœ… æ‰¾åˆ° {len(md_files)} å€‹ MD æª”æ¡ˆ")

            # 2. è§£æ MD æª”æ¡ˆ
            print("ğŸ”„ è§£æ MD æª”æ¡ˆ...")
            processed_companies = []

            for i, md_file in enumerate(md_files, 1):
                try:
                    print(f"   è™•ç†ä¸­ ({i}/{len(md_files)}): {os.path.basename(md_file)}")
                    parsed_data = self.md_parser.parse_md_file(md_file)
                    processed_companies.append(parsed_data)
                except Exception as e:
                    print(f"   âš ï¸ è§£æå¤±æ•—: {os.path.basename(md_file)} - {e}")
                    continue

            if not processed_companies:
                print("âŒ æ²’æœ‰æˆåŠŸè™•ç†çš„å…¬å¸è³‡æ–™")
                return False

            print(f"âœ… æˆåŠŸè™•ç† {len(processed_companies)} å®¶å…¬å¸")

            # 3. ç”Ÿæˆè©³ç´°å ±å‘Š CSV
            print("ğŸ“‹ ç”Ÿæˆè©³ç´°å ±å‘Š...")
            detailed_report = self.report_generator.generate_detailed_report(processed_companies)
            print(f"âœ… è©³ç´°å ±å‘Š: {len(detailed_report)} ç­†è¨˜éŒ„")

            # 4. åƒ…å„²å­˜è©³ç´°å ±å‘Š CSV
            output_path = os.path.join(self.report_generator.output_dir, 'factset_detailed_report_latest.csv')
            detailed_report.to_csv(output_path, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ å·²å„²å­˜: {output_path}")

            print(f"\nâœ… CSV å ±å‘Šç”Ÿæˆå®Œæˆï¼")
            print(f"ğŸ“Š ç”¨é€”: Quarantine workflow åµæ¸¬ä¾†æº")

            return True

        except Exception as e:
            print(f"âŒ CSV ç”Ÿæˆå¤±æ•—: {e}")
            traceback.print_exc()
            return False

    def show_stats(self) -> bool:
        """é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š - å¢å¼·å…§å®¹æ—¥æœŸçµ±è¨ˆ"""
        print(f"\n=== ProcessCLI v{self.version} çµ±è¨ˆè³‡è¨Š ===")

        try:
            # MD æª”æ¡ˆçµ±è¨ˆ
            md_files = self.md_scanner.scan_all_md_files()
            stats = self.md_scanner.get_stats()

            print(f"ğŸ“ MD æª”æ¡ˆçµ±è¨ˆ:")
            print(f"   ç¸½æª”æ¡ˆæ•¸: {len(md_files)}")
            print(f"   æª”æ¡ˆç¸½å¤§å°: {stats.get('total_size_mb', 0):.1f} MB")
            print(f"   æœ€æ–°æª”æ¡ˆ: {stats.get('newest_file', 'N/A')}")
            print(f"   æœ€èˆŠæª”æ¡ˆ: {stats.get('oldest_file', 'N/A')}")

            # å¿«é€Ÿå…§å®¹æ—¥æœŸçµ±è¨ˆ
            if md_files:
                print(f"\nğŸ“… å…§å®¹æ—¥æœŸæå–å¿«é€Ÿçµ±è¨ˆ (æŠ½æ¨£å‰10å€‹æª”æ¡ˆ):")
                sample_files = md_files[:10]
                sample_stats = {'with_date': 0, 'without_date': 0, 'low_quality': 0}

                for md_file in sample_files:
                    try:
                        parsed_data = self.md_parser.parse_md_file(md_file)
                        content_date = parsed_data.get('content_date', '')
                        quality_score = parsed_data.get('quality_score', 0)

                        if content_date:
                            sample_stats['with_date'] += 1
                        else:
                            sample_stats['without_date'] += 1
                            if quality_score <= 1:
                                sample_stats['low_quality'] += 1
                    except:
                        sample_stats['without_date'] += 1

                sample_success_rate = (sample_stats['with_date'] / len(sample_files) * 100)
                print(f"   æŠ½æ¨£æˆåŠŸç‡: {sample_success_rate:.1f}% ({sample_stats['with_date']}/{len(sample_files)})")
                print(f"   ä½å“è³ª(ç¼ºæ—¥æœŸ): {sample_stats['low_quality']}")

            # æ¨¡çµ„ç‹€æ…‹
            print(f"\nğŸ”§ æ¨¡çµ„ç‹€æ…‹:")
            print(f"   MD Parser: âœ… v{self.md_parser.version if self.md_parser else 'N/A'}")
            print(f"   Quality Analyzer: {'âœ…' if self.quality_analyzer else 'âŒ'}")
            print(f"   Keyword Analyzer: {'âœ…' if self.keyword_analyzer else 'âŒ'}")
            print(f"   Watchlist Analyzer: {'âœ…' if self.watchlist_analyzer else 'âŒ'}")
            print(f"   Report Generator: {'âœ…' if self.report_generator else 'âŒ'}")
            print(f"   Sheets Uploader: {'âœ…' if self.sheets_uploader else 'âŒ'}")

            return True

        except Exception as e:
            print(f"âŒ çµ±è¨ˆè³‡è¨Šç²å–å¤±æ•—: {e}")
            return False


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    parser = argparse.ArgumentParser(
        description='FactSet è™•ç†ç³»çµ± v3.6.1-modified - å¢å¼·å…§å®¹æ—¥æœŸè™•ç†',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
å‘½ä»¤ç¯„ä¾‹:
  python process_cli.py validate                    # é©—è­‰ç³»çµ±è¨­å®š
  python process_cli.py process                     # å®Œæ•´è™•ç†æ‰€æœ‰æª”æ¡ˆ
  python process_cli.py process --no-upload         # è™•ç†ä½†ä¸ä¸Šå‚³
  python process_cli.py generate-csv                # åƒ…ç”Ÿæˆ CSV (ç”¨æ–¼ Quarantine)
  python process_cli.py analyze-content-date        # åˆ†æå…§å®¹æ—¥æœŸæå–
  python process_cli.py analyze-keywords            # æŸ¥è©¢æ¨¡å¼åˆ†æ
  python process_cli.py analyze-watchlist           # è§€å¯Ÿåå–®åˆ†æ
  python process_cli.py keyword-summary             # ç”ŸæˆæŸ¥è©¢æ¨¡å¼å ±å‘Š
  python process_cli.py watchlist-summary           # ç”Ÿæˆè§€å¯Ÿåå–®å ±å‘Š
  python process_cli.py stats                       # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š

v3.6.1-modified å¢å¼·åŠŸèƒ½:
  âœ… ç¼ºå°‘å…§å®¹æ—¥æœŸçš„æª”æ¡ˆé¡¯ç¤ºä½å“è³ªè©•åˆ†è€Œéæ’é™¤
  âœ… è©³ç´°çš„å…§å®¹æ—¥æœŸæå–æˆåŠŸç‡çµ±è¨ˆ
  âœ… æ¨™æº–åŒ–æŸ¥è©¢æ¨¡å¼åˆ†æå’Œå ±å‘Š
  âœ… è§€å¯Ÿåå–®è¦†è“‹ç‡åˆ†æå’Œå ±å‘Š
  âœ… è¼•é‡ç´š CSV ç”Ÿæˆ (generate-csv) ç”¨æ–¼ Quarantine åµæ¸¬
        """
    )
    
    parser.add_argument('command', choices=[
        'validate',                # é©—è­‰ç³»çµ±è¨­å®š
        'process',                 # è™•ç†æ‰€æœ‰ MD æª”æ¡ˆ
        'process-recent',          # è™•ç†æœ€è¿‘çš„ MD æª”æ¡ˆ
        'process-single',          # è™•ç†å–®ä¸€å…¬å¸
        'generate-csv',            # åƒ…ç”Ÿæˆ CSV (ç”¨æ–¼ Quarantine)
        'analyze-quality',         # å“è³ªåˆ†æ
        'analyze-keywords',        # æŸ¥è©¢æ¨¡å¼åˆ†æ (v3.6.1)
        'analyze-watchlist',       # è§€å¯Ÿåå–®åˆ†æ (v3.6.1)
        'analyze-content-date',    # å…§å®¹æ—¥æœŸæå–åˆ†æ (modified)
        'keyword-summary',         # æŸ¥è©¢æ¨¡å¼çµ±è¨ˆå ±å‘Š (v3.6.1)
        'watchlist-summary',       # è§€å¯Ÿåå–®çµ±è¨ˆå ±å‘Š (v3.6.1)
        'stats',                   # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
    ])
    
    parser.add_argument('--company', help='å…¬å¸ä»£è™Ÿ')
    parser.add_argument('--hours', type=int, default=24, help='å°æ™‚æ•¸')
    parser.add_argument('--no-upload', action='store_true', help='ä¸ä¸Šå‚³åˆ° Sheets')
    parser.add_argument('--force-upload', action='store_true', help='å¼·åˆ¶ä¸Šå‚³ï¼Œå¿½ç•¥é©—è­‰éŒ¯èª¤')
    parser.add_argument('--min-usage', type=int, default=1, help='æŸ¥è©¢æ¨¡å¼æœ€å°ä½¿ç”¨æ¬¡æ•¸')
    parser.add_argument('--include-missing', action='store_true', help='åŒ…å«ç¼ºå¤±å…¬å¸è³‡è¨Š')
    parser.add_argument('--dry-run', action='store_true', help='é è¦½æ¨¡å¼ï¼Œä¸å¯¦éš›åŸ·è¡Œ')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ– CLI
    try:
        cli = ProcessCLI()
    except Exception as e:
        print(f"âŒ ProcessCLI åˆå§‹åŒ–å¤±æ•—: {e}")
        sys.exit(1)
    
    # åŸ·è¡Œå‘½ä»¤
    success = False
    upload_sheets = not args.no_upload
    
    try:
        if args.command == 'validate':
            success = cli.validate_setup()

        elif args.command == 'process':
            success = cli.process_all_md_files(upload_sheets=upload_sheets)

        elif args.command == 'generate-csv':
            success = cli.generate_csv_only()

        elif args.command == 'analyze-content-date':
            success = cli.analyze_content_date_extraction()

        elif args.command == 'analyze-keywords':
            success = cli.analyze_keywords_only()

        elif args.command == 'analyze-watchlist':
            success = cli.analyze_watchlist_only()

        elif args.command == 'keyword-summary':
            success = cli.generate_keyword_summary(upload_sheets=upload_sheets)

        elif args.command == 'watchlist-summary':
            success = cli.generate_watchlist_summary(upload_sheets=upload_sheets)

        elif args.command == 'stats':
            success = cli.show_stats()

        else:
            print(f"âŒ æœªå¯¦ç¾çš„å‘½ä»¤: {args.command}")
            success = False
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ç”¨æˆ¶ä¸­æ–·æ“ä½œ")
        success = False
    except Exception as e:
        print(f"âŒ åŸ·è¡Œå‘½ä»¤æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        traceback.print_exc()
        success = False
    
    # çµæŸ
    if success:
        print(f"\nâœ… å‘½ä»¤ '{args.command}' åŸ·è¡ŒæˆåŠŸ")
        sys.exit(0)
    else:
        print(f"\nâŒ å‘½ä»¤ '{args.command}' åŸ·è¡Œå¤±æ•—")
        sys.exit(1)


if __name__ == "__main__":
    main()
