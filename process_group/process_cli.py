#!/usr/bin/env python3
"""
FactSet Pipeline v3.6.1 - Process CLI (Complete Implementation)
è™•ç†ç¾¤çµ„çš„å‘½ä»¤åˆ—ä»‹é¢ - å®Œæ•´çš„è§€å¯Ÿåå–®çµ±è¨ˆå’Œåˆ†æåŠŸèƒ½
"""

import sys
import os
import re
import argparse
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any

# ğŸ”§ è¼‰å…¥ç’°å¢ƒè®Šæ•¸
try:
    from dotenv import load_dotenv
    # è¼‰å…¥ .env æª”æ¡ˆ - å˜—è©¦å¤šå€‹è·¯å¾‘
    env_paths = [
        '.env',
        '../.env', 
        '../../.env',
        os.path.join(os.path.dirname(__file__), '.env'),
        os.path.join(os.path.dirname(__file__), '../.env')
    ]
    
    env_loaded = False
    for env_path in env_paths:
        if os.path.exists(env_path):
            load_dotenv(env_path)
            print(f"âœ… è¼‰å…¥ç’°å¢ƒè®Šæ•¸: {env_path}")
            env_loaded = True
            break
    
    if not env_loaded:
        print("âš ï¸ æœªæ‰¾åˆ° .env æª”æ¡ˆï¼Œä½¿ç”¨ç³»çµ±ç’°å¢ƒè®Šæ•¸")

except ImportError:
    print("âš ï¸ python-dotenv æœªå®‰è£ï¼Œä½¿ç”¨ç³»çµ±ç’°å¢ƒè®Šæ•¸")

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å°å…¥çµ„ä»¶
try:
    from md_scanner import MDScanner
    print("âœ… MDScanner å·²è¼‰å…¥")
except ImportError as e:
    print(f"âŒ MDScanner è¼‰å…¥å¤±æ•—: {e}")
    MDScanner = None

try:
    from md_parser import MDParser
    print("âœ… MDParser å·²è¼‰å…¥")
except ImportError as e:
    print(f"âŒ MDParser è¼‰å…¥å¤±æ•—: {e}")
    MDParser = None

try:
    from quality_analyzer import QualityAnalyzer
    print("âœ… QualityAnalyzer å·²è¼‰å…¥")
except ImportError as e:
    print(f"âŒ QualityAnalyzer è¼‰å…¥å¤±æ•—: {e}")
    QualityAnalyzer = None

try:
    from keyword_analyzer import KeywordAnalyzer
    print("âœ… KeywordAnalyzer å·²è¼‰å…¥")
except ImportError as e:
    print(f"âŒ KeywordAnalyzer è¼‰å…¥å¤±æ•—: {e}")
    KeywordAnalyzer = None

try:
    from watchlist_analyzer import WatchlistAnalyzer
    print("âœ… WatchlistAnalyzer v3.6.1 å·²è¼‰å…¥")
except ImportError as e:
    print(f"âŒ WatchlistAnalyzer è¼‰å…¥å¤±æ•—: {e}")
    WatchlistAnalyzer = None

try:
    from report_generator import ReportGenerator
    print("âœ… ReportGenerator å·²è¼‰å…¥")
except ImportError as e:
    print(f"âŒ ReportGenerator è¼‰å…¥å¤±æ•—: {e}")
    ReportGenerator = None

try:
    from sheets_uploader import SheetsUploader
    print("âœ… SheetsUploader å·²è¼‰å…¥")
except ImportError as e:
    print(f"âš ï¸ SheetsUploader è¼‰å…¥å¤±æ•—: {e}")
    SheetsUploader = None


class ProcessCLI:
    """è™•ç†å‘½ä»¤åˆ—ä»‹é¢ - v3.6.1 å®Œæ•´å¯¦ç¾"""
    
    def __init__(self):
        print("ğŸ”§ åˆå§‹åŒ– ProcessCLI v3.6.1...")
        
        # æ ¸å¿ƒçµ„ä»¶ - MDScanner æ˜¯å¿…é ˆçš„
        if MDScanner:
            self.md_scanner = MDScanner()
            print("âœ… MDScanner å·²åˆå§‹åŒ–")
        else:
            raise ImportError("MDScanner æ˜¯å¿…é ˆçš„çµ„ä»¶")
        
        # åˆå§‹åŒ–å…¶ä»–çµ„ä»¶
        self.md_parser = None
        self.quality_analyzer = None  
        self.keyword_analyzer = None
        self.watchlist_analyzer = None
        self.report_generator = None
        self.sheets_uploader = None
        
        self._init_components()
        self._ensure_output_directories()
    
    def _init_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çµ„ä»¶"""
        # MD Parser
        if MDParser:
            try:
                self.md_parser = MDParser()
                validation_status = "å•Ÿç”¨" if self.md_parser.validation_enabled else "åœç”¨"
                watch_list_size = len(self.md_parser.watch_list_mapping)
                print(f"âœ… MDParser å·²åˆå§‹åŒ– - è§€å¯Ÿåå–®é©—è­‰: {validation_status} ({watch_list_size} å®¶å…¬å¸)")
            except Exception as e:
                print(f"âŒ MDParser åˆå§‹åŒ–å¤±æ•—: {e}")
        
        # Quality Analyzer
        if QualityAnalyzer:
            try:
                self.quality_analyzer = QualityAnalyzer()
                print("âœ… QualityAnalyzer å·²åˆå§‹åŒ–")
            except Exception as e:
                print(f"âŒ QualityAnalyzer åˆå§‹åŒ–å¤±æ•—: {e}")
        
        # Keyword Analyzer
        if KeywordAnalyzer:
            try:
                self.keyword_analyzer = KeywordAnalyzer()
                print("âœ… KeywordAnalyzer å·²åˆå§‹åŒ–")
            except Exception as e:
                print(f"âŒ KeywordAnalyzer åˆå§‹åŒ–å¤±æ•—: {e}")
                self.keyword_analyzer = None
        
        # Watchlist Analyzer v3.6.1
        if WatchlistAnalyzer:
            try:
                self.watchlist_analyzer = WatchlistAnalyzer()
                watchlist_status = "å•Ÿç”¨" if self.watchlist_analyzer.validation_enabled else "åœç”¨"
                watchlist_size = len(self.watchlist_analyzer.watchlist_mapping)
                print(f"âœ… WatchlistAnalyzer v3.6.1 å·²åˆå§‹åŒ– - è§€å¯Ÿåå–®åˆ†æ: {watchlist_status} ({watchlist_size} å®¶å…¬å¸)")
            except Exception as e:
                print(f"âŒ WatchlistAnalyzer åˆå§‹åŒ–å¤±æ•—: {e}")
                self.watchlist_analyzer = None
        
        # Report Generator
        if ReportGenerator:
            try:
                self.report_generator = ReportGenerator()
                print("âœ… ReportGenerator å·²åˆå§‹åŒ–")
            except Exception as e:
                print(f"âŒ ReportGenerator åˆå§‹åŒ–å¤±æ•—: {e}")
                self.report_generator = None
        
        # Sheets Uploader
        if SheetsUploader:
            try:
                self.sheets_uploader = SheetsUploader()
                print("âœ… SheetsUploader å·²åˆå§‹åŒ–")
            except Exception as e:
                print(f"âš ï¸ SheetsUploader åˆå§‹åŒ–å¤±æ•—: {e}")
                self.sheets_uploader = None

    def process_all_md_files(self, upload_sheets=True, **kwargs):
        """ğŸ”§ v3.6.1 è™•ç†æ‰€æœ‰ MD æª”æ¡ˆ - æ–°å¢è§€å¯Ÿåå–®åˆ†æ"""
        print("\nğŸš€ é–‹å§‹è™•ç†æ‰€æœ‰ MD æª”æ¡ˆ v3.6.1...")
        
        # æª¢æŸ¥è§€å¯Ÿåå–®ç‹€æ…‹
        if self.md_parser:
            validation_status = "å•Ÿç”¨" if self.md_parser.validation_enabled else "åœç”¨"
            watch_list_size = len(self.md_parser.watch_list_mapping)
            print(f"ğŸ“‹ è§€å¯Ÿåå–®é©—è­‰: {validation_status} ({watch_list_size} å®¶å…¬å¸)")
        
        if self.watchlist_analyzer:
            watchlist_analysis_status = "å•Ÿç”¨" if self.watchlist_analyzer.validation_enabled else "åœç”¨"
            watchlist_size = len(self.watchlist_analyzer.watchlist_mapping)
            print(f"ğŸ“Š è§€å¯Ÿåå–®åˆ†æ: {watchlist_analysis_status} ({watchlist_size} å®¶å…¬å¸)")
        
        # æƒæ MD æª”æ¡ˆ
        md_files = self.md_scanner.scan_all_md_files()
        print(f"ğŸ“ ç™¼ç¾ {len(md_files)} å€‹ MD æª”æ¡ˆ")
        
        if not md_files:
            print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½• MD æª”æ¡ˆ")
            print("ğŸ’¡ è«‹å…ˆåŸ·è¡Œæœå°‹ç¾¤çµ„ä¾†ç”Ÿæˆ MD æª”æ¡ˆ")
            return []
        
        # è™•ç†æª”æ¡ˆ
        processed_companies = self._process_md_file_list_v361(md_files, **kwargs)
        
        # ç”Ÿæˆå ±å‘Šå‰çš„æœ€çµ‚çµ±è¨ˆ
        if processed_companies:
            print(f"\nğŸ¯ å ±å‘Šç”Ÿæˆéšæ®µ v3.6.1:")
            
            # é å…ˆæª¢æŸ¥æœ‰å¤šå°‘å…¬å¸æœƒè¢«åŒ…å«åœ¨å ±å‘Šä¸­
            if self.report_generator:
                companies_for_report = [c for c in processed_companies 
                                      if self.report_generator._should_include_in_report_v351(c)]
                
                excluded_count = len(processed_companies) - len(companies_for_report)
                
                print(f"ğŸ“Š è™•ç†çµæœæ‘˜è¦:")
                print(f"   å·²è™•ç†å…¬å¸: {len(processed_companies)} å®¶")
                print(f"   å°‡ç´å…¥å ±å‘Š: {len(companies_for_report)} å®¶")
                print(f"   å› é©—è­‰å¤±æ•—æ’é™¤: {excluded_count} å®¶")
                
                if excluded_count > 0:
                    print(f"   âœ… æˆåŠŸéæ¿¾äº† {excluded_count} å®¶æœ‰å•é¡Œçš„å…¬å¸")
            
            # ç”Ÿæˆå’Œä¸Šå‚³å ±å‘Š (åŒ…å«è§€å¯Ÿåå–®å ±å‘Š)
            self._generate_and_upload_reports_v361(processed_companies, upload_sheets, 
                                                   force_upload=kwargs.get('force_upload', False))
            
            print(f"âœ… è™•ç†å®Œæˆ")
            
            # é¡¯ç¤ºæœ€çµ‚é©—è­‰æ‘˜è¦
            self._display_processing_validation_summary_v361(processed_companies)
        else:
            print("âŒ æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•æª”æ¡ˆ")
        
        return processed_companies

    def process_recent_files(self, hours=24, upload_sheets=True, **kwargs):
        """ğŸ”§ v3.6.1 è™•ç†æœ€è¿‘çš„ MD æª”æ¡ˆ"""
        print(f"\nğŸš€ è™•ç†æœ€è¿‘ {hours} å°æ™‚çš„ MD æª”æ¡ˆ v3.6.1...")
        
        recent_files = self.md_scanner.scan_recent_files(hours)
        print(f"ğŸ“ ç™¼ç¾ {len(recent_files)} å€‹æœ€è¿‘çš„ MD æª”æ¡ˆ")
        
        if not recent_files:
            print(f"âŒ æœ€è¿‘ {hours} å°æ™‚å…§æ²’æœ‰ MD æª”æ¡ˆ")
            return []
        
        # è™•ç†æœ€è¿‘çš„æª”æ¡ˆ
        processed_companies = self._process_md_file_list_v361(recent_files, **kwargs)
        
        if processed_companies:
            # ç”Ÿæˆå’Œä¸Šå‚³å ±å‘Š
            self._generate_and_upload_reports_v361(processed_companies, upload_sheets,
                                                   force_upload=kwargs.get('force_upload', False))
            
            print(f"âœ… æœ€è¿‘æª”æ¡ˆè™•ç†å®Œæˆ")
        
        return processed_companies

    def process_single_company(self, company_code, upload_sheets=True, **kwargs):
        """ğŸ”§ v3.6.1 è™•ç†å–®ä¸€å…¬å¸"""
        print(f"\nğŸš€ è™•ç†å–®ä¸€å…¬å¸: {company_code} v3.6.1...")
        
        company_files = self.md_scanner.find_company_files(company_code)
        print(f"ğŸ“ ç™¼ç¾å…¬å¸ {company_code} çš„ {len(company_files)} å€‹ MD æª”æ¡ˆ")
        
        if not company_files:
            print(f"âŒ æ²’æœ‰æ‰¾åˆ°å…¬å¸ {company_code} çš„ MD æª”æ¡ˆ")
            return []
        
        # è™•ç†å…¬å¸æª”æ¡ˆ
        processed_companies = self._process_md_file_list_v361(company_files, **kwargs)
        
        if processed_companies:
            # ç”Ÿæˆå’Œä¸Šå‚³å ±å‘Š
            self._generate_and_upload_reports_v361(processed_companies, upload_sheets,
                                                   force_upload=kwargs.get('force_upload', False))
            
            print(f"âœ… å…¬å¸ {company_code} è™•ç†å®Œæˆ")
        
        return processed_companies

    def analyze_quality_only(self, **kwargs):
        """ğŸ”§ v3.6.1 åªé€²è¡Œå“è³ªåˆ†æ"""
        print("\nğŸ“Š åŸ·è¡Œå“è³ªåˆ†æ v3.6.1...")
        
        md_files = self.md_scanner.scan_all_md_files()
        
        if not md_files:
            print("âŒ æ²’æœ‰æ‰¾åˆ° MD æª”æ¡ˆ")
            return {}
        
        processed_companies = []
        quality_stats = {
            'total_files': len(md_files),
            'processed_files': 0,
            'quality_distribution': {'excellent': 0, 'good': 0, 'partial': 0, 'insufficient': 0},
            'validation_stats': {'passed': 0, 'failed': 0, 'disabled': 0}
        }
        
        print(f"ğŸ“Š åˆ†æ {len(md_files)} å€‹ MD æª”æ¡ˆçš„å“è³ª")
        
        for md_file in md_files:
            try:
                if self.md_parser and self.quality_analyzer:
                    parsed_data = self.md_parser.parse_md_file(md_file)
                    quality_data = self.quality_analyzer.analyze(parsed_data)
                    
                    company_data = {**parsed_data, **quality_data}
                    processed_companies.append(company_data)
                    
                    # çµ±è¨ˆå“è³ªåˆ†å¸ƒ
                    quality_category = quality_data.get('quality_category', 'insufficient')
                    quality_stats['quality_distribution'][quality_category] += 1
                    
                    # çµ±è¨ˆé©—è­‰ç‹€æ…‹
                    if parsed_data.get('content_validation_passed', True):
                        quality_stats['validation_stats']['passed'] += 1
                    elif parsed_data.get('validation_result', {}).get('validation_method') == 'disabled':
                        quality_stats['validation_stats']['disabled'] += 1
                    else:
                        quality_stats['validation_stats']['failed'] += 1
                    
                    quality_stats['processed_files'] += 1
                
            except Exception as e:
                print(f"âŒ åˆ†æå¤±æ•—: {os.path.basename(md_file)} - {e}")
                continue
        
        # é¡¯ç¤ºå“è³ªåˆ†æçµæœ
        self._display_quality_analysis_results(quality_stats)
        
        # å„²å­˜å“è³ªåˆ†æçµæœ
        self._save_quality_analysis(quality_stats, processed_companies)
        
        print(f"âœ… å“è³ªåˆ†æå®Œæˆ: {quality_stats['processed_files']}/{quality_stats['total_files']} æˆåŠŸ")
        return quality_stats

    def analyze_keywords_only(self, min_usage=1, **kwargs):
        """ğŸ”§ v3.6.1 åªé€²è¡Œé—œéµå­—åˆ†æ"""
        print(f"\nğŸ“Š åŸ·è¡Œé—œéµå­—åˆ†æ v3.6.1 (æœ€å°ä½¿ç”¨æ¬¡æ•¸: {min_usage})...")
        
        if not self.keyword_analyzer:
            print("âŒ KeywordAnalyzer æœªè¼‰å…¥ï¼Œç„¡æ³•é€²è¡Œé—œéµå­—åˆ†æ")
            return {}
        
        md_files = self.md_scanner.scan_all_md_files()
        
        if not md_files:
            print("âŒ æ²’æœ‰æ‰¾åˆ° MD æª”æ¡ˆ")
            return {}
        
        print(f"ğŸ“Š åˆ†æ {len(md_files)} å€‹ MD æª”æ¡ˆçš„é—œéµå­—ä½¿ç”¨æƒ…æ³")
        
        # è™•ç†æª”æ¡ˆä»¥æå–é—œéµå­—
        processed_companies = []
        success_count = 0
        
        for md_file in md_files:
            try:
                file_info = self.md_scanner.get_file_info(md_file)
                
                if self.md_parser and self.quality_analyzer:
                    parsed_data = self.md_parser.parse_md_file(md_file)
                    quality_data = self.quality_analyzer.analyze(parsed_data)
                    
                    company_data = {
                        **parsed_data,
                        **quality_data,
                        'processed_at': datetime.now()
                    }
                else:
                    company_data = self._basic_process_md_file(md_file, file_info)
                
                processed_companies.append(company_data)
                success_count += 1
                
            except Exception as e:
                print(f"âŒ è™•ç†å¤±æ•—: {os.path.basename(md_file)} - {e}")
                continue
        
        # åŸ·è¡Œé—œéµå­—åˆ†æ
        keyword_analysis = self.keyword_analyzer.analyze_all_keywords(processed_companies)
        
        # éæ¿¾ä½ä½¿ç”¨ç‡é—œéµå­—
        if min_usage > 1:
            keyword_analysis = self.keyword_analyzer.filter_keywords_by_usage(
                keyword_analysis, min_usage
            )
        
        # å„²å­˜é—œéµå­—åˆ†æçµæœ
        self._save_keyword_analysis(keyword_analysis)
        self._display_keyword_summary(keyword_analysis, min_usage)
        
        print(f"âœ… é—œéµå­—åˆ†æå®Œæˆ: {success_count}/{len(md_files)} æˆåŠŸ")
        return keyword_analysis

    def analyze_watchlist_only(self, **kwargs):
        """ğŸ†• v3.6.1 åªé€²è¡Œè§€å¯Ÿåå–®åˆ†æ"""
        print("\nğŸ“Š åŸ·è¡Œè§€å¯Ÿåå–®åˆ†æ v3.6.1...")
        
        if not self.watchlist_analyzer:
            print("âŒ WatchlistAnalyzer æœªè¼‰å…¥ï¼Œç„¡æ³•é€²è¡Œè§€å¯Ÿåå–®åˆ†æ")
            return {}
        
        md_files = self.md_scanner.scan_all_md_files()
        
        if not md_files:
            print("âŒ æ²’æœ‰æ‰¾åˆ° MD æª”æ¡ˆ")
            return {}
        
        print(f"ğŸ“Š åˆ†æ {len(md_files)} å€‹ MD æª”æ¡ˆçš„è§€å¯Ÿåå–®è¦†è“‹æƒ…æ³")
        
        # è™•ç†æª”æ¡ˆ
        processed_companies = []
        success_count = 0
        
        for md_file in md_files:
            try:
                file_info = self.md_scanner.get_file_info(md_file)
                
                if self.md_parser and self.quality_analyzer:
                    parsed_data = self.md_parser.parse_md_file(md_file)
                    quality_data = self.quality_analyzer.analyze(parsed_data)
                    
                    company_data = {
                        **parsed_data,
                        **quality_data,
                        'processed_at': datetime.now()
                    }
                else:
                    company_data = self._basic_process_md_file(md_file, file_info)
                
                processed_companies.append(company_data)
                success_count += 1
                
            except Exception as e:
                print(f"âŒ è™•ç†å¤±æ•—: {os.path.basename(md_file)} - {e}")
                continue
        
        # åŸ·è¡Œè§€å¯Ÿåå–®åˆ†æ
        watchlist_analysis = self.watchlist_analyzer.analyze_watchlist_coverage(processed_companies)
        
        # å„²å­˜åˆ†æçµæœ
        self._save_watchlist_analysis(watchlist_analysis)
        self._display_watchlist_summary(watchlist_analysis)
        
        print(f"âœ… è§€å¯Ÿåå–®åˆ†æå®Œæˆ: {success_count}/{len(md_files)} æˆåŠŸ")
        return watchlist_analysis

    def generate_keyword_summary(self, upload_sheets=True, min_usage=1, **kwargs):
        """ğŸ”§ v3.6.1 ç”Ÿæˆé—œéµå­—çµ±è¨ˆå ±å‘Š"""
        print(f"\nğŸ“Š ç”Ÿæˆé—œéµå­—çµ±è¨ˆå ±å‘Š v3.6.1...")
        
        if not self.keyword_analyzer:
            print("âŒ KeywordAnalyzer æœªè¼‰å…¥ï¼Œç„¡æ³•ç”Ÿæˆé—œéµå­—å ±å‘Š")
            return {}, ""
        
        # å…ˆåŸ·è¡Œé—œéµå­—åˆ†æ
        keyword_analysis = self.analyze_keywords_only(min_usage=min_usage, **kwargs)
        
        if not keyword_analysis:
            print("âŒ é—œéµå­—åˆ†æå¤±æ•—")
            return {}, ""
        
        # ç”Ÿæˆé—œéµå­—å ±å‘Š
        if self.report_generator:
            keyword_summary = self.report_generator.generate_keyword_summary(keyword_analysis)
            
            # å„²å­˜ CSV
            csv_path = self.report_generator.save_keyword_summary(keyword_summary)
            
            # ä¸Šå‚³ (å¯é¸)
            if upload_sheets and self.sheets_uploader:
                try:
                    self.sheets_uploader._upload_keyword_summary(keyword_summary)
                    print("â˜ï¸ é—œéµå­—å ±å‘Šå·²ä¸Šå‚³åˆ° Google Sheets")
                except Exception as e:
                    print(f"âš ï¸ Google Sheets ä¸Šå‚³å¤±æ•—: {e}")
            
            # é¡¯ç¤ºçµ±è¨ˆæ‘˜è¦
            self._display_keyword_generation_summary(keyword_analysis, min_usage)
            
            return keyword_analysis, csv_path
        else:
            print("âŒ ReportGenerator æœªè¼‰å…¥ï¼Œç„¡æ³•ç”Ÿæˆå ±å‘Š")
            return keyword_analysis, ""

    def generate_watchlist_summary(self, upload_sheets=True, include_missing=False, **kwargs):
        """ğŸ†• v3.6.1 ç”Ÿæˆè§€å¯Ÿåå–®çµ±è¨ˆå ±å‘Š"""
        print(f"\nğŸ“Š ç”Ÿæˆè§€å¯Ÿåå–®çµ±è¨ˆå ±å‘Š v3.6.1...")
        
        if not self.watchlist_analyzer:
            print("âŒ WatchlistAnalyzer æœªè¼‰å…¥ï¼Œç„¡æ³•ç”Ÿæˆè§€å¯Ÿåå–®å ±å‘Š")
            return {}, ""
        
        # æƒææª”æ¡ˆ
        md_files = self.md_scanner.scan_all_md_files()
        print(f"ğŸ“ æƒæåˆ° {len(md_files)} å€‹ MD æª”æ¡ˆ")
        
        # è§£ææª”æ¡ˆ
        processed_companies = []
        for md_file in md_files:
            try:
                parsed_data = self.md_parser.parse_md_file(md_file) if self.md_parser else {}
                quality_data = self.quality_analyzer.analyze(parsed_data) if self.quality_analyzer else {}
                company_data = {**parsed_data, **quality_data}
                processed_companies.append(company_data)
            except Exception as e:
                print(f"âš ï¸ è™•ç†æª”æ¡ˆå¤±æ•—: {os.path.basename(md_file)} - {e}")
                continue
        
        print(f"ğŸ“Š æˆåŠŸè™•ç† {len(processed_companies)} å€‹æª”æ¡ˆ")
        
        # è§€å¯Ÿåå–®åˆ†æ
        watchlist_analysis = self.watchlist_analyzer.analyze_watchlist_coverage(processed_companies)
        
        # ç”Ÿæˆè§€å¯Ÿåå–®å ±å‘Š
        if self.report_generator:
            watchlist_summary = self.report_generator.generate_watchlist_summary(watchlist_analysis)
            
            # å¯é¸ï¼šåŒ…å«ç¼ºå¤±å…¬å¸è³‡è¨Š
            if include_missing:
                print("ğŸ“‹ åŒ…å«ç¼ºå¤±å…¬å¸è³‡è¨Š...")
                missing_companies = self.watchlist_analyzer.generate_missing_companies_report(processed_companies)
                watchlist_summary = self._append_missing_companies(watchlist_summary, missing_companies)
            
            # å„²å­˜ CSV
            csv_path = self.report_generator.save_watchlist_summary(watchlist_summary)
            
            # ä¸Šå‚³ (å¯é¸)
            if upload_sheets and self.sheets_uploader:
                try:
                    self.sheets_uploader._upload_watchlist_summary(watchlist_summary)
                    print("â˜ï¸ è§€å¯Ÿåå–®å ±å‘Šå·²ä¸Šå‚³åˆ° Google Sheets")
                except Exception as e:
                    print(f"âš ï¸ Google Sheets ä¸Šå‚³å¤±æ•—: {e}")
            
            # é¡¯ç¤ºçµ±è¨ˆæ‘˜è¦
            missing_count = len(missing_companies) if include_missing else 0
            self._display_watchlist_generation_summary(watchlist_analysis, missing_count)
            
            return watchlist_analysis, csv_path
        else:
            print("âŒ ReportGenerator æœªè¼‰å…¥ï¼Œç„¡æ³•ç”Ÿæˆå ±å‘Š")
            return watchlist_analysis, ""

    def show_stats(self, **kwargs):
        """ğŸ”§ v3.6.1 é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š"""
        print("\nğŸ“Š ç³»çµ±çµ±è¨ˆè³‡è¨Š v3.6.1")
        print("=" * 50)
        
        # MD æª”æ¡ˆçµ±è¨ˆ
        try:
            stats = self.md_scanner.get_stats()
            print(f"ğŸ“ MD æª”æ¡ˆçµ±è¨ˆ:")
            print(f"   ç¸½æª”æ¡ˆæ•¸: {stats['total_files']}")
            print(f"   æœ€è¿‘ 24h: {stats['recent_files_24h']}")
            print(f"   å…¬å¸æ•¸é‡: {stats['unique_companies']}")
            print(f"   ç¸½å¤§å°: {stats['total_size_mb']} MB")
            
            if stats['oldest_file']:
                print(f"   æœ€èˆŠæª”æ¡ˆ: {os.path.basename(stats['oldest_file'])}")
            if stats['newest_file']:
                print(f"   æœ€æ–°æª”æ¡ˆ: {os.path.basename(stats['newest_file'])}")
        except Exception as e:
            print(f"âŒ MD æª”æ¡ˆçµ±è¨ˆå¤±æ•—: {e}")
        
        # è§€å¯Ÿåå–®çµ±è¨ˆ
        if self.md_parser and self.md_parser.validation_enabled:
            watch_list_size = len(self.md_parser.watch_list_mapping)
            print(f"\nğŸ“‹ è§€å¯Ÿåå–®çµ±è¨ˆ:")
            print(f"   è§€å¯Ÿåå–®å…¬å¸æ•¸: {watch_list_size}")
            print(f"   é©—è­‰ç‹€æ…‹: å•Ÿç”¨")
        else:
            print(f"\nğŸ“‹ è§€å¯Ÿåå–®çµ±è¨ˆ:")
            print(f"   é©—è­‰ç‹€æ…‹: åœç”¨")
        
        # è§€å¯Ÿåå–®åˆ†æçµ±è¨ˆ (v3.6.1)
        if self.watchlist_analyzer and self.watchlist_analyzer.validation_enabled:
            watchlist_analysis_size = len(self.watchlist_analyzer.watchlist_mapping)
            print(f"\nğŸ“Š è§€å¯Ÿåå–®åˆ†æçµ±è¨ˆ (v3.6.1):")
            print(f"   åˆ†æç¯„åœå…¬å¸æ•¸: {watchlist_analysis_size}")
            print(f"   åˆ†æç‹€æ…‹: å•Ÿç”¨")
        else:
            print(f"\nğŸ“Š è§€å¯Ÿåå–®åˆ†æçµ±è¨ˆ (v3.6.1):")
            print(f"   åˆ†æç‹€æ…‹: åœç”¨")
        
        # æŒ‰å…¬å¸æª”æ¡ˆæ•¸é‡æ’åº
        try:
            if stats['companies_with_files']:
                sorted_companies = sorted(stats['companies_with_files'].items(), 
                                        key=lambda x: x[1], reverse=True)
                print(f"\nğŸ“Š æª”æ¡ˆæœ€å¤šçš„å‰ 10 å®¶å…¬å¸:")
                for company, count in sorted_companies[:10]:
                    print(f"   {company}: {count} å€‹æª”æ¡ˆ")
        except Exception as e:
            print(f"âš ï¸ å…¬å¸æª”æ¡ˆçµ±è¨ˆé¡¯ç¤ºå¤±æ•—: {e}")
        
        # çµ„ä»¶ç‹€æ…‹
        print(f"\nğŸ”§ çµ„ä»¶ç‹€æ…‹:")
        components = [
            ('MD Scanner', self.md_scanner is not None),
            ('MD Parser', self.md_parser is not None),
            ('Quality Analyzer', self.quality_analyzer is not None),
            ('Keyword Analyzer', self.keyword_analyzer is not None),
            ('Watchlist Analyzer', self.watchlist_analyzer is not None),
            ('Report Generator', self.report_generator is not None),
            ('Sheets Uploader', self.sheets_uploader is not None)
        ]
        
        for name, status in components:
            status_icon = "âœ…" if status else "âŒ"
            print(f"   {name}: {status_icon}")

    def validate_setup(self, **kwargs):
        """ğŸ”§ v3.6.1 é©—è­‰è™•ç†ç’°å¢ƒè¨­å®š - åŒ…å«è§€å¯Ÿåå–®åˆ†ææª¢æŸ¥"""
        print("\nğŸ”§ é©—è­‰è™•ç†ç’°å¢ƒè¨­å®š v3.6.1")
        print("=" * 50)
        
        validation_results = {}
        
        # æª¢æŸ¥ MD ç›®éŒ„
        try:
            md_files = self.md_scanner.scan_all_md_files()
            validation_results['md_scanner'] = {
                'status': 'âœ… æ­£å¸¸',
                'details': f'æ‰¾åˆ° {len(md_files)} å€‹ MD æª”æ¡ˆ'
            }
        except Exception as e:
            validation_results['md_scanner'] = {
                'status': 'âŒ éŒ¯èª¤',
                'details': str(e)
            }
        
        # æª¢æŸ¥è§€å¯Ÿåå–®è¼‰å…¥ç‹€æ…‹
        if self.md_parser:
            validation_enabled = self.md_parser.validation_enabled
            watch_list_size = len(self.md_parser.watch_list_mapping)
            
            if validation_enabled:
                validation_results['watch_list'] = {
                    'status': 'âœ… å·²è¼‰å…¥',
                    'details': f'è§€å¯Ÿåå–®åŒ…å« {watch_list_size} å®¶å…¬å¸'
                }
            else:
                validation_results['watch_list'] = {
                    'status': 'âš ï¸ æœªè¼‰å…¥',
                    'details': 'è§€å¯Ÿåå–®æª”æ¡ˆç„¡æ³•è¼‰å…¥æˆ–ç‚ºç©ºï¼Œé©—è­‰åŠŸèƒ½å·²åœç”¨'
                }
        else:
            validation_results['watch_list'] = {
                'status': 'âŒ ç„¡æ³•æª¢æŸ¥',
                'details': 'MD Parser æœªè¼‰å…¥'
            }
        
        # æª¢æŸ¥è§€å¯Ÿåå–®åˆ†æå™¨
        if self.watchlist_analyzer:
            watchlist_analysis_enabled = self.watchlist_analyzer.validation_enabled
            watchlist_size = len(self.watchlist_analyzer.watchlist_mapping)
            
            if watchlist_analysis_enabled:
                validation_results['watchlist_analyzer'] = {
                    'status': 'âœ… å·²è¼‰å…¥',
                    'details': f'è§€å¯Ÿåå–®åˆ†æå™¨åŒ…å« {watchlist_size} å®¶å…¬å¸'
                }
            else:
                validation_results['watchlist_analyzer'] = {
                    'status': 'âš ï¸ æœªè¼‰å…¥',
                    'details': 'è§€å¯Ÿåå–®åˆ†æå™¨ç„¡æ³•è¼‰å…¥è§€å¯Ÿåå–®æª”æ¡ˆ'
                }
        else:
            validation_results['watchlist_analyzer'] = {
                'status': 'âŒ æœªè¼‰å…¥',
                'details': 'WatchlistAnalyzer æ¨¡çµ„æœªè¼‰å…¥'
            }
        
        # æª¢æŸ¥å…¶ä»–çµ„ä»¶
        components = [
            ('md_parser', self.md_parser),
            ('quality_analyzer', self.quality_analyzer),
            ('keyword_analyzer', self.keyword_analyzer),
            ('report_generator', self.report_generator),
            ('sheets_uploader', self.sheets_uploader)
        ]
        
        for name, component in components:
            if component:
                try:
                    if hasattr(component, 'test_connection'):
                        result = component.test_connection()
                        validation_results[name] = {
                            'status': 'âœ… æ­£å¸¸' if result else 'âš ï¸ è­¦å‘Š',
                            'details': 'é€£ç·šæ¸¬è©¦æˆåŠŸ' if result else 'é€£ç·šæ¸¬è©¦å¤±æ•—'
                        }
                    else:
                        validation_results[name] = {
                            'status': 'âœ… å·²è¼‰å…¥',
                            'details': 'æ¨¡çµ„å·²æˆåŠŸè¼‰å…¥'
                        }
                except Exception as e:
                    validation_results[name] = {
                        'status': 'âŒ éŒ¯èª¤',
                        'details': str(e)
                    }
            else:
                validation_results[name] = {
                    'status': 'âš ï¸ æœªè¼‰å…¥',
                    'details': 'æ¨¡çµ„æœªå®‰è£æˆ–è¼‰å…¥å¤±æ•—'
                }
        
        # é¡¯ç¤ºçµæœ
        for component, result in validation_results.items():
            print(f"{component:18}: {result['status']} - {result['details']}")
        
        return validation_results

    # ç§æœ‰è¼”åŠ©æ–¹æ³•
    def _process_md_file_list_v361(self, md_files, **kwargs):
        """ğŸ”§ v3.6.1 è™•ç† MD æª”æ¡ˆæ¸…å–® - åŒ…å«è§€å¯Ÿåå–®åˆ†æçµ±è¨ˆ"""
        processed_companies = []
        validation_stats = {
            'total_processed': 0,
            'validation_passed': 0,
            'validation_failed': 0,
            'validation_disabled': 0,
            'not_in_watchlist': 0,
            'name_mismatch': 0,
            'invalid_format': 0,
            'other_errors': 0
        }
        
        print(f"\nğŸ“„ é–‹å§‹è™•ç† {len(md_files)} å€‹ MD æª”æ¡ˆ...")
        
        for i, md_file in enumerate(md_files, 1):
            try:
                print(f"ğŸ“„ è™•ç† {i}/{len(md_files)}: {os.path.basename(md_file)}")
                
                file_info = self.md_scanner.get_file_info(md_file)
                validation_stats['total_processed'] += 1
                
                if self.md_parser:
                    parsed_data = self.md_parser.parse_md_file(md_file)
                    
                    # è©³ç´°çš„é©—è­‰ç‹€æ…‹åˆ†æ
                    validation_result = parsed_data.get('validation_result', {})
                    validation_status = validation_result.get('overall_status', 'unknown')
                    validation_method = validation_result.get('validation_method', 'unknown')
                    validation_errors = parsed_data.get('validation_errors', [])
                    
                    # çµ±è¨ˆé©—è­‰ç‹€æ…‹
                    if validation_method == 'disabled':
                        validation_stats['validation_disabled'] += 1
                        status_icon = "âš ï¸"
                        status_msg = "é©—è­‰åœç”¨ (è§€å¯Ÿåå–®æœªè¼‰å…¥)"
                        validation_passed = True
                        
                    elif validation_status == 'valid':
                        validation_stats['validation_passed'] += 1
                        status_icon = "âœ…"
                        status_msg = "é©—è­‰é€šé"
                        validation_passed = True
                        
                    elif validation_status == 'error':
                        validation_stats['validation_failed'] += 1
                        status_icon = "âŒ"
                        validation_passed = False
                        
                        # åˆ†æå¤±æ•—åŸå› 
                        main_error = validation_errors[0] if validation_errors else "æœªçŸ¥éŒ¯èª¤"
                        main_error_str = str(main_error)
                        
                        if "ä¸åœ¨è§€å¯Ÿåå–®ä¸­" in main_error_str:
                            validation_stats['not_in_watchlist'] += 1
                            status_msg = "ä¸åœ¨è§€å¯Ÿåå–®"
                        elif "å…¬å¸åç¨±ä¸ç¬¦è§€å¯Ÿåå–®" in main_error_str or "è§€å¯Ÿåå–®é¡¯ç¤ºæ‡‰ç‚º" in main_error_str:
                            validation_stats['name_mismatch'] += 1
                            status_msg = "è§€å¯Ÿåå–®åç¨±ä¸ç¬¦"
                        elif "å…¬å¸ä»£è™Ÿæ ¼å¼ç„¡æ•ˆ" in main_error_str or "åƒæ•¸éŒ¯èª¤" in main_error_str:
                            validation_stats['invalid_format'] += 1
                            status_msg = "æ ¼å¼ç„¡æ•ˆ"
                        else:
                            validation_stats['other_errors'] += 1
                            status_msg = "å…¶ä»–é©—è­‰éŒ¯èª¤"
                    
                    else:
                        validation_stats['validation_failed'] += 1
                        status_icon = "â“"
                        status_msg = "æœªçŸ¥é©—è­‰ç‹€æ…‹"
                        validation_passed = False
                    
                    # æ›´æ–° parsed_data çš„é©—è­‰ç‹€æ…‹
                    parsed_data['content_validation_passed'] = validation_passed
                    
                    if self.quality_analyzer:
                        quality_data = self.quality_analyzer.analyze(parsed_data)
                        
                        company_data = {
                            **parsed_data,
                            'quality_score': quality_data.get('quality_score', 0),
                            'quality_status': quality_data.get('quality_status', 'ğŸ”´ ä¸è¶³'),
                            'quality_category': quality_data.get('quality_category', 'insufficient'),
                            'processed_at': datetime.now()
                        }
                    else:
                        company_data = {
                            **parsed_data,
                            'quality_score': parsed_data.get('data_richness_score', 0),
                            'quality_status': self._get_quality_status(parsed_data.get('data_richness_score', 0)),
                            'quality_category': 'partial',
                            'processed_at': datetime.now()
                        }
                else:
                    company_data = self._basic_process_md_file(md_file, file_info)
                    validation_stats['validation_passed'] += 1
                    status_icon = "âœ…"
                    status_msg = "åŸºæœ¬è™•ç†"
                
                processed_companies.append(company_data)
                
                # è©³ç´°çš„è™•ç†çµæœé¡¯ç¤º
                company_name = company_data.get('company_name', 'Unknown')
                company_code = company_data.get('company_code', 'Unknown')
                quality_score = company_data.get('quality_score', 0)
                quality_status = company_data.get('quality_status', 'ğŸ”´ ä¸è¶³')
                
                print(f"   {status_icon} {company_name} ({company_code}) - å“è³ª: {quality_score:.1f} {quality_status} - {status_msg}")
                
                # å¦‚æœé©—è­‰å¤±æ•—ï¼Œé¡¯ç¤ºè©³ç´°åŸå› 
                if not validation_passed and validation_errors:
                    error_preview = str(validation_errors[0])[:80]
                    print(f"      ğŸ” é©—è­‰å•é¡Œ: {error_preview}...")
                    
                    if "ä¸åœ¨è§€å¯Ÿåå–®" in error_preview:
                        print(f"      ğŸ’¡ æ­¤å…¬å¸å°‡è¢«æ’é™¤åœ¨æœ€çµ‚å ±å‘Šä¹‹å¤–")
                
            except Exception as e:
                print(f"   âŒ è™•ç†å¤±æ•—: {os.path.basename(md_file)} - {e}")
                continue
        
        # è™•ç†å®Œæˆå¾Œé¡¯ç¤ºè©³ç´°çµ±è¨ˆ
        self._display_processing_statistics_v361(validation_stats)
        
        return processed_companies

    def _generate_and_upload_reports_v361(self, processed_companies, upload_sheets=True, force_upload=False):
        """ğŸ†• v3.6.1 ç”Ÿæˆå ±å‘Šä¸¦ä¸Šå‚³ - åŒ…å«è§€å¯Ÿåå–®å ±å‘Š"""
        try:
            if self.report_generator:
                print("ğŸ“Š ä½¿ç”¨ ReportGenerator v3.6.1 ç”Ÿæˆå ±å‘Š...")
                
                # ç”Ÿæˆæ¨™æº–å ±å‘Š
                portfolio_summary = self.report_generator.generate_portfolio_summary(processed_companies)
                detailed_report = self.report_generator.generate_detailed_report(processed_companies)
                
                # ç”Ÿæˆé—œéµå­—å ±å‘Š
                keyword_summary = None
                if self.keyword_analyzer:
                    try:
                        keyword_analysis = self.keyword_analyzer.analyze_all_keywords(processed_companies)
                        keyword_summary = self.report_generator.generate_keyword_summary(keyword_analysis)
                        print("ğŸ“Š é—œéµå­—å ±å‘Šå·²ç”Ÿæˆ")
                    except Exception as e:
                        print(f"âš ï¸ é—œéµå­—å ±å‘Šç”Ÿæˆå¤±æ•—: {e}")
                
                # ç”Ÿæˆè§€å¯Ÿåå–®å ±å‘Š
                watchlist_summary = None
                if self.watchlist_analyzer:
                    try:
                        watchlist_analysis = self.watchlist_analyzer.analyze_watchlist_coverage(processed_companies)
                        watchlist_summary = self.report_generator.generate_watchlist_summary(watchlist_analysis)
                        print("ğŸ“Š è§€å¯Ÿåå–®å ±å‘Šå·²ç”Ÿæˆ")
                    except Exception as e:
                        print(f"âš ï¸ è§€å¯Ÿåå–®å ±å‘Šç”Ÿæˆå¤±æ•—: {e}")
                
                # å„²å­˜å ±å‘Š
                saved_files = self.report_generator.save_all_reports(
                    portfolio_summary, detailed_report, keyword_summary, watchlist_summary
                )
                
                if saved_files:
                    print("ğŸ“ å ±å‘Šå·²æˆåŠŸå„²å­˜:")
                    for report_type, file_path in saved_files.items():
                        if 'latest' in report_type:
                            print(f"   âœ… {report_type}: {file_path}")
                
                # ç”Ÿæˆçµ±è¨ˆå ±å‘Š
                try:
                    statistics = self.report_generator.generate_statistics_report(processed_companies)
                    stats_file = self.report_generator.save_statistics_report(statistics)
                    if stats_file:
                        print(f"   ğŸ“Š çµ±è¨ˆå ±å‘Š: {stats_file}")
                except Exception as e:
                    print(f"   âš ï¸ çµ±è¨ˆå ±å‘Šç”Ÿæˆå¤±æ•—: {e}")
                
                # ä¸Šå‚³åˆ° Google Sheets
                if upload_sheets and self.sheets_uploader:
                    try:
                        print("â˜ï¸ ä¸Šå‚³åˆ° Google Sheets v3.6.1...")
                        
                        if force_upload:
                            print("âš ï¸ å¼·åˆ¶ä¸Šå‚³æ¨¡å¼ï¼šå¿½ç•¥é©—è­‰éŒ¯èª¤")
                        
                        success = self.sheets_uploader.upload_all_reports(
                            portfolio_summary, detailed_report, keyword_summary, watchlist_summary
                        )
                        
                        if success:
                            print("   âœ… Google Sheets ä¸Šå‚³æˆåŠŸ (åŒ…å«è§€å¯Ÿåå–®å·¥ä½œè¡¨)")
                        else:
                            print("   âŒ Google Sheets ä¸Šå‚³å¤±æ•—")
                    except Exception as e:
                        print(f"   âŒ Google Sheets ä¸Šå‚³éŒ¯èª¤: {e}")
                
            else:
                print("âŒ ReportGenerator æœªè¼‰å…¥ï¼Œç„¡æ³•ç”Ÿæˆæ¨™æº–å ±å‘Š")
                self._generate_minimal_reports(processed_companies)
        
        except Exception as e:
            print(f"âŒ å ±å‘Šç”Ÿæˆæˆ–ä¸Šå‚³å¤±æ•—: {e}")

    # è¼”åŠ©é¡¯ç¤ºå’Œå„²å­˜æ–¹æ³•
    def _display_quality_analysis_results(self, quality_stats):
        """é¡¯ç¤ºå“è³ªåˆ†æçµæœ"""
        print(f"\nğŸ“Š å“è³ªåˆ†æçµæœ:")
        print(f"=" * 40)
        print(f"ğŸ“ è™•ç†çµ±è¨ˆ: {quality_stats['processed_files']}/{quality_stats['total_files']}")
        
        print(f"\nğŸ“Š å“è³ªåˆ†å¸ƒ:")
        dist = quality_stats['quality_distribution']
        for category, count in dist.items():
            category_name = {
                'excellent': 'ğŸŸ¢ å„ªç§€ (9-10åˆ†)',
                'good': 'ğŸŸ¡ è‰¯å¥½ (8-9åˆ†)',
                'partial': 'ğŸŸ  éƒ¨åˆ† (3-8åˆ†)',
                'insufficient': 'ğŸ”´ ä¸è¶³ (0-3åˆ†)'
            }.get(category, category)
            print(f"   {category_name}: {count}")
        
        print(f"\nğŸ“Š é©—è­‰çµ±è¨ˆ:")
        validation = quality_stats['validation_stats']
        print(f"   âœ… é©—è­‰é€šé: {validation['passed']}")
        print(f"   âŒ é©—è­‰å¤±æ•—: {validation['failed']}")
        print(f"   âš ï¸ é©—è­‰åœç”¨: {validation['disabled']}")

    def _save_quality_analysis(self, quality_stats, processed_companies):
        """å„²å­˜å“è³ªåˆ†æçµæœ"""
        output_file = "data/reports/quality_analysis.json"
        
        analysis_data = {
            'timestamp': datetime.now().isoformat(),
            'version': '3.6.1',
            'analysis_type': 'quality_only',
            'statistics': quality_stats,
            'company_count': len(processed_companies)
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ“ å“è³ªåˆ†æçµæœå·²å„²å­˜: {output_file}")

    def _display_keyword_summary(self, keyword_analysis, min_usage):
        """é¡¯ç¤ºé—œéµå­—åˆ†ææ‘˜è¦"""
        if 'error' in keyword_analysis:
            print(f"âŒ é—œéµå­—åˆ†æå¤±æ•—: {keyword_analysis['error']}")
            return
        
        keyword_stats = keyword_analysis.get('keyword_stats', {})
        
        print(f"\nğŸ“Š é—œéµå­—åˆ†ææ‘˜è¦ (æœ€å°ä½¿ç”¨æ¬¡æ•¸: {min_usage}):")
        print(f"æœ‰æ•ˆé—œéµå­—æ•¸é‡: {len(keyword_stats)}")
        
        # é¡¯ç¤ºæ•ˆæœæœ€å¥½çš„é—œéµå­—
        top_keywords = sorted(keyword_stats.items(), 
                            key=lambda x: x[1]['avg_quality_score'], reverse=True)[:10]
        
        print(f"æ•ˆæœæœ€å¥½çš„é—œéµå­—:")
        for keyword, stats in top_keywords:
            print(f"  {keyword}: å¹³å‡åˆ†æ•¸ {stats['avg_quality_score']:.1f} (ä½¿ç”¨ {stats['usage_count']} æ¬¡)")

    def _save_keyword_analysis(self, keyword_analysis):
        """å„²å­˜é—œéµå­—åˆ†æçµæœ"""
        output_file = "data/reports/keyword_analysis.json"
        
        analysis_data = {
            'timestamp': datetime.now().isoformat(),
            'version': '3.6.1',
            'analysis_type': 'keyword_analysis',
            'results': keyword_analysis
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ“ é—œéµå­—åˆ†æçµæœå·²å„²å­˜: {output_file}")

    def _save_watchlist_analysis(self, watchlist_analysis):
        """å„²å­˜è§€å¯Ÿåå–®åˆ†æçµæœ"""
        output_file = "data/reports/watchlist_analysis.json"
        
        analysis_data = {
            'timestamp': datetime.now().isoformat(),
            'version': '3.6.1',
            'analysis_type': 'watchlist_coverage',
            'results': watchlist_analysis
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ“ è§€å¯Ÿåå–®åˆ†æçµæœå·²å„²å­˜: {output_file}")

    def _display_watchlist_summary(self, watchlist_analysis):
        """é¡¯ç¤ºè§€å¯Ÿåå–®åˆ†ææ‘˜è¦"""
        if 'error' in watchlist_analysis:
            print(f"âŒ è§€å¯Ÿåå–®åˆ†æå¤±æ•—: {watchlist_analysis['error']}")
            return
        
        total_companies = watchlist_analysis['total_watchlist_companies']
        companies_with_files = watchlist_analysis['companies_with_md_files']
        coverage_rate = watchlist_analysis['coverage_rate']
        success_rate = watchlist_analysis['success_rate']
        
        print("\nğŸ“Š è§€å¯Ÿåå–®åˆ†ææ‘˜è¦:")
        print(f"è§€å¯Ÿåå–®ç¸½å…¬å¸æ•¸: {total_companies}")
        print(f"æœ‰MDæª”æ¡ˆå…¬å¸æ•¸: {companies_with_files}")
        print(f"è¦†è“‹ç‡: {coverage_rate}%")
        print(f"æˆåŠŸè™•ç†ç‡: {success_rate}%")
        
        # é¡¯ç¤ºç‹€æ…‹åˆ†ä½ˆ
        status_summary = watchlist_analysis['company_status_summary']
        print("å…¬å¸ç‹€æ…‹åˆ†ä½ˆ:")
        for status, count in status_summary.items():
            status_name = {
                'processed': 'å·²è™•ç†',
                'not_found': 'æœªæ‰¾åˆ°MDæª”æ¡ˆ',
                'validation_failed': 'é©—è­‰å¤±æ•—',
                'low_quality': 'å“è³ªéä½',
                'multiple_files': 'å¤šå€‹æª”æ¡ˆ'
            }.get(status, status)
            print(f"  {status_name}: {count} å®¶")

    def _append_missing_companies(self, watchlist_summary, missing_companies):
        """å°‡ç¼ºå¤±å…¬å¸è³‡è¨Šé™„åŠ åˆ°è§€å¯Ÿåå–®å ±å‘Š"""
        import pandas as pd
        
        # æº–å‚™ç¼ºå¤±å…¬å¸è³‡æ–™
        missing_data = []
        for company in missing_companies:
            missing_data.append({
                'å…¬å¸ä»£è™Ÿ': company['company_code'],
                'å…¬å¸åç¨±': company['company_name'],
                'MDæª”æ¡ˆæ•¸é‡': 0,
                'è™•ç†ç‹€æ…‹': 'âŒ ç¼ºå¤±MDæª”æ¡ˆ',
                'å¹³å‡å“è³ªè©•åˆ†': 0.0,
                'æœ€é«˜å“è³ªè©•åˆ†': 0.0,
                'æœå°‹é—œéµå­—æ•¸é‡': len(company.get('suggested_keywords', [])),
                'ä¸»è¦é—œéµå­—': ', '.join(company.get('suggested_keywords', [])[:3]),
                'é—œéµå­—å¹³å‡å“è³ª': 0.0,
                'æœ€æ–°æª”æ¡ˆæ—¥æœŸ': '',
                'é©—è­‰ç‹€æ…‹': 'âŒ ç„¡è³‡æ–™',
                'æ›´æ–°æ—¥æœŸ': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            })
        
        missing_df = pd.DataFrame(missing_data)
        
        # åˆä½µç¾æœ‰å ±å‘Šå’Œç¼ºå¤±å…¬å¸è³‡æ–™
        if not missing_df.empty:
            # ç¢ºä¿æ¬„ä½ä¸€è‡´
            common_columns = list(set(watchlist_summary.columns) & set(missing_df.columns))
            combined_df = pd.concat([
                watchlist_summary[common_columns],
                missing_df[common_columns]
            ], ignore_index=True)
            
            print(f"ğŸ“‹ å·²é™„åŠ  {len(missing_companies)} å®¶ç¼ºå¤±å…¬å¸åˆ°è§€å¯Ÿåå–®å ±å‘Š")
            return combined_df
        
        return watchlist_summary

    def _display_keyword_generation_summary(self, keyword_analysis, min_usage):
        """é¡¯ç¤ºé—œéµå­—å ±å‘Šç”Ÿæˆæ‘˜è¦"""
        keyword_stats = keyword_analysis.get('keyword_stats', {})
        
        print(f"\nğŸ“Š é—œéµå­—å ±å‘Šç”Ÿæˆæ‘˜è¦:")
        print(f"=" * 40)
        print(f"ğŸ“Š é—œéµå­—çµ±è¨ˆ (æœ€å°ä½¿ç”¨: {min_usage} æ¬¡):")
        print(f"   æœ‰æ•ˆé—œéµå­—: {len(keyword_stats)}")
        
        if keyword_stats:
            # çµ±è¨ˆåˆ†é¡
            categories = {}
            for keyword, stats in keyword_stats.items():
                category = stats.get('category', 'other')
                categories[category] = categories.get(category, 0) + 1
            
            print(f"ğŸ“Š é—œéµå­—åˆ†é¡:")
            for category, count in categories.items():
                print(f"   {category}: {count}")

    def _display_watchlist_generation_summary(self, watchlist_analysis, missing_count=0):
        """é¡¯ç¤ºè§€å¯Ÿåå–®å ±å‘Šç”Ÿæˆæ‘˜è¦"""
        total_companies = watchlist_analysis.get('total_watchlist_companies', 0)
        companies_with_files = watchlist_analysis.get('companies_with_md_files', 0)
        companies_processed = watchlist_analysis.get('companies_processed_successfully', 0)
        coverage_rate = watchlist_analysis.get('coverage_rate', 0)
        success_rate = watchlist_analysis.get('success_rate', 0)
        
        print(f"\nğŸ“Š è§€å¯Ÿåå–®å ±å‘Šç”Ÿæˆæ‘˜è¦:")
        print(f"=" * 40)
        print(f"ğŸ“‹ è§€å¯Ÿåå–®ç¸½å…¬å¸æ•¸: {total_companies}")
        print(f"ğŸ“ æœ‰MDæª”æ¡ˆå…¬å¸æ•¸: {companies_with_files}")
        print(f"âœ… æˆåŠŸè™•ç†å…¬å¸æ•¸: {companies_processed}")
        print(f"ğŸ“Š è¦†è“‹ç‡: {coverage_rate}%")
        print(f"ğŸ¯ æˆåŠŸç‡: {success_rate}%")
        
        if missing_count > 0:
            print(f"âŒ ç¼ºå¤±å…¬å¸æ•¸: {missing_count}")

    def _ensure_output_directories(self):
        """ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨"""
        directories = [
            "data/reports",
            "data/quarantine",
            "data/quarantine/watch_list_issues",
            "logs/process"
        ]
        
        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)

    def _display_processing_statistics_v361(self, validation_stats: Dict):
        """é¡¯ç¤ºè™•ç†çµ±è¨ˆè³‡è¨Š"""
        total = validation_stats['total_processed']
        passed = validation_stats['validation_passed']
        failed = validation_stats['validation_failed']
        disabled = validation_stats['validation_disabled']
        
        print(f"\nğŸ“Š è™•ç†çµ±è¨ˆæ‘˜è¦ v3.6.1:")
        print(f"=" * 40)
        print(f"ğŸ“ ç¸½è™•ç†æª”æ¡ˆ: {total}")
        print(f"âœ… é©—è­‰é€šé: {passed} ({passed/total*100:.1f}%)")
        print(f"âŒ é©—è­‰å¤±æ•—: {failed} ({failed/total*100:.1f}%)")
        print(f"âš ï¸ é©—è­‰åœç”¨: {disabled} ({disabled/total*100:.1f}%)")
        
        if failed > 0:
            print(f"\nâŒ é©—è­‰å¤±æ•—è©³ç´°åˆ†é¡:")
            not_in_watchlist = validation_stats['not_in_watchlist']
            name_mismatch = validation_stats['name_mismatch']
            invalid_format = validation_stats['invalid_format']
            other_errors = validation_stats['other_errors']
            
            if not_in_watchlist > 0:
                print(f"   ğŸš« ä¸åœ¨è§€å¯Ÿåå–®: {not_in_watchlist} å€‹")
            if name_mismatch > 0:
                print(f"   ğŸ“ è§€å¯Ÿåå–®åç¨±ä¸ç¬¦: {name_mismatch} å€‹")
            if invalid_format > 0:
                print(f"   ğŸ“‹ æ ¼å¼ç„¡æ•ˆ: {invalid_format} å€‹")
            if other_errors > 0:
                print(f"   âš ï¸ å…¶ä»–éŒ¯èª¤: {other_errors} å€‹")
            
            print(f"\nğŸ’¡ é€™äº›é©—è­‰å¤±æ•—çš„å…¬å¸å°‡ä¸æœƒå‡ºç¾åœ¨æœ€çµ‚å ±å‘Šä¸­")

    def _display_processing_validation_summary_v361(self, processed_companies: List):
        """é¡¯ç¤ºè™•ç†éç¨‹ä¸­çš„é©—è­‰æ‘˜è¦"""
        validation_passed = sum(1 for c in processed_companies if c.get('content_validation_passed', True))
        validation_failed = len(processed_companies) - validation_passed
        validation_disabled = sum(1 for c in processed_companies 
                                if c.get('validation_result', {}).get('validation_method') == 'disabled')
        
        if validation_failed > 0 or validation_disabled > 0:
            print(f"\nâš ï¸ æœ€çµ‚é©—è­‰æ‘˜è¦ v3.6.1:")
            print(f"âœ… é©—è­‰é€šé: {validation_passed}")
            print(f"âŒ é©—è­‰å¤±æ•—: {validation_failed}")
            print(f"âš ï¸ é©—è­‰åœç”¨: {validation_disabled}")

    def _basic_process_md_file(self, md_file, file_info):
        """åŸºæœ¬çš„ MD æª”æ¡ˆè™•ç†ï¼ˆç•¶å…¶ä»–æ¨¡çµ„ä¸å¯ç”¨æ™‚ï¼‰"""
        with open(md_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        basic_score = self._calculate_basic_quality_score(file_info)
        
        company_data = {
            **file_info,
            'content': content,
            'content_length': len(content),
            'quality_score': basic_score,
            'quality_status': self._get_quality_status(basic_score),
            'quality_category': 'partial',
            'processed_at': datetime.now(),
            'processing_method': 'basic',
            'content_validation_passed': True,
            'validation_errors': [],
            'validation_warnings': []
        }
        
        return company_data

    def _calculate_basic_quality_score(self, file_info):
        """åŸºæ–¼æª”æ¡ˆè³‡è¨Šè¨ˆç®—åŸºæœ¬å“è³ªè©•åˆ†"""
        score = 0
        
        file_size = file_info.get('file_size', 0)
        if file_size > 5000:
            score += 3
        elif file_size > 2000:
            score += 2
        elif file_size > 500:
            score += 1
        
        if file_info.get('company_code', 'Unknown') != 'Unknown':
            score += 2
        if file_info.get('company_name', 'Unknown') != 'Unknown':
            score += 2
        
        source = file_info.get('data_source', 'Unknown').lower()
        if 'factset' in source:
            score += 3
        elif source in ['yahoo', 'reuters', 'bloomberg']:
            score += 2
        elif source != 'unknown':
            score += 1
        
        return min(10, max(0, score))

    def _get_quality_status(self, score):
        """æ ¹æ“šè©•åˆ†å–å¾—å“è³ªç‹€æ…‹"""
        if score >= 9:
            return "ğŸŸ¢ å®Œæ•´"
        elif score >= 8:
            return "ğŸŸ¡ è‰¯å¥½"
        elif score >= 3:
            return "ğŸŸ  éƒ¨åˆ†"
        else:
            return "ğŸ”´ ä¸è¶³"

    def _generate_minimal_reports(self, processed_companies):
        """ç”Ÿæˆæœ€å°åŒ–å ±å‘Šï¼ˆç•¶ ReportGenerator ä¸å¯ç”¨æ™‚ï¼‰"""
        import pandas as pd
        
        print("ğŸ“„ ç”Ÿæˆæœ€å°åŒ–å ±å‘Š...")
        
        summary_data = []
        for company in processed_companies:
            summary_data.append({
                'ä»£è™Ÿ': company.get('company_code', ''),
                'åç¨±': company.get('company_name', ''),
                'å“è³ªè©•åˆ†': company.get('quality_score', 0),
                'ç‹€æ…‹': company.get('quality_status', ''),
                'é©—è­‰é€šé': company.get('content_validation_passed', True),
                'è™•ç†æ™‚é–“': company.get('processed_at', '')
            })
        
        df = pd.DataFrame(summary_data)
        emergency_file = "data/reports/emergency_summary.csv"
        df.to_csv(emergency_file, index=False, encoding='utf-8-sig')
        print(f"ğŸ“ ç·Šæ€¥å ±å‘Šå·²å„²å­˜: {emergency_file}")


def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    parser = argparse.ArgumentParser(description='FactSet è™•ç†ç³»çµ± v3.6.1 (å®Œæ•´å¯¦ç¾)')
    parser.add_argument('command', choices=[
        'process',            # è™•ç†æ‰€æœ‰ MD æª”æ¡ˆ
        'process-recent',     # è™•ç†æœ€è¿‘çš„ MD æª”æ¡ˆ
        'process-single',     # è™•ç†å–®ä¸€å…¬å¸
        'analyze-quality',    # å“è³ªåˆ†æ
        'analyze-keywords',   # é—œéµå­—åˆ†æ
        'analyze-watchlist',  # è§€å¯Ÿåå–®åˆ†æ
        'keyword-summary',    # é—œéµå­—çµ±è¨ˆå ±å‘Š
        'watchlist-summary',  # è§€å¯Ÿåå–®çµ±è¨ˆå ±å‘Š
        'stats',             # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
        'validate'           # é©—è­‰ç’°å¢ƒè¨­å®š
    ])
    parser.add_argument('--company', help='å–®ä¸€å…¬å¸ä»£è™Ÿ (ç”¨æ–¼ process-single)')
    parser.add_argument('--hours', type=int, default=24, help='æœ€è¿‘å°æ™‚æ•¸ (ç”¨æ–¼ process-recent)')
    parser.add_argument('--no-upload', action='store_true', help='ä¸ä¸Šå‚³åˆ° Google Sheets')
    parser.add_argument('--force-upload', action='store_true', help='å¼·åˆ¶ä¸Šå‚³ï¼Œå¿½ç•¥é©—è­‰éŒ¯èª¤')
    parser.add_argument('--min-usage', type=int, default=1, help='é—œéµå­—æœ€å°ä½¿ç”¨æ¬¡æ•¸')
    parser.add_argument('--include-missing', action='store_true', help='åŒ…å«ç¼ºå¤±å…¬å¸è³‡è¨Š (ç”¨æ–¼ watchlist-summary)')
    parser.add_argument('--dry-run', action='store_true', help='é è¦½æ¨¡å¼ï¼Œä¸å¯¦éš›åŸ·è¡Œ')
    
    args = parser.parse_args()
    
    # å»ºç«‹ CLI å¯¦ä¾‹
    try:
        cli = ProcessCLI()
    except Exception as e:
        print(f"âŒ ProcessCLI åˆå§‹åŒ–å¤±æ•—: {e}")
        sys.exit(1)
    
    # é è¦½æ¨¡å¼
    if args.dry_run:
        print("ğŸ” é è¦½æ¨¡å¼ï¼šé¡¯ç¤ºå°‡è¦åŸ·è¡Œçš„æ“ä½œ")
        print(f"å‘½ä»¤: {args.command}")
        print(f"åƒæ•¸: {vars(args)}")
        return
    
    # åŸ·è¡Œå°æ‡‰å‘½ä»¤
    try:
        if args.command == 'process':
            cli.process_all_md_files(upload_sheets=not args.no_upload, force_upload=args.force_upload)
        
        elif args.command == 'process-recent':
            cli.process_recent_files(hours=args.hours, upload_sheets=not args.no_upload, force_upload=args.force_upload)
            
        elif args.command == 'process-single':
            if not args.company:
                print("âŒ è«‹æä¾› --company åƒæ•¸")
                sys.exit(1)
            cli.process_single_company(args.company, upload_sheets=not args.no_upload, force_upload=args.force_upload)
        
        elif args.command == 'analyze-quality':
            cli.analyze_quality_only()
        
        elif args.command == 'analyze-keywords':
            cli.analyze_keywords_only(min_usage=args.min_usage)
        
        elif args.command == 'analyze-watchlist':
            cli.analyze_watchlist_only()
        
        elif args.command == 'keyword-summary':
            cli.generate_keyword_summary(upload_sheets=not args.no_upload, min_usage=args.min_usage)
        
        elif args.command == 'watchlist-summary':
            cli.generate_watchlist_summary(upload_sheets=not args.no_upload, include_missing=args.include_missing)
        
        elif args.command == 'stats':
            cli.show_stats()
        
        elif args.command == 'validate':
            cli.validate_setup()
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ä½¿ç”¨è€…ä¸­æ–·æ“ä½œ")
        sys.exit(0)
    except Exception as e:
        print(f"âŒ åŸ·è¡ŒéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()