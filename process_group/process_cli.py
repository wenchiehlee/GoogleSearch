#!/usr/bin/env python3
"""
Process CLI - FactSet Pipeline v3.6.1
å‘½ä»¤åˆ—ä»‹é¢ç”¨æ–¼è™•ç†MDæª”æ¡ˆã€åˆ†æå’Œæ¸…ç†åŠŸèƒ½
å®Œæ•´ç‰ˆåŒ…å«æ‰€æœ‰v3.6.1åŠŸèƒ½
"""

import os
import sys
import argparse
import json
from datetime import datetime
from typing import Dict, Any, List, Optional

# å°å…¥Process Groupæ¨¡çµ„
try:
    from md_scanner import MDScanner
    MD_SCANNER_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ MDæƒæåŠŸèƒ½ä¸å¯ç”¨: {e}")
    MD_SCANNER_AVAILABLE = False

try:
    from md_parser import MDParser
    MD_PARSER_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ MDè§£æåŠŸèƒ½ä¸å¯ç”¨: {e}")
    MD_PARSER_AVAILABLE = False

try:
    from quality_analyzer import QualityAnalyzer
    QUALITY_ANALYZER_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ å“è³ªåˆ†æåŠŸèƒ½ä¸å¯ç”¨: {e}")
    QUALITY_ANALYZER_AVAILABLE = False

try:
    from keyword_analyzer import KeywordAnalyzer
    KEYWORD_ANALYZER_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ é—œéµå­—åˆ†æåŠŸèƒ½ä¸å¯ç”¨: {e}")
    KEYWORD_ANALYZER_AVAILABLE = False

try:
    from watchlist_analyzer import WatchlistAnalyzer
    WATCHLIST_ANALYZER_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ è§€å¯Ÿåå–®åˆ†æåŠŸèƒ½ä¸å¯ç”¨: {e}")
    WATCHLIST_ANALYZER_AVAILABLE = False

try:
    from report_generator import ReportGenerator
    REPORT_GENERATOR_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ å ±å‘Šç”ŸæˆåŠŸèƒ½ä¸å¯ç”¨: {e}")
    REPORT_GENERATOR_AVAILABLE = False

try:
    from sheets_uploader import SheetsUploader
    SHEETS_UPLOADER_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ Google Sheetsä¸Šå‚³åŠŸèƒ½ä¸å¯ç”¨: {e}")
    SHEETS_UPLOADER_AVAILABLE = False

try:
    from md_cleaner import MDFileCleanupManager
    MD_CLEANER_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ MDæ¸…ç†åŠŸèƒ½ä¸å¯ç”¨: {e}")
    MD_CLEANER_AVAILABLE = False


class ProcessCLI:
    """
    Process CLI v3.6.1 - FactSet Pipeline å‘½ä»¤åˆ—ä»‹é¢
    æ”¯æ´MDæª”æ¡ˆè™•ç†ã€åˆ†æã€æ¸…ç†ç­‰å®Œæ•´åŠŸèƒ½
    """
    
    def __init__(self):
        self.version = "3.6.1"
        
        # åˆå§‹åŒ–MDæƒæå™¨ (å¿…éœ€)
        if MD_SCANNER_AVAILABLE:
            try:
                self.md_scanner = MDScanner()
                print(f"âœ… MDæƒæå™¨åˆå§‹åŒ–æˆåŠŸ (v{self.md_scanner.version})")
            except Exception as e:
                print(f"âŒ MDæƒæå™¨åˆå§‹åŒ–å¤±æ•—: {e}")
                self.md_scanner = None
        else:
            self.md_scanner = None
        
        # åˆå§‹åŒ–å¯é¸æ¨¡çµ„ (graceful degradation)
        self.md_parser = None
        self.quality_analyzer = None
        self.keyword_analyzer = None
        self.watchlist_analyzer = None
        self.report_generator = None
        self.sheets_uploader = None
        self.md_cleaner = None
        
        self._init_optional_components()
        
        print(f"ğŸ”§ Process CLI v{self.version} åˆå§‹åŒ–å®Œæˆ")
    
    def _init_optional_components(self):
        """åˆå§‹åŒ–å¯é¸çµ„ä»¶"""
        
        # MDè§£æå™¨
        if MD_PARSER_AVAILABLE:
            try:
                self.md_parser = MDParser()
                print(f"âœ… MDè§£æå™¨åˆå§‹åŒ–æˆåŠŸ (v{self.md_parser.version})")
            except Exception as e:
                print(f"âš ï¸ MDè§£æå™¨åˆå§‹åŒ–å¤±æ•—: {e}")
        
        # å“è³ªåˆ†æå™¨
        if QUALITY_ANALYZER_AVAILABLE:
            try:
                self.quality_analyzer = QualityAnalyzer()
                print(f"âœ… å“è³ªåˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ å“è³ªåˆ†æå™¨åˆå§‹åŒ–å¤±æ•—: {e}")
        
        # é—œéµå­—åˆ†æå™¨ (v3.6.1å‡ç´š)
        if KEYWORD_ANALYZER_AVAILABLE:
            try:
                self.keyword_analyzer = KeywordAnalyzer()
                print(f"âœ… é—œéµå­—åˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ é—œéµå­—åˆ†æå™¨åˆå§‹åŒ–å¤±æ•—: {e}")
        
        # è§€å¯Ÿåå–®åˆ†æå™¨ (v3.6.1æ–°å¢)
        if WATCHLIST_ANALYZER_AVAILABLE:
            try:
                self.watchlist_analyzer = WatchlistAnalyzer()
                print(f"âœ… è§€å¯Ÿåå–®åˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ è§€å¯Ÿåå–®åˆ†æå™¨åˆå§‹åŒ–å¤±æ•—: {e}")
        
        # å ±å‘Šç”Ÿæˆå™¨
        if REPORT_GENERATOR_AVAILABLE:
            try:
                self.report_generator = ReportGenerator()
                print(f"âœ… å ±å‘Šç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ å ±å‘Šç”Ÿæˆå™¨åˆå§‹åŒ–å¤±æ•—: {e}")
        
        # Google Sheetsä¸Šå‚³å™¨
        if SHEETS_UPLOADER_AVAILABLE:
            try:
                self.sheets_uploader = SheetsUploader()
                print(f"âœ… Google Sheetsä¸Šå‚³å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ Google Sheetsä¸Šå‚³å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
        
        # MDæ¸…ç†ç®¡ç†å™¨ (v3.6.1æ–°å¢)
        if MD_CLEANER_AVAILABLE:
            try:
                self.md_cleaner = MDFileCleanupManager()
                print(f"âœ… MDæ¸…ç†åŠŸèƒ½å·²å•Ÿç”¨ (v{self.md_cleaner.version})")
            except Exception as e:
                print(f"âš ï¸ MDæ¸…ç†ç®¡ç†å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
    
    def process_all_md_files(self, upload_sheets=True):
        """è™•ç†æ‰€æœ‰MDæª”æ¡ˆ - v3.6.1å®Œæ•´ç‰ˆ"""
        if not self.md_scanner:
            print("âŒ MDæƒæå™¨ä¸å¯ç”¨")
            return False
        
        try:
            print(f"ğŸ”„ é–‹å§‹è™•ç†æ‰€æœ‰MDæª”æ¡ˆ...")
            
            # 1. æƒææª”æ¡ˆ
            md_files = self.md_scanner.scan_all_md_files()
            if not md_files:
                print("ğŸ“ æœªæ‰¾åˆ°MDæª”æ¡ˆ")
                return True
            
            print(f"ğŸ“„ æ‰¾åˆ° {len(md_files)} å€‹MDæª”æ¡ˆ")
            
            # 2. è§£ææ¯å€‹æª”æ¡ˆ
            processed_companies = []
            parse_errors = 0
            
            for i, md_file in enumerate(md_files, 1):
                try:
                    print(f"ğŸ“– è™•ç† {i}/{len(md_files)}: {os.path.basename(md_file)}")
                    
                    if self.md_parser:
                        parsed_data = self.md_parser.parse_md_file(md_file)
                    else:
                        # ç°¡åŒ–è§£æ
                        parsed_data = self._simple_parse_md_file(md_file)
                    
                    # å“è³ªåˆ†æ
                    if self.quality_analyzer:
                        quality_data = self.quality_analyzer.analyze(parsed_data)
                        parsed_data.update(quality_data)
                    
                    processed_companies.append(parsed_data)
                    
                except Exception as e:
                    print(f"âš ï¸ è™•ç†æª”æ¡ˆå¤±æ•— {os.path.basename(md_file)}: {e}")
                    parse_errors += 1
            
            print(f"âœ… è™•ç†å®Œæˆ: {len(processed_companies)} æˆåŠŸ, {parse_errors} å¤±æ•—")
            
            # 3. é—œéµå­—åˆ†æ (v3.6.1å‡ç´š)
            pattern_analysis = None
            if self.keyword_analyzer:
                print(f"ğŸ” åŸ·è¡ŒæŸ¥è©¢æ¨¡å¼åˆ†æ...")
                pattern_analysis = self.keyword_analyzer.analyze_query_patterns(processed_companies)
            
            # 4. è§€å¯Ÿåå–®åˆ†æ (v3.6.1æ–°å¢)
            watchlist_analysis = None
            if self.watchlist_analyzer:
                print(f"ğŸ“‹ åŸ·è¡Œè§€å¯Ÿåå–®åˆ†æ...")
                watchlist_analysis = self.watchlist_analyzer.analyze_watchlist_coverage(processed_companies)
            
            # 5. ç”Ÿæˆå ±å‘Š
            if self.report_generator:
                print(f"ğŸ“Š ç”Ÿæˆå ±å‘Š...")
                portfolio_summary = self.report_generator.generate_portfolio_summary(processed_companies)
                detailed_report = self.report_generator.generate_detailed_report(processed_companies)
                
                pattern_summary = None
                if pattern_analysis:
                    pattern_summary = self.report_generator.generate_keyword_summary(pattern_analysis)
                
                watchlist_summary = None
                if watchlist_analysis:
                    watchlist_summary = self.report_generator.generate_watchlist_summary(watchlist_analysis)
                
                # å„²å­˜å ±å‘Š
                saved_reports = self.report_generator.save_all_reports(
                    portfolio_summary, detailed_report, pattern_summary, watchlist_summary
                )
                
                for report_name, file_path in saved_reports.items():
                    print(f"ğŸ’¾ {report_name} å·²å„²å­˜: {file_path}")
                
                # 6. ä¸Šå‚³ (å¯é¸)
                if upload_sheets and self.sheets_uploader:
                    print(f"â˜ï¸ ä¸Šå‚³åˆ°Google Sheets...")
                    upload_success = self.sheets_uploader.upload_all_reports(
                        portfolio_summary, detailed_report, pattern_summary, watchlist_summary
                    )
                    if upload_success:
                        print(f"âœ… Google Sheetsä¸Šå‚³æˆåŠŸ")
                    else:
                        print(f"âš ï¸ Google Sheetsä¸Šå‚³å¤±æ•—")
            
            return True
            
        except Exception as e:
            print(f"âŒ è™•ç†MDæª”æ¡ˆå¤±æ•—: {e}")
            return False
    
    def process_recent_md_files(self, hours=24, upload_sheets=True):
        """è™•ç†æœ€è¿‘çš„MDæª”æ¡ˆ"""
        if not self.md_scanner:
            print("âŒ MDæƒæå™¨ä¸å¯ç”¨")
            return False
        
        try:
            print(f"ğŸ”„ è™•ç†æœ€è¿‘ {hours} å°æ™‚çš„MDæª”æ¡ˆ...")
            
            recent_files = self.md_scanner.scan_recent_files(hours)
            if not recent_files:
                print(f"ğŸ“ æœ€è¿‘ {hours} å°æ™‚å…§æœªæ‰¾åˆ°æ–°æª”æ¡ˆ")
                return True
            
            print(f"ğŸ“„ æ‰¾åˆ° {len(recent_files)} å€‹æœ€è¿‘æª”æ¡ˆ")
            
            # ä½¿ç”¨ç›¸åŒçš„è™•ç†é‚è¼¯ï¼Œä½†åªè™•ç†æœ€è¿‘æª”æ¡ˆ
            # é€™è£¡å¯ä»¥é‡ç”¨ process_all_md_files çš„é‚è¼¯
            return self._process_file_list(recent_files, upload_sheets)
            
        except Exception as e:
            print(f"âŒ è™•ç†æœ€è¿‘æª”æ¡ˆå¤±æ•—: {e}")
            return False
    
    def process_single_company(self, company_code, upload_sheets=True):
        """è™•ç†å–®ä¸€å…¬å¸"""
        if not self.md_scanner:
            print("âŒ MDæƒæå™¨ä¸å¯ç”¨")
            return False
        
        try:
            print(f"ğŸ”„ è™•ç†å…¬å¸ {company_code}...")
            
            company_files = self.md_scanner.find_company_files(company_code)
            if not company_files:
                print(f"ğŸ“ æœªæ‰¾åˆ°å…¬å¸ {company_code} çš„æª”æ¡ˆ")
                return False
            
            print(f"ğŸ“„ æ‰¾åˆ° {len(company_files)} å€‹æª”æ¡ˆ")
            
            return self._process_file_list(company_files, upload_sheets)
            
        except Exception as e:
            print(f"âŒ è™•ç†å–®ä¸€å…¬å¸å¤±æ•—: {e}")
            return False
    
    def analyze_quality(self):
        """å“è³ªåˆ†æ"""
        if not self.md_scanner:
            print("âŒ MDæƒæå™¨ä¸å¯ç”¨")
            return False
        
        try:
            print(f"ğŸ“Š åŸ·è¡Œå“è³ªåˆ†æ...")
            
            md_files = self.md_scanner.scan_all_md_files()
            if not md_files:
                print("ğŸ“ æœªæ‰¾åˆ°MDæª”æ¡ˆ")
                return True
            
            processed_companies = []
            for md_file in md_files:
                try:
                    if self.md_parser:
                        parsed_data = self.md_parser.parse_md_file(md_file)
                    else:
                        parsed_data = self._simple_parse_md_file(md_file)
                    
                    if self.quality_analyzer:
                        quality_data = self.quality_analyzer.analyze(parsed_data)
                        parsed_data.update(quality_data)
                    
                    processed_companies.append(parsed_data)
                    
                except Exception as e:
                    print(f"âš ï¸ åˆ†ææª”æ¡ˆå¤±æ•—: {os.path.basename(md_file)}: {e}")
            
            # é¡¯ç¤ºå“è³ªçµ±è¨ˆ
            if processed_companies:
                quality_scores = [c.get('quality_score', 0) for c in processed_companies if c.get('quality_score')]
                if quality_scores:
                    avg_quality = sum(quality_scores) / len(quality_scores)
                    print(f"ğŸ“ˆ å¹³å‡å“è³ªè©•åˆ†: {avg_quality:.2f}")
                    print(f"ğŸ“Š é«˜å“è³ªæª”æ¡ˆ (>8): {len([s for s in quality_scores if s > 8])}")
                    print(f"ğŸ“Š ä¸­ç­‰å“è³ªæª”æ¡ˆ (5-8): {len([s for s in quality_scores if 5 <= s <= 8])}")
                    print(f"ğŸ“Š ä½å“è³ªæª”æ¡ˆ (<5): {len([s for s in quality_scores if s < 5])}")
            
            return True
            
        except Exception as e:
            print(f"âŒ å“è³ªåˆ†æå¤±æ•—: {e}")
            return False
    
    def analyze_keywords(self):
        """æŸ¥è©¢æ¨¡å¼åˆ†æ (v3.6.1å‡ç´š)"""
        if not self.keyword_analyzer:
            print("âŒ é—œéµå­—åˆ†æå™¨ä¸å¯ç”¨")
            return False
        
        try:
            print(f"ğŸ” åŸ·è¡ŒæŸ¥è©¢æ¨¡å¼åˆ†æ...")
            
            # å…ˆç²å–è™•ç†éçš„å…¬å¸è³‡æ–™
            processed_companies = self._get_processed_companies()
            if not processed_companies:
                print("ğŸ“ ç„¡è™•ç†éçš„å…¬å¸è³‡æ–™")
                return False
            
            # åŸ·è¡ŒæŸ¥è©¢æ¨¡å¼åˆ†æ
            pattern_analysis = self.keyword_analyzer.analyze_query_patterns(processed_companies)
            
            # é¡¯ç¤ºåˆ†æçµæœ
            if pattern_analysis:
                print(f"ğŸ“Š æŸ¥è©¢æ¨¡å¼çµ±è¨ˆ:")
                patterns = pattern_analysis.get('pattern_statistics', {})
                print(f"   ç¸½æ¨¡å¼æ•¸: {patterns.get('total_patterns', 0)}")
                print(f"   æœ‰æ•ˆæ¨¡å¼æ•¸: {patterns.get('valid_patterns', 0)}")
                print(f"   å¹³å‡ä½¿ç”¨æ¬¡æ•¸: {patterns.get('average_usage', 0):.1f}")
                
                # é¡¯ç¤ºç†±é–€æ¨¡å¼
                top_patterns = pattern_analysis.get('top_patterns', [])
                print(f"ğŸ“ˆ ç†±é–€æŸ¥è©¢æ¨¡å¼:")
                for i, (pattern, count) in enumerate(top_patterns[:5], 1):
                    print(f"   {i}. {pattern}: {count} æ¬¡")
            
            return True
            
        except Exception as e:
            print(f"âŒ æŸ¥è©¢æ¨¡å¼åˆ†æå¤±æ•—: {e}")
            return False
    
    def analyze_watchlist(self):
        """è§€å¯Ÿåå–®åˆ†æ (v3.6.1æ–°å¢)"""
        if not self.watchlist_analyzer:
            print("âŒ è§€å¯Ÿåå–®åˆ†æå™¨ä¸å¯ç”¨")
            return False
        
        try:
            print(f"ğŸ“‹ åŸ·è¡Œè§€å¯Ÿåå–®åˆ†æ...")
            
            # å…ˆç²å–è™•ç†éçš„å…¬å¸è³‡æ–™
            processed_companies = self._get_processed_companies()
            if not processed_companies:
                print("ğŸ“ ç„¡è™•ç†éçš„å…¬å¸è³‡æ–™")
                return False
            
            # åŸ·è¡Œè§€å¯Ÿåå–®åˆ†æ
            watchlist_analysis = self.watchlist_analyzer.analyze_watchlist_coverage(processed_companies)
            
            # é¡¯ç¤ºåˆ†æçµæœ
            if watchlist_analysis:
                print(f"ğŸ“Š è§€å¯Ÿåå–®è¦†è“‹çµ±è¨ˆ:")
                print(f"   ç¸½è§€å¯Ÿåå–®å…¬å¸: {watchlist_analysis.get('total_watchlist_companies', 0)}")
                print(f"   æœ‰MDæª”æ¡ˆå…¬å¸: {watchlist_analysis.get('companies_with_md_files', 0)}")
                print(f"   æˆåŠŸè™•ç†å…¬å¸: {watchlist_analysis.get('companies_processed_successfully', 0)}")
                print(f"   è¦†è“‹ç‡: {watchlist_analysis.get('coverage_rate', 0):.1f}%")
                print(f"   æˆåŠŸç‡: {watchlist_analysis.get('success_rate', 0):.1f}%")
                
                # é¡¯ç¤ºç‹€æ…‹çµ±è¨ˆ
                status_summary = watchlist_analysis.get('company_status_summary', {})
                print(f"ğŸ“ˆ è™•ç†ç‹€æ…‹åˆ†å¸ƒ:")
                for status, count in status_summary.items():
                    print(f"   {status}: {count} å®¶å…¬å¸")
            
            return True
            
        except Exception as e:
            print(f"âŒ è§€å¯Ÿåå–®åˆ†æå¤±æ•—: {e}")
            return False
    
    def generate_keyword_summary(self, upload_sheets=True, min_usage=1):
        """ç”ŸæˆæŸ¥è©¢æ¨¡å¼çµ±è¨ˆå ±å‘Š (v3.6.1å‡ç´š)"""
        if not self.keyword_analyzer or not self.report_generator:
            print("âŒ é—œéµå­—åˆ†ææˆ–å ±å‘Šç”ŸæˆåŠŸèƒ½ä¸å¯ç”¨")
            return False
        
        try:
            print(f"ğŸ“Š ç”ŸæˆæŸ¥è©¢æ¨¡å¼çµ±è¨ˆå ±å‘Š...")
            
            processed_companies = self._get_processed_companies()
            if not processed_companies:
                return False
            
            # åŸ·è¡Œåˆ†æ
            pattern_analysis = self.keyword_analyzer.analyze_query_patterns(processed_companies)
            
            # ç”Ÿæˆå ±å‘Š
            pattern_summary = self.report_generator.generate_keyword_summary(pattern_analysis)
            
            # å„²å­˜å ±å‘Š
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"query-pattern-summary_{timestamp}.csv"
            filepath = os.path.join("data/reports", filename)
            
            os.makedirs("data/reports", exist_ok=True)
            pattern_summary.to_csv(filepath, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ æŸ¥è©¢æ¨¡å¼å ±å‘Šå·²å„²å­˜: {filename}")
            
            # ä¸Šå‚³åˆ°Sheets
            if upload_sheets and self.sheets_uploader:
                upload_success = self.sheets_uploader._upload_keyword_summary(pattern_summary)
                if upload_success:
                    print(f"â˜ï¸ å·²ä¸Šå‚³åˆ°Google Sheets")
            
            return True
            
        except Exception as e:
            print(f"âŒ ç”ŸæˆæŸ¥è©¢æ¨¡å¼å ±å‘Šå¤±æ•—: {e}")
            return False
    
    def generate_watchlist_summary(self, upload_sheets=True, include_missing=False):
        """ç”Ÿæˆè§€å¯Ÿåå–®çµ±è¨ˆå ±å‘Š (v3.6.1æ–°å¢)"""
        if not self.watchlist_analyzer or not self.report_generator:
            print("âŒ è§€å¯Ÿåå–®åˆ†ææˆ–å ±å‘Šç”ŸæˆåŠŸèƒ½ä¸å¯ç”¨")
            return False
        
        try:
            print(f"ğŸ“‹ ç”Ÿæˆè§€å¯Ÿåå–®çµ±è¨ˆå ±å‘Š...")
            
            processed_companies = self._get_processed_companies()
            if not processed_companies:
                return False
            
            # åŸ·è¡Œåˆ†æ
            watchlist_analysis = self.watchlist_analyzer.analyze_watchlist_coverage(processed_companies)
            
            # ç”Ÿæˆå ±å‘Š
            watchlist_summary = self.report_generator.generate_watchlist_summary(watchlist_analysis)
            
            # å„²å­˜å ±å‘Š
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"watchlist-summary_{timestamp}.csv"
            filepath = os.path.join("data/reports", filename)
            
            os.makedirs("data/reports", exist_ok=True)
            watchlist_summary.to_csv(filepath, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ è§€å¯Ÿåå–®å ±å‘Šå·²å„²å­˜: {filename}")
            
            # ä¸Šå‚³åˆ°Sheets
            if upload_sheets and self.sheets_uploader:
                upload_success = self.sheets_uploader._upload_watchlist_summary(watchlist_summary)
                if upload_success:
                    print(f"â˜ï¸ å·²ä¸Šå‚³åˆ°Google Sheets")
            
            return True
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆè§€å¯Ÿåå–®å ±å‘Šå¤±æ•—: {e}")
            return False
    
    # MDæª”æ¡ˆæ¸…ç†åŠŸèƒ½ (v3.6.1æ–°å¢)
    
    def cleanup_md_files(self, days=90, quality_threshold=8, dry_run=True, 
                        create_backup=True, force=False, generate_report=True):
        """MDæª”æ¡ˆæ¸…ç†å‘½ä»¤ - åŸºæ–¼è²¡å‹™æ•¸æ“šç™¼å¸ƒæ—¥æœŸ"""
        if not self.md_cleaner:
            print("âŒ MDæ¸…ç†åŠŸèƒ½ä¸å¯ç”¨")
            return False
        
        try:
            print(f"ğŸ§¹ é–‹å§‹MDæª”æ¡ˆæ¸…ç†...")
            print(f"   ä¿ç•™æœŸé™: {days} å¤©")
            print(f"   è³ªé‡é–¾å€¼: {quality_threshold}")
            print(f"   æ¨¡å¼: {'é è¦½' if dry_run else 'å¯¦éš›åŸ·è¡Œ'}")
            
            # æƒæMDæª”æ¡ˆ
            md_files = self.md_cleaner.scan_md_files()
            if not md_files:
                print("ğŸ“ æœªæ‰¾åˆ°MDæª”æ¡ˆ")
                return True
            
            # ç”Ÿæˆæ¸…ç†è¨ˆåŠƒ
            plan = self.md_cleaner.analyze_files_for_cleanup(
                md_files, 
                retention_days=days, 
                quality_threshold=quality_threshold
            )
            
            # å®‰å…¨æª¢æŸ¥
            if not plan.safety_checks_passed and not force and not dry_run:
                print(f"\nâš ï¸ å®‰å…¨æª¢æŸ¥æœªé€šéï¼Œæ¸…ç†è¨ˆåŠƒå­˜åœ¨é¢¨éšª:")
                for warning in plan.warnings:
                    print(f"   - {warning}")
                print(f"\nğŸ’¡ é¸é …:")
                print(f"   1. ä½¿ç”¨ --dry-run é è¦½è©³ç´°è³‡è¨Š")
                print(f"   2. ä½¿ç”¨ --force å¼·åˆ¶åŸ·è¡Œ")
                print(f"   3. èª¿æ•´ --days æˆ– --quality-threshold åƒæ•¸")
                return False
            
            # åŸ·è¡Œæ¸…ç†
            result = self.md_cleaner.execute_cleanup(
                plan, 
                dry_run=dry_run, 
                create_backup=create_backup
            )
            
            # ç”Ÿæˆå ±å‘Š
            if generate_report:
                report = self.md_cleaner.generate_cleanup_report(md_files, plan, result)
                self._save_cleanup_report(report, dry_run)
            
            return result.files_deleted > 0 or dry_run
            
        except Exception as e:
            print(f"âŒ MDæª”æ¡ˆæ¸…ç†å¤±æ•—: {e}")
            return False
    
    def show_md_cleanup_preview(self, days=90, quality_threshold=8, show_details=False):
        """é¡¯ç¤ºMDæª”æ¡ˆæ¸…ç†é è¦½"""
        if not self.md_cleaner:
            print("âŒ MDæ¸…ç†åŠŸèƒ½ä¸å¯ç”¨")
            return False
        
        try:
            print(f"ğŸ” MDæª”æ¡ˆæ¸…ç†é è¦½ (ä¿ç•™æœŸ: {days}å¤©, è³ªé‡é–¾å€¼: {quality_threshold})")
            
            # æƒæå’Œåˆ†ææª”æ¡ˆ
            md_files = self.md_cleaner.scan_md_files()
            if not md_files:
                print("ğŸ“ æœªæ‰¾åˆ°MDæª”æ¡ˆ")
                return True
            
            plan = self.md_cleaner.analyze_files_for_cleanup(
                md_files, retention_days=days, quality_threshold=quality_threshold
            )
            
            # é¡¯ç¤ºè©³ç´°é è¦½
            print(f"\nğŸ“‹ æ¸…ç†è¨ˆåŠƒè©³æƒ…:")
            print(f"   ğŸ“„ ç¸½æª”æ¡ˆæ•¸: {plan.total_files}")
            print(f"   ğŸ—‘ï¸  åˆªé™¤å€™é¸: {len(plan.deletion_candidates)}")
            print(f"   ğŸ’¾ ä¿ç•™æª”æ¡ˆ: {len(plan.preserved_files)}")
            print(f"   â“ ç„¡æ—¥æœŸæª”æ¡ˆ: {len(plan.no_date_files)}")
            print(f"   ğŸ’½ é ä¼°ç¯€çœ: {self.md_cleaner._format_size(plan.estimated_space_saved)}")
            
            # é¡¯ç¤ºåˆªé™¤å€™é¸è©³æƒ…
            if plan.deletion_candidates and show_details:
                print(f"\nğŸ—‘ï¸  åˆªé™¤å€™é¸æª”æ¡ˆæ¸…å–®:")
                for i, file_info in enumerate(plan.deletion_candidates):
                    date_str = file_info.md_date.strftime('%Y-%m-%d') if file_info.md_date else 'ç„¡æ—¥æœŸ'
                    quality_str = f"(Q:{file_info.quality_score:.1f})" if file_info.quality_score else "(ç„¡è©•åˆ†)"
                    size_str = self.md_cleaner._format_size(file_info.file_size)
                    print(f"   {i+1:3d}. {file_info.filename}")
                    print(f"        æ—¥æœŸ: {date_str} | å¹´é½¡: {file_info.age_days}å¤© | å¤§å°: {size_str} {quality_str}")
                    print(f"        åŸå› : {file_info.preservation_reason}")
            
            # å®‰å…¨æª¢æŸ¥çµæœ
            if plan.safety_checks_passed:
                print(f"\nâœ… å®‰å…¨æª¢æŸ¥é€šéï¼Œå¯ä»¥åŸ·è¡Œæ¸…ç†")
            else:
                print(f"\nâš ï¸  å®‰å…¨æª¢æŸ¥è­¦å‘Š:")
                for warning in plan.warnings:
                    print(f"   - {warning}")
                print(f"\nğŸ’¡ å»ºè­°ä½¿ç”¨ --force åƒæ•¸å¼·åˆ¶åŸ·è¡Œæˆ–èª¿æ•´åƒæ•¸")
            
            return True
            
        except Exception as e:
            print(f"âŒ é è¦½ç”Ÿæˆå¤±æ•—: {e}")
            return False
    
    def analyze_md_files(self):
        """åˆ†æMDæª”æ¡ˆç‹€æ…‹"""
        if not self.md_cleaner:
            print("âŒ MDæ¸…ç†åŠŸèƒ½ä¸å¯ç”¨")
            return False
        
        try:
            print(f"ğŸ“Š åˆ†æMDæª”æ¡ˆç‹€æ…‹...")
            
            # ç²å–çµ±è¨ˆè³‡è¨Š
            stats = self.md_cleaner.get_statistics()
            
            print(f"\nğŸ“‹ MDæª”æ¡ˆç›®éŒ„çµ±è¨ˆ:")
            print(f"   ğŸ“ ç›®éŒ„: {self.md_cleaner.md_dir}")
            print(f"   ğŸ“„ ç¸½æª”æ¡ˆæ•¸: {stats['total_files']}")
            print(f"   ğŸ’¾ ç¸½å¤§å°: {stats.get('total_size_formatted', '0 B')}")
            print(f"   ğŸ“… æ—¥æœŸæå–æˆåŠŸç‡: {stats.get('date_extraction_success_rate', 0):.1f}%")
            print(f"   ğŸ”§ è§£æå™¨å¯ç”¨: {'æ˜¯' if stats.get('parser_available', False) else 'å¦'}")
            
            if stats.get('age_statistics'):
                age_stats = stats['age_statistics']
                print(f"\nğŸ“ˆ å¹´é½¡çµ±è¨ˆ:")
                print(f"   å¹³å‡å¹´é½¡: {age_stats.get('average_age_days', 0):.1f} å¤©")
                print(f"   ä¸­ä½æ•¸å¹´é½¡: {age_stats.get('median_age_days', 0):.1f} å¤©")
                print(f"   æœ€èˆŠæª”æ¡ˆ: {age_stats.get('oldest_file_days', 0)} å¤©")
                print(f"   æœ€æ–°æª”æ¡ˆ: {age_stats.get('newest_file_days', 0)} å¤©")
            
            if stats.get('quality_statistics'):
                quality_stats = stats['quality_statistics']
                print(f"\nâ­ è³ªé‡çµ±è¨ˆ:")
                print(f"   å¹³å‡è³ªé‡: {quality_stats.get('average_quality', 0):.1f}")
                print(f"   æœ€é«˜è³ªé‡: {quality_stats.get('highest_quality', 0):.1f}")
                print(f"   æœ€ä½è³ªé‡: {quality_stats.get('lowest_quality', 0):.1f}")
            
            # å¹´é½¡åˆ†å¸ƒ
            if stats.get('age_distribution'):
                print(f"\nğŸ“Š å¹´é½¡åˆ†å¸ƒ:")
                for age_group, count in stats['age_distribution'].items():
                    print(f"   {age_group}: {count} æª”æ¡ˆ")
            
            # è³ªé‡åˆ†å¸ƒ
            if stats.get('quality_distribution'):
                print(f"\nğŸ† è³ªé‡åˆ†å¸ƒ:")
                for quality_group, count in stats['quality_distribution'].items():
                    print(f"   {quality_group}: {count} æª”æ¡ˆ")
            
            # ç†±é–€å…¬å¸
            if stats.get('top_companies'):
                print(f"\nğŸ¢ æª”æ¡ˆæ•¸æœ€å¤šçš„å…¬å¸ (Top 5):")
                for i, (company, count) in enumerate(list(stats['top_companies'].items())[:5]):
                    print(f"   {i+1}. {company}: {count} æª”æ¡ˆ")
            
            return True
            
        except Exception as e:
            print(f"âŒ MDæª”æ¡ˆåˆ†æå¤±æ•—: {e}")
            return False
    
    def show_statistics(self):
        """é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š"""
        if not self.md_scanner:
            print("âŒ MDæƒæå™¨ä¸å¯ç”¨")
            return False
        
        try:
            print(f"ğŸ“Š Process CLI v{self.version} çµ±è¨ˆè³‡è¨Š")
            
            # MDæƒæå™¨çµ±è¨ˆ
            stats = self.md_scanner.get_stats()
            print(f"\nğŸ“„ MDæª”æ¡ˆçµ±è¨ˆ:")
            print(f"   ç¸½æª”æ¡ˆæ•¸: {stats['total_files']}")
            print(f"   æœ€è¿‘24h: {stats['recent_files_24h']}")
            print(f"   å…¬å¸æ•¸é‡: {stats['unique_companies']}")
            
            if 'file_size_stats' in stats:
                size_stats = stats['file_size_stats']
                print(f"   ç¸½å¤§å°: {size_stats.get('total_size_mb', 0)} MB")
                print(f"   å¹³å‡å¤§å°: {size_stats.get('average_size_kb', 0)} KB")
            
            # çµ„ä»¶ç‹€æ…‹
            print(f"\nğŸ”§ çµ„ä»¶ç‹€æ…‹:")
            print(f"   MDæƒæå™¨: {'âœ…' if self.md_scanner else 'âŒ'}")
            print(f"   MDè§£æå™¨: {'âœ…' if self.md_parser else 'âŒ'}")
            print(f"   å“è³ªåˆ†æå™¨: {'âœ…' if self.quality_analyzer else 'âŒ'}")
            print(f"   é—œéµå­—åˆ†æå™¨: {'âœ…' if self.keyword_analyzer else 'âŒ'}")
            print(f"   è§€å¯Ÿåå–®åˆ†æå™¨: {'âœ…' if self.watchlist_analyzer else 'âŒ'}")
            print(f"   å ±å‘Šç”Ÿæˆå™¨: {'âœ…' if self.report_generator else 'âŒ'}")
            print(f"   Google Sheetsä¸Šå‚³: {'âœ…' if self.sheets_uploader else 'âŒ'}")
            print(f"   MDæ¸…ç†åŠŸèƒ½: {'âœ…' if self.md_cleaner else 'âŒ'}")
            
            return True
            
        except Exception as e:
            print(f"âŒ çµ±è¨ˆè³‡è¨Šç²å–å¤±æ•—: {e}")
            return False
    
    def validate_setup(self):
        """é©—è­‰ç’°å¢ƒè¨­å®š"""
        try:
            print(f"ğŸ” é©—è­‰ Process CLI v{self.version} ç’°å¢ƒè¨­å®š...")
            
            validation_results = {}
            
            # é©—è­‰MDæƒæå™¨
            if self.md_scanner:
                md_files = self.md_scanner.scan_all_md_files()
                validation_results['md_scanner'] = f"âœ… æ‰¾åˆ° {len(md_files)} å€‹æª”æ¡ˆ"
            else:
                validation_results['md_scanner'] = "âŒ MDæƒæå™¨ä¸å¯ç”¨"
            
            # é©—è­‰å…¶ä»–çµ„ä»¶
            validation_results['md_parser'] = "âœ… MDè§£æå™¨å·²è¼‰å…¥" if self.md_parser else "âŒ MDè§£æå™¨ä¸å¯ç”¨"
            validation_results['quality_analyzer'] = "âœ… å“è³ªåˆ†æå™¨å·²è¼‰å…¥" if self.quality_analyzer else "âŒ å“è³ªåˆ†æå™¨ä¸å¯ç”¨"
            validation_results['keyword_analyzer'] = "âœ… é—œéµå­—åˆ†æå™¨å·²è¼‰å…¥" if self.keyword_analyzer else "âŒ é—œéµå­—åˆ†æå™¨ä¸å¯ç”¨"
            validation_results['watchlist_analyzer'] = "âœ… è§€å¯Ÿåå–®åˆ†æå™¨å·²è¼‰å…¥" if self.watchlist_analyzer else "âŒ è§€å¯Ÿåå–®åˆ†æå™¨ä¸å¯ç”¨"
            validation_results['report_generator'] = "âœ… å ±å‘Šç”Ÿæˆå™¨å·²è¼‰å…¥" if self.report_generator else "âŒ å ±å‘Šç”Ÿæˆå™¨ä¸å¯ç”¨"
            validation_results['sheets_uploader'] = "âœ… Google Sheetsä¸Šå‚³å™¨å·²è¼‰å…¥" if self.sheets_uploader else "âŒ Google Sheetsä¸Šå‚³å™¨ä¸å¯ç”¨"
            validation_results['md_cleaner'] = "âœ… MDæ¸…ç†åŠŸèƒ½å·²è¼‰å…¥" if self.md_cleaner else "âŒ MDæ¸…ç†åŠŸèƒ½ä¸å¯ç”¨"
            
            # é¡¯ç¤ºé©—è­‰çµæœ
            print(f"\nğŸ“‹ é©—è­‰çµæœ:")
            for component, status in validation_results.items():
                print(f"   {component}: {status}")
            
            # æª¢æŸ¥è§€å¯Ÿåå–®
            if self.watchlist_analyzer:
                watchlist_size = len(self.watchlist_analyzer.watchlist_mapping)
                print(f"\nğŸ“‹ è§€å¯Ÿåå–®: è¼‰å…¥ {watchlist_size} å®¶å…¬å¸")
            
            # æª¢æŸ¥ç›®éŒ„çµæ§‹
            directories = ['data', 'data/md', 'data/reports']
            print(f"\nğŸ“ ç›®éŒ„çµæ§‹:")
            for directory in directories:
                exists = os.path.exists(directory)
                print(f"   {directory}: {'âœ…' if exists else 'âŒ'}")
                if not exists:
                    os.makedirs(directory, exist_ok=True)
                    print(f"      å·²å‰µå»ºç›®éŒ„: {directory}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ç’°å¢ƒé©—è­‰å¤±æ•—: {e}")
            return False
    
    # è¼”åŠ©æ–¹æ³•
    
    def _process_file_list(self, file_list, upload_sheets=True):
        """è™•ç†æŒ‡å®šçš„æª”æ¡ˆæ¸…å–®"""
        processed_companies = []
        
        for md_file in file_list:
            try:
                if self.md_parser:
                    parsed_data = self.md_parser.parse_md_file(md_file)
                else:
                    parsed_data = self._simple_parse_md_file(md_file)
                
                if self.quality_analyzer:
                    quality_data = self.quality_analyzer.analyze(parsed_data)
                    parsed_data.update(quality_data)
                
                processed_companies.append(parsed_data)
                
            except Exception as e:
                print(f"âš ï¸ è™•ç†æª”æ¡ˆå¤±æ•—: {os.path.basename(md_file)}: {e}")
        
        # ç”Ÿæˆå’Œä¸Šå‚³å ±å‘Š
        if processed_companies and self.report_generator:
            portfolio_summary = self.report_generator.generate_portfolio_summary(processed_companies)
            detailed_report = self.report_generator.generate_detailed_report(processed_companies)
            
            if upload_sheets and self.sheets_uploader:
                self.sheets_uploader.upload_all_reports(portfolio_summary, detailed_report)
        
        return len(processed_companies) > 0
    
    def _simple_parse_md_file(self, file_path):
        """ç°¡åŒ–çš„MDæª”æ¡ˆè§£æ (ç•¶MDè§£æå™¨ä¸å¯ç”¨æ™‚)"""
        filename = os.path.basename(file_path)
        parts = filename.replace('.md', '').split('_')
        
        return {
            'filename': filename,
            'company_code': parts[0] if len(parts) >= 1 else 'Unknown',
            'company_name': parts[1] if len(parts) >= 2 else 'Unknown',
            'data_source': parts[2] if len(parts) >= 3 else 'Unknown',
            'file_mtime': datetime.fromtimestamp(os.path.getmtime(file_path)),
            'search_keywords': [],
            'quality_score': 5.0,  # é è¨­åˆ†æ•¸
            'has_eps_data': False,
            'has_target_price': False,
            'has_analyst_info': False
        }
    
    def _get_processed_companies(self):
        """ç²å–è™•ç†éçš„å…¬å¸è³‡æ–™"""
        if not self.md_scanner:
            return []
        
        md_files = self.md_scanner.scan_all_md_files()
        processed_companies = []
        
        for md_file in md_files:
            try:
                if self.md_parser:
                    parsed_data = self.md_parser.parse_md_file(md_file)
                else:
                    parsed_data = self._simple_parse_md_file(md_file)
                
                if self.quality_analyzer:
                    quality_data = self.quality_analyzer.analyze(parsed_data)
                    parsed_data.update(quality_data)
                
                processed_companies.append(parsed_data)
                
            except Exception as e:
                continue
        
        return processed_companies
    
    def _save_cleanup_report(self, report: Dict[str, Any], dry_run: bool):
        """å„²å­˜æ¸…ç†å ±å‘Š"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            mode = "preview" if dry_run else "execution"
            
            # ç¢ºä¿å ±å‘Šç›®éŒ„å­˜åœ¨
            reports_dir = "data/reports"
            os.makedirs(reports_dir, exist_ok=True)
            
            # å„²å­˜JSONå ±å‘Š
            report_filename = f"md_cleanup_{mode}_{timestamp}.json"
            report_path = os.path.join(reports_dir, report_filename)
            
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            # å„²å­˜æœ€æ–°ç‰ˆæœ¬
            latest_path = os.path.join(reports_dir, f"md_cleanup_{mode}_latest.json")
            with open(latest_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“„ æ¸…ç†å ±å‘Šå·²å„²å­˜: {report_filename}")
            
        except Exception as e:
            print(f"âš ï¸ å ±å‘Šå„²å­˜å¤±æ•—: {e}")


def main():
    """ä¸»ç¨‹å¼å…¥å£é»"""
    parser = argparse.ArgumentParser(description='FactSet è™•ç†ç³»çµ± v3.6.1 (å«MDæª”æ¡ˆæ¸…ç†)')
    parser.add_argument('command', choices=[
        'process',            # è™•ç†æ‰€æœ‰ MD æª”æ¡ˆ
        'process-recent',     # è™•ç†æœ€è¿‘çš„ MD æª”æ¡ˆ
        'process-single',     # è™•ç†å–®ä¸€å…¬å¸
        'analyze-quality',    # å“è³ªåˆ†æ
        'analyze-keywords',   # æŸ¥è©¢æ¨¡å¼åˆ†æ
        'analyze-watchlist',  # è§€å¯Ÿåå–®åˆ†æ
        'keyword-summary',    # æŸ¥è©¢æ¨¡å¼çµ±è¨ˆå ±å‘Š
        'watchlist-summary',  # è§€å¯Ÿåå–®çµ±è¨ˆå ±å‘Š
        'cleanup',           # MDæª”æ¡ˆæ¸…ç†
        'cleanup-preview',   # MDæª”æ¡ˆæ¸…ç†é è¦½
        'analyze-md',        # MDæª”æ¡ˆç‹€æ…‹åˆ†æ
        'stats',             # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
        'validate'           # é©—è­‰ç’°å¢ƒè¨­å®š
    ])
    
    # ç¾æœ‰åƒæ•¸
    parser.add_argument('--company', help='å…¬å¸ä»£è™Ÿ')
    parser.add_argument('--hours', type=int, default=24, help='å°æ™‚æ•¸')
    parser.add_argument('--no-upload', action='store_true', help='ä¸ä¸Šå‚³åˆ° Sheets')
    parser.add_argument('--force-upload', action='store_true', help='å¼·åˆ¶ä¸Šå‚³ï¼Œå¿½ç•¥é©—è­‰éŒ¯èª¤')
    parser.add_argument('--min-usage', type=int, default=1, help='æŸ¥è©¢æ¨¡å¼æœ€å°ä½¿ç”¨æ¬¡æ•¸')
    parser.add_argument('--include-missing', action='store_true', help='åŒ…å«ç¼ºå¤±å…¬å¸è³‡è¨Š')
    parser.add_argument('--dry-run', action='store_true', help='é è¦½æ¨¡å¼ï¼Œä¸å¯¦éš›åŸ·è¡Œ')
    
    # æ–°å¢æ¸…ç†ç›¸é—œåƒæ•¸
    parser.add_argument('--days', type=int, default=90, help='MDæª”æ¡ˆä¿ç•™å¤©æ•¸ (é è¨­: 90)')
    parser.add_argument('--quality-threshold', type=float, default=8, help='é«˜è³ªé‡æª”æ¡ˆå»¶é•·ä¿ç•™é–¾å€¼ (é è¨­: 8)')
    parser.add_argument('--no-backup', action='store_true', help='æ¸…ç†æ™‚ä¸å‰µå»ºå‚™ä»½')
    parser.add_argument('--force', action='store_true', help='å¼·åˆ¶åŸ·è¡Œæ¸…ç†ï¼Œå¿½ç•¥å®‰å…¨æª¢æŸ¥')
    parser.add_argument('--show-details', action='store_true', help='é¡¯ç¤ºè©³ç´°çš„æª”æ¡ˆæ¸…å–®')
    parser.add_argument('--no-report', action='store_true', help='ä¸ç”Ÿæˆæ¸…ç†å ±å‘Š')
    
    args = parser.parse_args()
    
    # å‰µå»ºCLIå¯¦ä¾‹
    cli = ProcessCLI()
    success = False
    
    try:
        # å‘½ä»¤è™•ç†
        if args.command == 'process':
            success = cli.process_all_md_files(upload_sheets=not args.no_upload)
        elif args.command == 'process-recent':
            success = cli.process_recent_md_files(hours=args.hours, upload_sheets=not args.no_upload)
        elif args.command == 'process-single':
            if not args.company:
                print("âŒ è™•ç†å–®ä¸€å…¬å¸éœ€è¦æŒ‡å®š --company åƒæ•¸")
                success = False
            else:
                success = cli.process_single_company(args.company, upload_sheets=not args.no_upload)
        elif args.command == 'analyze-quality':
            success = cli.analyze_quality()
        elif args.command == 'analyze-keywords':
            success = cli.analyze_keywords()
        elif args.command == 'analyze-watchlist':
            success = cli.analyze_watchlist()
        elif args.command == 'keyword-summary':
            success = cli.generate_keyword_summary(upload_sheets=not args.no_upload, min_usage=args.min_usage)
        elif args.command == 'watchlist-summary':
            success = cli.generate_watchlist_summary(upload_sheets=not args.no_upload, include_missing=args.include_missing)
        elif args.command == 'cleanup':
            success = cli.cleanup_md_files(
                days=args.days,
                quality_threshold=args.quality_threshold,
                dry_run=args.dry_run,
                create_backup=not args.no_backup,
                force=args.force,
                generate_report=not args.no_report
            )
        elif args.command == 'cleanup-preview':
            success = cli.show_md_cleanup_preview(
                days=args.days,
                quality_threshold=args.quality_threshold,
                show_details=args.show_details
            )
        elif args.command == 'analyze-md':
            success = cli.analyze_md_files()
        elif args.command == 'stats':
            success = cli.show_statistics()
        elif args.command == 'validate':
            success = cli.validate_setup()
        else:
            print(f"âŒ æœªçŸ¥å‘½ä»¤: {args.command}")
            success = False
    
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ç”¨æˆ¶ä¸­æ–·æ“ä½œ")
        success = False
    except Exception as e:
        print(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")
        success = False
    
    # é€€å‡ºç¨‹å¼ç¢¼
    exit_code = 0 if success else 1
    print(f"\nğŸ åŸ·è¡Œå®Œæˆ (é€€å‡ºç¢¼: {exit_code})")
    exit(exit_code)


if __name__ == "__main__":
    main()