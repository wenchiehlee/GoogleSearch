#!/usr/bin/env python3
"""
MD Cleaner - FactSet Pipeline v3.6.1
MDæª”æ¡ˆå¹´é½¡ç®¡ç†å·¥å…·ï¼ŒåŸºæ–¼æå–çš„è²¡å‹™æ•¸æ“šç™¼å¸ƒæ—¥æœŸé€²è¡Œæ¸…ç†
å®Œå…¨æ•´åˆProcess Groupæ¶æ§‹ï¼Œé‡ç”¨md_parserçš„æ—¥æœŸæå–é‚è¼¯
"""

import os
import shutil
import json
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
import statistics

# å°å…¥ç¾æœ‰çš„MDè§£æå™¨ä»¥ç¢ºä¿æ—¥æœŸæå–é‚è¼¯ä¸€è‡´
try:
    from md_parser import MDParser
except ImportError:
    # å¦‚æœåœ¨ä¸åŒç›®éŒ„é‹è¡Œï¼Œå˜—è©¦ç›¸å°å°å…¥
    import sys
    sys.path.append(os.path.dirname(__file__))
    from md_parser import MDParser

@dataclass
class MDFileInfo:
    """MDæª”æ¡ˆè³‡è¨Šæ•¸æ“šçµæ§‹"""
    filepath: str
    filename: str
    file_size: int
    file_mtime: datetime
    md_date: Optional[datetime]  # è²¡å‹™æ•¸æ“šç™¼å¸ƒæ—¥æœŸ
    extracted_date: Optional[datetime]  # ç³»çµ±æå–æ™‚é–“æˆ³
    quality_score: Optional[float]
    company_code: str
    company_name: str
    age_days: int
    deletion_candidate: bool
    preservation_reason: Optional[str]
    date_extraction_method: str

@dataclass
class CleanupPlan:
    """æ¸…ç†è¨ˆåŠƒæ•¸æ“šçµæ§‹"""
    total_files: int
    deletion_candidates: List[MDFileInfo]
    preserved_files: List[MDFileInfo]
    no_date_files: List[MDFileInfo]
    estimated_space_saved: int
    safety_checks_passed: bool
    warnings: List[str]
    errors: List[str]

@dataclass
class CleanupResult:
    """æ¸…ç†çµæœæ•¸æ“šçµæ§‹"""
    files_deleted: int
    files_preserved: int
    files_backed_up: int
    space_freed: int
    errors: List[str]
    backup_location: Optional[str]
    execution_time: float
    dry_run: bool

class MDFileCleanupManager:
    """MDæª”æ¡ˆæ¸…ç†ç®¡ç†å™¨ v3.6.1"""
    
    def __init__(self, md_dir="data/md"):
        """åˆå§‹åŒ–æ¸…ç†ç®¡ç†å™¨"""
        self.md_dir = os.path.abspath(md_dir)
        self.version = "3.6.1"
        
        # åˆå§‹åŒ–MDè§£æå™¨ï¼ˆé‡ç”¨ç¾æœ‰é‚è¼¯ï¼‰
        try:
            self.md_parser = MDParser()
            self.parser_available = True
            print(f"âœ… MDè§£æå™¨åˆå§‹åŒ–æˆåŠŸ (v{self.md_parser.version})")
        except Exception as e:
            print(f"âš ï¸ MDè§£æå™¨åˆå§‹åŒ–å¤±æ•—: {e}")
            print("ğŸ”§ å°‡ä½¿ç”¨ç°¡åŒ–çš„æ—¥æœŸæå–é‚è¼¯")
            self.md_parser = None
            self.parser_available = False
        
        # æ¸…ç†é…ç½®
        self.config = {
            'default_retention_days': 90,
            'quality_threshold': 8,
            'extended_retention_days': 30,
            'safety_thresholds': {
                'max_deletion_percentage': 0.7,  # æœ€å¤šåˆªé™¤70%çš„æª”æ¡ˆ
                'minimum_files_remaining': 10    # è‡³å°‘ä¿ç•™10å€‹æª”æ¡ˆ
            },
            'backup': {
                'enabled': True,
                'archive_path': '../data/archive',
                'compression': False  # ç°¡åŒ–å¯¦ç¾ï¼Œä¸å£“ç¸®
            },
            'aggressive_deletion': True  # ç„¡æ³•æå–æ—¥æœŸçš„æª”æ¡ˆç›´æ¥åˆªé™¤
        }
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        self._ensure_directories()
        
        print(f"ğŸ”§ MDæª”æ¡ˆæ¸…ç†ç®¡ç†å™¨ v{self.version} åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ ç›£æ§ç›®éŒ„: {self.md_dir}")
        print(f"âš™ï¸ é è¨­ä¿ç•™æœŸ: {self.config['default_retention_days']}å¤©")
        print(f"â­ è³ªé‡é–¾å€¼: {self.config['quality_threshold']}")

    def _ensure_directories(self):
        """ç¢ºä¿å¿…è¦çš„ç›®éŒ„å­˜åœ¨"""
        os.makedirs(self.md_dir, exist_ok=True)
        if self.config['backup']['enabled']:
            archive_path = os.path.abspath(self.config['backup']['archive_path'])
            os.makedirs(archive_path, exist_ok=True)

    def scan_md_files(self) -> List[MDFileInfo]:
        """æƒææ‰€æœ‰MDæª”æ¡ˆä¸¦æå–è³‡è¨Š"""
        print(f"ğŸ” æƒæMDæª”æ¡ˆç›®éŒ„: {self.md_dir}")
        
        if not os.path.exists(self.md_dir):
            print(f"âŒ ç›®éŒ„ä¸å­˜åœ¨: {self.md_dir}")
            return []
        
        md_files = []
        total_files = 0
        successful_extractions = 0
        failed_extractions = 0
        
        for filename in os.listdir(self.md_dir):
            if not filename.endswith('.md'):
                continue
                
            total_files += 1
            filepath = os.path.join(self.md_dir, filename)
            
            try:
                file_info = self._extract_file_info(filepath)
                md_files.append(file_info)
                
                if file_info.md_date:
                    successful_extractions += 1
                else:
                    failed_extractions += 1
                    
            except Exception as e:
                failed_extractions += 1
                print(f"âš ï¸ è™•ç†æª”æ¡ˆå¤±æ•— {filename}: {e}")
        
        print(f"ğŸ“Š æƒæçµæœ:")
        print(f"   ç¸½æª”æ¡ˆæ•¸: {total_files}")
        print(f"   æˆåŠŸè§£æ: {successful_extractions}")
        print(f"   è§£æå¤±æ•—: {failed_extractions}")
        print(f"   æˆåŠŸç‡: {successful_extractions/total_files*100:.1f}%" if total_files > 0 else "   æˆåŠŸç‡: 0%")
        
        return md_files

    def _extract_file_info(self, filepath: str) -> MDFileInfo:
        """æå–å–®å€‹MDæª”æ¡ˆçš„è³‡è¨Š"""
        filename = os.path.basename(filepath)
        file_stat = os.stat(filepath)
        file_size = file_stat.st_size
        file_mtime = datetime.fromtimestamp(file_stat.st_mtime)
        
        # å¾æª”æ¡ˆåæå–åŸºæœ¬è³‡è¨Š
        company_code, company_name = self._parse_filename(filename)
        
        # æå–MDæ—¥æœŸï¼ˆè²¡å‹™æ•¸æ“šç™¼å¸ƒæ—¥æœŸï¼‰
        md_date = None
        extracted_date = None
        quality_score = None
        date_extraction_method = "failed"
        
        try:
            if self.parser_available:
                # ä½¿ç”¨ç¾æœ‰çš„MDè§£æå™¨ï¼ˆç¢ºä¿ä¸€è‡´æ€§ï¼‰
                md_date, extracted_date, quality_score, date_extraction_method = self._extract_dates_with_parser(filepath)
            else:
                # é™ç´šåˆ°ç°¡åŒ–æ—¥æœŸæå–
                md_date, extracted_date, date_extraction_method = self._extract_dates_fallback(filepath)
            
        except Exception as e:
            print(f"âš ï¸ æ—¥æœŸæå–å¤±æ•— {filename}: {e}")
        
        # è¨ˆç®—æª”æ¡ˆå¹´é½¡
        age_days = 0
        if md_date:
            age_days = (datetime.now() - md_date).days
        elif extracted_date:
            age_days = (datetime.now() - extracted_date).days
        
        return MDFileInfo(
            filepath=filepath,
            filename=filename,
            file_size=file_size,
            file_mtime=file_mtime,
            md_date=md_date,
            extracted_date=extracted_date,
            quality_score=quality_score,
            company_code=company_code,
            company_name=company_name,
            age_days=age_days,
            deletion_candidate=False,  # ç¨å¾Œç¢ºå®š
            preservation_reason=None,
            date_extraction_method=date_extraction_method
        )

    def _extract_dates_with_parser(self, filepath: str) -> Tuple[Optional[datetime], Optional[datetime], Optional[float], str]:
        """ä½¿ç”¨ç¾æœ‰MDè§£æå™¨æå–æ—¥æœŸï¼ˆç¢ºä¿ä¸€è‡´æ€§ï¼‰"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ä½¿ç”¨ç¾æœ‰çš„bulletproofæ—¥æœŸæå–é‚è¼¯
            content_date_str = self.md_parser._extract_content_date_bulletproof(content)
            md_date = None
            if content_date_str:
                try:
                    # è§£ææ—¥æœŸå­—ç¬¦ä¸² (æ ¼å¼: YYYY/M/D)
                    md_date = datetime.strptime(content_date_str, '%Y/%m/%d')
                except ValueError as e:
                    print(f"âš ï¸ æ—¥æœŸæ ¼å¼è§£æå¤±æ•— {content_date_str}: {e}")
            
            # æå–YAMLä¸­çš„extracted_date
            yaml_data = self.md_parser._extract_yaml_frontmatter_enhanced(content)
            extracted_date = None
            if yaml_data.get('extracted_date'):
                try:
                    extracted_date_str = yaml_data['extracted_date']
                    # è™•ç†ISOæ ¼å¼æ™‚é–“æˆ³
                    if 'T' in extracted_date_str:
                        extracted_date = datetime.fromisoformat(extracted_date_str.replace('T', ' ').split('.')[0])
                    else:
                        extracted_date = datetime.fromisoformat(extracted_date_str)
                except (ValueError, TypeError):
                    pass
            
            # æå–è³ªé‡è©•åˆ†
            quality_score = yaml_data.get('quality_score')
            if quality_score is not None:
                try:
                    quality_score = float(quality_score)
                except (ValueError, TypeError):
                    quality_score = None
            
            # ç¢ºå®šæ—¥æœŸæå–æ–¹æ³•
            if md_date:
                method = "content_bulletproof"
            elif extracted_date:
                method = "yaml_fallback"
            else:
                method = "no_date_found"
            
            return md_date, extracted_date, quality_score, method
            
        except Exception as e:
            print(f"âš ï¸ è§£æå™¨æå–å¤±æ•—: {e}")
            return None, None, None, "parser_error"

    def _extract_dates_fallback(self, filepath: str) -> Tuple[Optional[datetime], Optional[datetime], str]:
        """ç°¡åŒ–çš„æ—¥æœŸæå–é‚è¼¯ï¼ˆé™ç´šæ–¹æ¡ˆï¼‰"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ç°¡åŒ–çš„å…§å®¹æ—¥æœŸåŒ¹é…
            import re
            date_patterns = [
                r'é‰…äº¨ç¶²æ–°èä¸­å¿ƒ\s*(\d{4})-(\d{1,2})-(\d{1,2})\s+\d{1,2}:\d{1,2}',
                r'(\d{4})-(\d{1,2})-(\d{1,2})\s+\d{1,2}:\d{1,2}',
                r'(\d{4})/(\d{1,2})/(\d{1,2})'
            ]
            
            md_date = None
            for pattern in date_patterns:
                matches = re.findall(pattern, content)
                if matches:
                    try:
                        year, month, day = matches[0]
                        md_date = datetime(int(year), int(month), int(day))
                        break
                    except ValueError:
                        continue
            
            # æå–YAMLä¸­çš„extracted_date
            extracted_date = None
            if content.startswith('---'):
                end_pos = content.find('---', 3)
                if end_pos != -1:
                    yaml_content = content[3:end_pos]
                    extracted_match = re.search(r'extracted_date:\s*([^\n]+)', yaml_content)
                    if extracted_match:
                        try:
                            extracted_date_str = extracted_match.group(1).strip()
                            if 'T' in extracted_date_str:
                                extracted_date = datetime.fromisoformat(extracted_date_str.replace('T', ' ').split('.')[0])
                        except ValueError:
                            pass
            
            if md_date:
                method = "content_simple"
            elif extracted_date:
                method = "yaml_simple"
            else:
                method = "no_date_fallback"
            
            return md_date, extracted_date, method
            
        except Exception:
            return None, None, "fallback_error"

    def _parse_filename(self, filename: str) -> Tuple[str, str]:
        """å¾æª”æ¡ˆåè§£æå…¬å¸ä»£ç¢¼å’Œåç¨±"""
        name_without_ext = filename.replace('.md', '')
        parts = name_without_ext.split('_')
        
        company_code = ""
        company_name = ""
        
        if len(parts) >= 2:
            if parts[0].isdigit() and len(parts[0]) == 4:
                company_code = parts[0]
                company_name = parts[1]
        
        return company_code, company_name

    def analyze_files_for_cleanup(self, md_files: List[MDFileInfo], 
                                 retention_days: int = 90, 
                                 quality_threshold: float = 8) -> CleanupPlan:
        """åˆ†ææª”æ¡ˆä¸¦ç”Ÿæˆæ¸…ç†è¨ˆåŠƒ"""
        print(f"ğŸ“‹ åˆ†ææª”æ¡ˆæ¸…ç†è¨ˆåŠƒ...")
        print(f"   ä¿ç•™æœŸé™: {retention_days} å¤©")
        print(f"   è³ªé‡é–¾å€¼: {quality_threshold}")
        
        deletion_candidates = []
        preserved_files = []
        no_date_files = []
        warnings = []
        errors = []
        
        for file_info in md_files:
            should_delete, reason = self._should_delete_file(
                file_info, retention_days, quality_threshold
            )
            
            if file_info.md_date is None and file_info.extracted_date is None:
                no_date_files.append(file_info)
                if self.config['aggressive_deletion']:
                    file_info.deletion_candidate = True
                    file_info.preservation_reason = "ç„¡æ—¥æœŸè³‡è¨Šï¼Œæ¨™è¨˜åˆªé™¤"
                    deletion_candidates.append(file_info)
                else:
                    file_info.deletion_candidate = False
                    file_info.preservation_reason = "ç„¡æ—¥æœŸè³‡è¨Šï¼Œä¿å®ˆä¿ç•™"
                    preserved_files.append(file_info)
            elif should_delete:
                file_info.deletion_candidate = True
                file_info.preservation_reason = reason
                deletion_candidates.append(file_info)
            else:
                file_info.deletion_candidate = False
                file_info.preservation_reason = reason
                preserved_files.append(file_info)
        
        # å®‰å…¨æª¢æŸ¥
        total_files = len(md_files)
        deletion_percentage = len(deletion_candidates) / total_files if total_files > 0 else 0
        files_remaining = total_files - len(deletion_candidates)
        
        safety_checks_passed = True
        
        if deletion_percentage > self.config['safety_thresholds']['max_deletion_percentage']:
            safety_checks_passed = False
            warnings.append(f"åˆªé™¤æ¯”ä¾‹éé«˜: {deletion_percentage:.1%} > {self.config['safety_thresholds']['max_deletion_percentage']:.1%}")
        
        if files_remaining < self.config['safety_thresholds']['minimum_files_remaining']:
            safety_checks_passed = False
            warnings.append(f"å‰©é¤˜æª”æ¡ˆéå°‘: {files_remaining} < {self.config['safety_thresholds']['minimum_files_remaining']}")
        
        if len(no_date_files) > total_files * 0.5:
            warnings.append(f"ç„¡æ—¥æœŸæª”æ¡ˆæ¯”ä¾‹éé«˜: {len(no_date_files)}/{total_files}")
        
        # è¨ˆç®—é ä¼°ç¯€çœç©ºé–“
        estimated_space_saved = sum(f.file_size for f in deletion_candidates)
        
        plan = CleanupPlan(
            total_files=total_files,
            deletion_candidates=deletion_candidates,
            preserved_files=preserved_files,
            no_date_files=no_date_files,
            estimated_space_saved=estimated_space_saved,
            safety_checks_passed=safety_checks_passed,
            warnings=warnings,
            errors=errors
        )
        
        self._print_cleanup_analysis(plan, retention_days, quality_threshold)
        return plan

    def _should_delete_file(self, file_info: MDFileInfo, 
                          retention_days: int, 
                          quality_threshold: float) -> Tuple[bool, str]:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²åˆªé™¤æª”æ¡ˆ"""
        
        # å¦‚æœæ²’æœ‰ä»»ä½•æ—¥æœŸè³‡è¨Š
        if not file_info.md_date and not file_info.extracted_date:
            if self.config['aggressive_deletion']:
                return True, f"ç„¡æ³•æå–æ—¥æœŸè³‡è¨Šï¼Œæ¨™è¨˜åˆªé™¤"
            else:
                return False, f"ç„¡æ—¥æœŸè³‡è¨Šä½†ä¿å®ˆä¿ç•™"
        
        # å„ªå…ˆä½¿ç”¨MDæ—¥æœŸï¼ˆè²¡å‹™æ•¸æ“šç™¼å¸ƒæ—¥æœŸï¼‰
        reference_date = file_info.md_date or file_info.extracted_date
        age_days = (datetime.now() - reference_date).days
        
        # åŸºæœ¬ä¿ç•™æœŸæª¢æŸ¥
        if age_days <= retention_days:
            return False, f"æª”æ¡ˆè¼ƒæ–° ({age_days}å¤© â‰¤ {retention_days}å¤©)"
        
        # é«˜è³ªé‡æª”æ¡ˆå»¶é•·ä¿ç•™æœŸ
        if (file_info.quality_score is not None and 
            file_info.quality_score >= quality_threshold and 
            age_days <= retention_days + self.config['extended_retention_days']):
            return False, f"é«˜è³ªé‡æª”æ¡ˆå»¶é•·ä¿ç•™ (è³ªé‡:{file_info.quality_score}, {age_days}å¤©)"
        
        # æ¨™è¨˜åˆªé™¤
        date_source = "MDæ—¥æœŸ" if file_info.md_date else "æå–æ—¥æœŸ"
        return True, f"è¶…éä¿ç•™æœŸ ({date_source}: {age_days}å¤© > {retention_days}å¤©)"

    def _print_cleanup_analysis(self, plan: CleanupPlan, retention_days: int, quality_threshold: float):
        """æ‰“å°æ¸…ç†åˆ†æçµæœ"""
        print(f"\nğŸ“Š æ¸…ç†åˆ†æçµæœ:")
        print(f"   ç¸½æª”æ¡ˆæ•¸: {plan.total_files}")
        print(f"   åˆªé™¤å€™é¸: {len(plan.deletion_candidates)}")
        print(f"   ä¿ç•™æª”æ¡ˆ: {len(plan.preserved_files)}")
        print(f"   ç„¡æ—¥æœŸæª”æ¡ˆ: {len(plan.no_date_files)}")
        print(f"   é ä¼°ç¯€çœç©ºé–“: {self._format_size(plan.estimated_space_saved)}")
        
        if plan.warnings:
            print(f"\nâš ï¸  è­¦å‘Š:")
            for warning in plan.warnings:
                print(f"   - {warning}")
        
        if plan.errors:
            print(f"\nâŒ éŒ¯èª¤:")
            for error in plan.errors:
                print(f"   - {error}")
        
        print(f"\nâœ… å®‰å…¨æª¢æŸ¥: {'é€šé' if plan.safety_checks_passed else 'æœªé€šé'}")
        
        # é¡¯ç¤ºåˆªé™¤å€™é¸æª”æ¡ˆç¤ºä¾‹
        if plan.deletion_candidates:
            print(f"\nğŸ—‘ï¸  åˆªé™¤å€™é¸ç¤ºä¾‹ (å‰5å€‹):")
            for i, file_info in enumerate(plan.deletion_candidates[:5]):
                date_str = file_info.md_date.strftime('%Y-%m-%d') if file_info.md_date else 'ç„¡æ—¥æœŸ'
                print(f"   {i+1}. {file_info.filename} - {date_str} ({file_info.age_days}å¤©)")

    def execute_cleanup(self, plan: CleanupPlan, dry_run: bool = True, 
                       create_backup: bool = True) -> CleanupResult:
        """åŸ·è¡Œæ¸…ç†è¨ˆåŠƒ"""
        start_time = datetime.now()
        
        if dry_run:
            print(f"ğŸ” åŸ·è¡Œæ¸…ç†è¨ˆåŠƒ (é è¦½æ¨¡å¼)")
        else:
            print(f"ğŸ—‘ï¸  åŸ·è¡Œæ¸…ç†è¨ˆåŠƒ (å¯¦éš›åˆªé™¤)")
        
        if not plan.safety_checks_passed and not dry_run:
            print(f"âŒ å®‰å…¨æª¢æŸ¥æœªé€šéï¼Œæ‹’çµ•åŸ·è¡Œå¯¦éš›åˆªé™¤")
            print(f"ğŸ’¡ ä½¿ç”¨ --force åƒæ•¸å¼·åˆ¶åŸ·è¡Œï¼Œæˆ–èª¿æ•´ä¿ç•™ç­–ç•¥")
            return CleanupResult(
                files_deleted=0,
                files_preserved=plan.total_files,
                files_backed_up=0,
                space_freed=0,
                errors=["å®‰å…¨æª¢æŸ¥æœªé€šé"],
                backup_location=None,
                execution_time=0,
                dry_run=dry_run
            )
        
        backup_location = None
        files_deleted = 0
        files_backed_up = 0
        space_freed = 0
        errors = []
        
        # å‰µå»ºå‚™ä»½
        if create_backup and not dry_run and plan.deletion_candidates:
            try:
                backup_location = self._create_backup(plan.deletion_candidates)
                files_backed_up = len(plan.deletion_candidates)
                print(f"ğŸ’¾ å‚™ä»½å·²å‰µå»º: {backup_location}")
            except Exception as e:
                error_msg = f"å‚™ä»½å‰µå»ºå¤±æ•—: {e}"
                errors.append(error_msg)
                print(f"âŒ {error_msg}")
                if not dry_run:
                    print(f"âš ï¸  ç„¡å‚™ä»½ç‹€æ…‹ä¸‹ä¸åŸ·è¡Œåˆªé™¤")
                    return CleanupResult(
                        files_deleted=0,
                        files_preserved=plan.total_files,
                        files_backed_up=0,
                        space_freed=0,
                        errors=errors,
                        backup_location=None,
                        execution_time=(datetime.now() - start_time).total_seconds(),
                        dry_run=dry_run
                    )
        
        # åŸ·è¡Œåˆªé™¤
        for file_info in plan.deletion_candidates:
            try:
                if dry_run:
                    print(f"ğŸ” [é è¦½] å°‡åˆªé™¤: {file_info.filename} ({self._format_size(file_info.file_size)})")
                else:
                    os.remove(file_info.filepath)
                    print(f"ğŸ—‘ï¸  å·²åˆªé™¤: {file_info.filename} ({self._format_size(file_info.file_size)})")
                
                files_deleted += 1
                space_freed += file_info.file_size
                
            except Exception as e:
                error_msg = f"åˆªé™¤å¤±æ•— {file_info.filename}: {e}"
                errors.append(error_msg)
                print(f"âŒ {error_msg}")
        
        execution_time = (datetime.now() - start_time).total_seconds()
        
        result = CleanupResult(
            files_deleted=files_deleted,
            files_preserved=len(plan.preserved_files),
            files_backed_up=files_backed_up,
            space_freed=space_freed,
            errors=errors,
            backup_location=backup_location,
            execution_time=execution_time,
            dry_run=dry_run
        )
        
        self._print_cleanup_result(result)
        return result

    def _create_backup(self, deletion_candidates: List[MDFileInfo]) -> str:
        """å‰µå»ºåˆªé™¤æª”æ¡ˆçš„å‚™ä»½"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_dir = os.path.join(
            os.path.abspath(self.config['backup']['archive_path']),
            f"cleanup_backup_{timestamp}"
        )
        
        os.makedirs(backup_dir, exist_ok=True)
        
        # è¤‡è£½æª”æ¡ˆåˆ°å‚™ä»½ç›®éŒ„
        for file_info in deletion_candidates:
            backup_path = os.path.join(backup_dir, file_info.filename)
            shutil.copy2(file_info.filepath, backup_path)
        
        # å‰µå»ºå‚™ä»½æ¸…å–®
        manifest = {
            'backup_timestamp': timestamp,
            'total_files': len(deletion_candidates),
            'files': [
                {
                    'filename': f.filename,
                    'original_path': f.filepath,
                    'company_code': f.company_code,
                    'company_name': f.company_name,
                    'md_date': f.md_date.isoformat() if f.md_date else None,
                    'file_size': f.file_size,
                    'age_days': f.age_days,
                    'preservation_reason': f.preservation_reason
                }
                for f in deletion_candidates
            ],
            'total_size': sum(f.file_size for f in deletion_candidates),
            'cleaner_version': self.version
        }
        
        manifest_path = os.path.join(backup_dir, 'backup_manifest.json')
        with open(manifest_path, 'w', encoding='utf-8') as f:
            json.dump(manifest, f, ensure_ascii=False, indent=2)
        
        return backup_dir

    def _print_cleanup_result(self, result: CleanupResult):
        """æ‰“å°æ¸…ç†çµæœ"""
        mode = "é è¦½æ¨¡å¼" if result.dry_run else "å¯¦éš›åŸ·è¡Œ"
        print(f"\nğŸ“‹ æ¸…ç†çµæœ ({mode}):")
        print(f"   åˆªé™¤æª”æ¡ˆ: {result.files_deleted}")
        print(f"   ä¿ç•™æª”æ¡ˆ: {result.files_preserved}")
        print(f"   å‚™ä»½æª”æ¡ˆ: {result.files_backed_up}")
        print(f"   é‡‹æ”¾ç©ºé–“: {self._format_size(result.space_freed)}")
        print(f"   åŸ·è¡Œæ™‚é–“: {result.execution_time:.2f}ç§’")
        
        if result.backup_location:
            print(f"   å‚™ä»½ä½ç½®: {result.backup_location}")
        
        if result.errors:
            print(f"\nâŒ éŒ¯èª¤ ({len(result.errors)}):")
            for error in result.errors:
                print(f"   - {error}")
        
        if not result.dry_run and result.files_deleted > 0:
            print(f"\nâœ… æ¸…ç†å®Œæˆï¼åˆªé™¤äº† {result.files_deleted} å€‹æª”æ¡ˆ")

    def generate_cleanup_report(self, md_files: List[MDFileInfo], 
                               plan: CleanupPlan, 
                               result: Optional[CleanupResult] = None) -> Dict[str, Any]:
        """ç”Ÿæˆæ¸…ç†å ±å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'cleaner_version': self.version,
            'configuration': {
                'md_directory': self.md_dir,
                'retention_days': self.config['default_retention_days'],
                'quality_threshold': self.config['quality_threshold'],
                'aggressive_deletion': self.config['aggressive_deletion']
            },
            'analysis': {
                'total_files': plan.total_files,
                'deletion_candidates': len(plan.deletion_candidates),
                'preserved_files': len(plan.preserved_files),
                'no_date_files': len(plan.no_date_files),
                'estimated_space_saved': plan.estimated_space_saved,
                'safety_checks_passed': plan.safety_checks_passed,
                'warnings': plan.warnings,
                'errors': plan.errors
            }
        }
        
        if result:
            report['execution'] = {
                'dry_run': result.dry_run,
                'files_deleted': result.files_deleted,
                'files_backed_up': result.files_backed_up,
                'space_freed': result.space_freed,
                'execution_time': result.execution_time,
                'backup_location': result.backup_location,
                'errors': result.errors
            }
        
        # æ·»åŠ æª”æ¡ˆè©³æƒ…
        report['file_details'] = {
            'by_age_group': self._group_files_by_age(md_files),
            'by_quality_score': self._group_files_by_quality(md_files),
            'by_company': self._group_files_by_company(md_files)
        }
        
        return report

    def _group_files_by_age(self, md_files: List[MDFileInfo]) -> Dict[str, int]:
        """æŒ‰å¹´é½¡åˆ†çµ„æª”æ¡ˆ"""
        groups = {
            '0-30å¤©': 0,
            '31-60å¤©': 0,
            '61-90å¤©': 0,
            '91-180å¤©': 0,
            '181-365å¤©': 0,
            '365å¤©ä»¥ä¸Š': 0,
            'ç„¡æ—¥æœŸ': 0
        }
        
        for file_info in md_files:
            if file_info.age_days == 0 and not file_info.md_date:
                groups['ç„¡æ—¥æœŸ'] += 1
            elif file_info.age_days <= 30:
                groups['0-30å¤©'] += 1
            elif file_info.age_days <= 60:
                groups['31-60å¤©'] += 1
            elif file_info.age_days <= 90:
                groups['61-90å¤©'] += 1
            elif file_info.age_days <= 180:
                groups['91-180å¤©'] += 1
            elif file_info.age_days <= 365:
                groups['181-365å¤©'] += 1
            else:
                groups['365å¤©ä»¥ä¸Š'] += 1
        
        return groups

    def _group_files_by_quality(self, md_files: List[MDFileInfo]) -> Dict[str, int]:
        """æŒ‰è³ªé‡è©•åˆ†åˆ†çµ„æª”æ¡ˆ"""
        groups = {
            'å„ªç§€(9-10)': 0,
            'è‰¯å¥½(7-8)': 0,
            'æ™®é€š(4-6)': 0,
            'è¼ƒå·®(1-3)': 0,
            'ç„¡è©•åˆ†': 0
        }
        
        for file_info in md_files:
            if file_info.quality_score is None:
                groups['ç„¡è©•åˆ†'] += 1
            elif file_info.quality_score >= 9:
                groups['å„ªç§€(9-10)'] += 1
            elif file_info.quality_score >= 7:
                groups['è‰¯å¥½(7-8)'] += 1
            elif file_info.quality_score >= 4:
                groups['æ™®é€š(4-6)'] += 1
            else:
                groups['è¼ƒå·®(1-3)'] += 1
        
        return groups

    def _group_files_by_company(self, md_files: List[MDFileInfo]) -> Dict[str, int]:
        """æŒ‰å…¬å¸åˆ†çµ„æª”æ¡ˆè¨ˆæ•¸"""
        company_counts = {}
        
        for file_info in md_files:
            key = f"{file_info.company_code}_{file_info.company_name}" if file_info.company_code else "Unknown"
            company_counts[key] = company_counts.get(key, 0) + 1
        
        # è¿”å›å‰10å€‹æª”æ¡ˆæ•¸æœ€å¤šçš„å…¬å¸
        sorted_companies = sorted(company_counts.items(), key=lambda x: x[1], reverse=True)
        return dict(sorted_companies[:10])

    def _format_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æª”æ¡ˆå¤§å°"""
        if size_bytes == 0:
            return "0 B"
        
        units = ['B', 'KB', 'MB', 'GB']
        unit_index = 0
        size = float(size_bytes)
        
        while size >= 1024 and unit_index < len(units) - 1:
            size /= 1024
            unit_index += 1
        
        return f"{size:.2f} {units[unit_index]}"

    def get_statistics(self) -> Dict[str, Any]:
        """ç²å–MDæª”æ¡ˆç›®éŒ„çµ±è¨ˆè³‡è¨Š"""
        md_files = self.scan_md_files()
        
        if not md_files:
            return {
                'total_files': 0,
                'total_size': 0,
                'date_extraction_success_rate': 0,
                'message': 'No MD files found'
            }
        
        total_size = sum(f.file_size for f in md_files)
        files_with_dates = len([f for f in md_files if f.md_date])
        success_rate = files_with_dates / len(md_files) * 100
        
        # å¹´é½¡çµ±è¨ˆ
        ages = [f.age_days for f in md_files if f.md_date]
        age_stats = {}
        if ages:
            age_stats = {
                'average_age_days': statistics.mean(ages),
                'median_age_days': statistics.median(ages),
                'oldest_file_days': max(ages),
                'newest_file_days': min(ages)
            }
        
        # è³ªé‡çµ±è¨ˆ
        quality_scores = [f.quality_score for f in md_files if f.quality_score is not None]
        quality_stats = {}
        if quality_scores:
            quality_stats = {
                'average_quality': statistics.mean(quality_scores),
                'median_quality': statistics.median(quality_scores),
                'highest_quality': max(quality_scores),
                'lowest_quality': min(quality_scores)
            }
        
        return {
            'total_files': len(md_files),
            'total_size': total_size,
            'total_size_formatted': self._format_size(total_size),
            'files_with_md_dates': files_with_dates,
            'date_extraction_success_rate': success_rate,
            'age_statistics': age_stats,
            'quality_statistics': quality_stats,
            'age_distribution': self._group_files_by_age(md_files),
            'quality_distribution': self._group_files_by_quality(md_files),
            'top_companies': self._group_files_by_company(md_files),
            'cleaner_version': self.version,
            'parser_available': self.parser_available
        }


# æ¸¬è©¦åŠŸèƒ½
if __name__ == "__main__":
    print("=== MDæª”æ¡ˆæ¸…ç†ç®¡ç†å™¨ v3.6.1 æ¸¬è©¦ ===")
    
    # åˆå§‹åŒ–æ¸…ç†ç®¡ç†å™¨
    cleaner = MDFileCleanupManager("data/md")
    
    # ç²å–çµ±è¨ˆè³‡è¨Š
    print("\nğŸ“Š ç›®éŒ„çµ±è¨ˆ:")
    stats = cleaner.get_statistics()
    print(f"   ç¸½æª”æ¡ˆæ•¸: {stats['total_files']}")
    print(f"   ç¸½å¤§å°: {stats.get('total_size_formatted', '0 B')}")
    print(f"   æ—¥æœŸæå–æˆåŠŸç‡: {stats.get('date_extraction_success_rate', 0):.1f}%")
    
    if stats['total_files'] > 0:
        # æƒæå’Œåˆ†ææª”æ¡ˆ
        print("\nğŸ” æƒæMDæª”æ¡ˆ...")
        md_files = cleaner.scan_md_files()
        
        if md_files:
            print("\nğŸ“‹ ç”Ÿæˆæ¸…ç†è¨ˆåŠƒ...")
            plan = cleaner.analyze_files_for_cleanup(md_files, retention_days=90, quality_threshold=8)
            
            print("\nğŸ” åŸ·è¡Œé è¦½æ¨¡å¼...")
            result = cleaner.execute_cleanup(plan, dry_run=True, create_backup=False)
            
            print("\nğŸ“„ ç”Ÿæˆæ¸…ç†å ±å‘Š...")
            report = cleaner.generate_cleanup_report(md_files, plan, result)
            
            print(f"\nâœ… æ¸¬è©¦å®Œæˆï¼")
            print(f"   æª¢æ¸¬åˆ° {len(plan.deletion_candidates)} å€‹åˆªé™¤å€™é¸æª”æ¡ˆ")
            print(f"   é ä¼°ç¯€çœç©ºé–“: {cleaner._format_size(plan.estimated_space_saved)}")
            print(f"   å®‰å…¨æª¢æŸ¥: {'é€šé' if plan.safety_checks_passed else 'æœªé€šé'}")
            
        else:
            print("âš ï¸  æœªæ‰¾åˆ°MDæª”æ¡ˆ")
    else:
        print("âš ï¸  MDç›®éŒ„ç‚ºç©ºæˆ–ä¸å­˜åœ¨")
    
    print(f"\nğŸ‰ MDæª”æ¡ˆæ¸…ç†ç®¡ç†å™¨ v{cleaner.version} å°±ç·’ï¼")
    print(f"ğŸ’¡ é›†æˆåˆ° process_cli.py å¾Œå¯ä½¿ç”¨å®Œæ•´åŠŸèƒ½")