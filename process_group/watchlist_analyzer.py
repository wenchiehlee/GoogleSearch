#!/usr/bin/env python3
"""
Watchlist Analyzer - FactSet Pipeline v3.6.1 (Refined)
æ–°å¢è§€å¯Ÿåå–®è™•ç†çµ±è¨ˆåˆ†æåŠŸèƒ½ - å¢å¼·ç‰ˆæœ¬
"""

import os
import re
import pandas as pd
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
from collections import Counter  # ğŸ”§ ä¿®å¾©ï¼šæ·»åŠ  Counter å°å…¥
import statistics
import json

class WatchlistAnalyzer:
    """è§€å¯Ÿåå–®åˆ†æå™¨ - v3.6.1 å¢å¼·ç‰ˆæœ¬"""
    
    def __init__(self):
        """åˆå§‹åŒ–è§€å¯Ÿåå–®åˆ†æå™¨"""
        
        # è¼‰å…¥è§€å¯Ÿåå–®
        self.watchlist_mapping = self._load_watchlist()
        self.validation_enabled = len(self.watchlist_mapping) > 0
        
        # åœç”¨è©
        self.stop_words = {
            'çš„', 'å’Œ', 'èˆ‡', 'æˆ–', 'åŠ', 'åœ¨', 'ç‚º', 'æ˜¯', 'æœ‰', 'æ­¤', 'å°‡', 'æœƒ',
            'and', 'or', 'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for'
        }
        
        # è§€å¯Ÿåå–®ç‹€æ…‹åˆ†é¡
        self.status_categories = {
            'processed': 'å·²è™•ç†',
            'not_found': 'æœªæ‰¾åˆ°MDæª”æ¡ˆ',
            'validation_failed': 'é©—è­‰å¤±æ•—',
            'low_quality': 'å“è³ªéä½',
            'multiple_files': 'å¤šå€‹æª”æ¡ˆ'
        }
        
        # é—œéµå­—åˆ†é¡
        self.keyword_categories = {
            'company': ['å…¬å¸', 'ä¼æ¥­', 'é›†åœ˜', 'corp', 'company', 'group'],
            'financial': ['eps', 'ç‡Ÿæ”¶', 'ç²åˆ©', 'è‚¡åƒ¹', 'ç›®æ¨™åƒ¹', 'revenue', 'profit'],
            'analysis': ['åˆ†æ', 'é ä¼°', 'è©•ä¼°', 'å±•æœ›', 'analysis', 'forecast', 'estimate'],
            'source': ['factset', 'bloomberg', 'åˆ¸å•†', 'æŠ•é¡§', 'analyst']
        }
        
        # å“è³ªè©•åˆ†é–¾å€¼
        self.quality_thresholds = {
            'excellent': 8.0,
            'good': 6.0,
            'average': 4.0,
            'poor': 0.0
        }

    def _load_watchlist(self) -> Dict[str, str]:
        """è¼‰å…¥StockID_TWSE_TPEX.csv - å¢å¼·ç‰ˆæœ¬"""
        mapping = {}
        
        possible_paths = [
            'StockID_TWSE_TPEX.csv',
            '../StockID_TWSE_TPEX.csv',
            '../../StockID_TWSE_TPEX.csv',
            'data/StockID_TWSE_TPEX.csv',
            '../data/StockID_TWSE_TPEX.csv',
            'watchlist.csv',
            '../watchlist.csv'
        ]
        
        for csv_path in possible_paths:
            if os.path.exists(csv_path):
                try:
                    print(f"ğŸ” è¼‰å…¥è§€å¯Ÿåå–®: {csv_path}")
                    
                    # ä½¿ç”¨å¤šç¨®ç·¨ç¢¼å˜—è©¦è®€å–
                    encodings = ['utf-8', 'utf-8-sig', 'big5', 'gbk', 'cp950']
                    df = None
                    
                    for encoding in encodings:
                        try:
                            df = pd.read_csv(csv_path, header=None, names=['code', 'name'], encoding=encoding)
                            print(f"âœ… æˆåŠŸä½¿ç”¨ {encoding} ç·¨ç¢¼è®€å–")
                            break
                        except UnicodeDecodeError:
                            continue
                        except Exception as e:
                            print(f"âš ï¸ ä½¿ç”¨ {encoding} ç·¨ç¢¼è®€å–å¤±æ•—: {e}")
                            continue
                    
                    if df is None:
                        print(f"âŒ ç„¡æ³•ä½¿ç”¨ä»»ä½•ç·¨ç¢¼è®€å– {csv_path}")
                        continue
                    
                    # é©—è­‰å’Œæ¸…ç†æ•¸æ“š
                    valid_count = 0
                    invalid_count = 0
                    duplicate_count = 0
                    
                    for idx, row in df.iterrows():
                        try:
                            code = str(row['code']).strip()
                            name = str(row['name']).strip()
                            
                            # é©—è­‰å…¬å¸ä»£è™Ÿæ ¼å¼
                            if not self._is_valid_company_code(code):
                                invalid_count += 1
                                continue
                                
                            # é©—è­‰å…¬å¸åç¨±
                            if not self._is_valid_company_name(name):
                                invalid_count += 1
                                continue
                            
                            # æª¢æŸ¥é‡è¤‡
                            if code in mapping:
                                duplicate_count += 1
                                print(f"âš ï¸ é‡è¤‡å…¬å¸ä»£è™Ÿ: {code}")
                                continue
                            
                            mapping[code] = name
                            valid_count += 1
                            
                        except Exception as e:
                            invalid_count += 1
                            print(f"âŒ è™•ç†ç¬¬{idx+1}è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                            continue
                    
                    print(f"ğŸ“Š è§€å¯Ÿåå–®è¼‰å…¥çµ±è¨ˆ:")
                    print(f"   æª”æ¡ˆ: {csv_path}")
                    print(f"   ç¸½è¡Œæ•¸: {len(df)}")
                    print(f"   æœ‰æ•ˆæ•¸æ“š: {valid_count}")
                    print(f"   ç„¡æ•ˆæ•¸æ“š: {invalid_count}")
                    print(f"   é‡è¤‡æ•¸æ“š: {duplicate_count}")
                    print(f"   æˆåŠŸç‡: {valid_count/len(df)*100:.1f}%")
                    
                    # é©—è­‰è¼‰å…¥çš„è³‡æ–™å“è³ª
                    self._validate_watchlist_quality(mapping)
                    
                    return mapping
                    
                except Exception as e:
                    print(f"âŒ è®€å–è§€å¯Ÿåå–®å¤±æ•— {csv_path}: {e}")
                    continue
        
        print("âŒ æ‰€æœ‰è§€å¯Ÿåå–®è¼‰å…¥å˜—è©¦å‡å¤±æ•—")
        print("ğŸ’¡ è«‹ç¢ºä¿StockID_TWSE_TPEX.csvæª”æ¡ˆå­˜åœ¨ä¸”æ ¼å¼æ­£ç¢º")
        return {}

    def _validate_watchlist_quality(self, mapping: Dict[str, str]):
        """é©—è­‰è§€å¯Ÿåå–®å“è³ª"""
        if not mapping:
            return
            
        # æª¢æŸ¥ä»£è™Ÿç¯„åœåˆ†å¸ƒ
        code_ranges = {'1000-2999': 0, '3000-5999': 0, '6000-8999': 0, '9000+': 0}
        
        for code in mapping.keys():
            try:
                code_num = int(code)
                if 1000 <= code_num <= 2999:
                    code_ranges['1000-2999'] += 1
                elif 3000 <= code_num <= 5999:
                    code_ranges['3000-5999'] += 1
                elif 6000 <= code_num <= 8999:
                    code_ranges['6000-8999'] += 1
                else:
                    code_ranges['9000+'] += 1
            except ValueError:
                continue
        
        print(f"ğŸ“Š è§€å¯Ÿåå–®ä»£è™Ÿåˆ†å¸ƒ:")
        for range_name, count in code_ranges.items():
            print(f"   {range_name}: {count} å®¶")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰çŸ¥åå…¬å¸
        famous_companies = {
            '2330': 'å°ç©é›»', '2317': 'é´»æµ·', '2454': 'è¯ç™¼ç§‘',
            '2882': 'åœ‹æ³°é‡‘', '2891': 'ä¸­ä¿¡é‡‘', '2412': 'ä¸­è¯é›»'
        }
        
        found_famous = 0
        for code, expected_name in famous_companies.items():
            if code in mapping:
                actual_name = mapping[code]
                if expected_name in actual_name or actual_name in expected_name:
                    found_famous += 1
                    print(f"âœ… æ‰¾åˆ°çŸ¥åå…¬å¸: {code} - {actual_name}")
        
        if found_famous == 0:
            print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•çŸ¥åå…¬å¸ï¼Œè«‹æª¢æŸ¥è§€å¯Ÿåå–®å…§å®¹")

    def _is_valid_company_code(self, code: str) -> bool:
        """é©—è­‰å…¬å¸ä»£è™Ÿæ ¼å¼ - å¢å¼·ç‰ˆæœ¬"""
        if not code or code in ['nan', 'NaN', 'null', 'None', '', 'NULL']:
            return False
        
        clean_code = code.strip().strip('\'"')
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºæ•¸å­—
        if not clean_code.isdigit():
            return False
            
        # æª¢æŸ¥é•·åº¦
        if len(clean_code) != 4:
            return False
        
        # æª¢æŸ¥æ•¸å­—ç¯„åœï¼ˆå°è‚¡ä»£è™Ÿç¯„åœï¼‰
        try:
            code_num = int(clean_code)
            if not (1000 <= code_num <= 9999):
                return False
        except ValueError:
            return False
        
        return True

    def _is_valid_company_name(self, name: str) -> bool:
        """é©—è­‰å…¬å¸åç¨± - å¢å¼·ç‰ˆæœ¬"""
        if not name or name in ['nan', 'NaN', 'null', 'None', '', 'NULL']:
            return False
        
        clean_name = name.strip()
        
        # æª¢æŸ¥é•·åº¦
        if len(clean_name) < 1 or len(clean_name) > 30:
            return False
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«ç„¡æ•ˆå­—ç¬¦
        invalid_chars = ['|', '\t', '\n', '\r', '\x00']
        if any(char in clean_name for char in invalid_chars):
            return False
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºæ˜é¡¯ç„¡æ•ˆçš„åç¨±
        invalid_names = ['test', 'example', 'æ¸¬è©¦', 'ç¯„ä¾‹', '123', 'abc']
        if clean_name.lower() in invalid_names:
            return False
        
        return True

    def analyze_watchlist_coverage(self, processed_companies: List[Dict]) -> Dict[str, Any]:
        """åˆ†æè§€å¯Ÿåå–®è¦†è“‹æƒ…æ³ - å¢å¼·ç‰ˆæœ¬"""
        if not self.validation_enabled:
            return {
                'error': 'è§€å¯Ÿåå–®æœªè¼‰å…¥',
                'total_watchlist_companies': 0,
                'companies_with_md_files': 0,
                'coverage_rate': 0.0,
                'analysis_timestamp': datetime.now().isoformat()
            }
        
        try:
            print("ğŸ“Š é–‹å§‹è§€å¯Ÿåå–®è¦†è“‹åˆ†æ...")
            
            # å»ºç«‹å…¬å¸è™•ç†ç‹€æ…‹å°æ‡‰
            company_processing_status = self.analyze_company_processing_status(processed_companies)
            
            # åŸºæœ¬çµ±è¨ˆ
            total_watchlist = len(self.watchlist_mapping)
            companies_with_files = len([c for c in company_processing_status.values() if c['file_count'] > 0])
            companies_processed = len([c for c in company_processing_status.values() if c['status'] == 'processed'])
            
            coverage_rate = (companies_with_files / total_watchlist) * 100 if total_watchlist > 0 else 0
            success_rate = (companies_processed / total_watchlist) * 100 if total_watchlist > 0 else 0
            
            # ç‹€æ…‹åˆ†ä½ˆçµ±è¨ˆ
            status_counts = {}
            for status_info in company_processing_status.values():
                status = status_info['status']
                status_counts[status] = status_counts.get(status, 0) + 1
            
            # å“è³ªçµ±è¨ˆ
            quality_scores = [c['average_quality_score'] for c in company_processing_status.values() 
                             if c['average_quality_score'] > 0]
            
            quality_stats = self._calculate_quality_statistics(quality_scores)
            
            # æª”æ¡ˆçµ±è¨ˆ
            file_stats = self._calculate_file_statistics(company_processing_status)
            
            # æœå°‹æ¨¡å¼åˆ†æ
            search_pattern_analysis = self.analyze_search_patterns(processed_companies)
            
            # é—œéµå­—æ•ˆæœåˆ†æ
            keyword_effectiveness = self.calculate_keyword_effectiveness_by_company(processed_companies)
            
            # ç¼ºå¤±å…¬å¸åˆ†æ
            missing_companies_analysis = self._analyze_missing_companies(company_processing_status)
            
            analysis_result = {
                'analysis_timestamp': datetime.now().isoformat(),
                'version': '3.6.1',
                'analysis_type': 'watchlist_coverage_enhanced',
                
                # åŸºæœ¬çµ±è¨ˆ
                'total_watchlist_companies': total_watchlist,
                'companies_with_md_files': companies_with_files,
                'companies_processed_successfully': companies_processed,
                'coverage_rate': round(coverage_rate, 1),
                'success_rate': round(success_rate, 1),
                
                # è©³ç´°åˆ†æ
                'company_status_summary': status_counts,
                'company_processing_status': company_processing_status,
                'quality_statistics': quality_stats,
                'file_statistics': file_stats,
                'search_pattern_analysis': search_pattern_analysis,
                'keyword_effectiveness_analysis': keyword_effectiveness,
                'missing_companies_analysis': missing_companies_analysis,
                
                # å¥åº·åº¦æŒ‡æ¨™
                'health_indicators': self._calculate_health_indicators(
                    total_watchlist, companies_processed, quality_scores, coverage_rate
                )
            }
            
            print(f"âœ… è§€å¯Ÿåå–®è¦†è“‹åˆ†æå®Œæˆ:")
            print(f"   è§€å¯Ÿåå–®å…¬å¸æ•¸: {total_watchlist}")
            print(f"   æœ‰MDæª”æ¡ˆå…¬å¸æ•¸: {companies_with_files}")
            print(f"   è¦†è“‹ç‡: {coverage_rate:.1f}%")
            print(f"   æˆåŠŸè™•ç†ç‡: {success_rate:.1f}%")
            
            return analysis_result
            
        except Exception as e:
            print(f"âŒ è§€å¯Ÿåå–®è¦†è“‹åˆ†æå¤±æ•—: {e}")
            return {
                'error': str(e),
                'analysis_timestamp': datetime.now().isoformat(),
                'total_watchlist_companies': len(self.watchlist_mapping) if self.watchlist_mapping else 0
            }

    def analyze_company_processing_status(self, processed_companies: List[Dict]) -> Dict[str, Dict]:
        """åˆ†ææ¯å®¶å…¬å¸çš„è™•ç†ç‹€æ…‹ - å¢å¼·ç‰ˆæœ¬"""
        company_status = {}
        
        # åˆå§‹åŒ–æ‰€æœ‰è§€å¯Ÿåå–®å…¬å¸
        for company_code, company_name in self.watchlist_mapping.items():
            company_status[company_code] = {
                'company_name': company_name,
                'file_count': 0,
                'files': [],
                'status': 'not_found',
                'average_quality_score': 0.0,
                'max_quality_score': 0.0,
                'min_quality_score': 0.0,
                'search_keywords': [],
                'latest_file_date': '',
                'earliest_file_date': '',
                'validation_passed': True,
                'validation_errors': [],
                'data_sources': set(),
                'analysis_coverage': {
                    'has_eps_data': False,
                    'has_target_price': False,
                    'has_analyst_info': False
                }
            }
        
        # è™•ç†å·²æœ‰è³‡æ–™çš„å…¬å¸
        for company_data in processed_companies:
            company_code = company_data.get('company_code', '')
            
            if company_code in company_status:
                company_status[company_code]['files'].append(company_data)
                company_status[company_code]['file_count'] += 1
                
                # æ”¶é›†æœå°‹é—œéµå­—
                search_keywords = company_data.get('search_keywords', [])
                if search_keywords:
                    existing_keywords = set(company_status[company_code]['search_keywords'])
                    new_keywords = set(search_keywords)
                    company_status[company_code]['search_keywords'] = list(existing_keywords | new_keywords)
                
                # æ›´æ–°æ—¥æœŸç¯„åœ
                content_date = company_data.get('content_date', '')
                if content_date:
                    if not company_status[company_code]['latest_file_date'] or content_date > company_status[company_code]['latest_file_date']:
                        company_status[company_code]['latest_file_date'] = content_date
                    
                    if not company_status[company_code]['earliest_file_date'] or content_date < company_status[company_code]['earliest_file_date']:
                        company_status[company_code]['earliest_file_date'] = content_date
                
                # æ”¶é›†è³‡æ–™ä¾†æº
                data_source = company_data.get('data_source', '')
                if data_source:
                    company_status[company_code]['data_sources'].add(data_source)
                
                # åˆ†ææ•¸æ“šè¦†è“‹åº¦
                analysis_coverage = company_status[company_code]['analysis_coverage']
                if company_data.get('has_eps_data', False):
                    analysis_coverage['has_eps_data'] = True
                if company_data.get('has_target_price', False):
                    analysis_coverage['has_target_price'] = True
                if company_data.get('analyst_count', 0) > 0:
                    analysis_coverage['has_analyst_info'] = True
                
                # æ”¶é›†é©—è­‰ç‹€æ…‹
                if not company_data.get('content_validation_passed', True):
                    company_status[company_code]['validation_passed'] = False
                    validation_errors = company_data.get('validation_errors', [])
                    company_status[company_code]['validation_errors'].extend(validation_errors)
        
        # è¨ˆç®—æ¯å®¶å…¬å¸çš„ç‹€æ…‹å’Œå“è³ªçµ±è¨ˆ
        for company_code, status_info in company_status.items():
            files = status_info['files']
            
            if files:
                # è¨ˆç®—å“è³ªçµ±è¨ˆ
                quality_scores = [f.get('quality_score', 0) for f in files]
                status_info['average_quality_score'] = round(statistics.mean(quality_scores), 2)
                status_info['max_quality_score'] = max(quality_scores)
                status_info['min_quality_score'] = min(quality_scores)
                
                # ç¢ºå®šè™•ç†ç‹€æ…‹
                status_info['status'] = self._categorize_company_status(company_code, files)
                
                # è½‰æ›è³‡æ–™ä¾†æºé›†åˆç‚ºåˆ—è¡¨
                status_info['data_sources'] = list(status_info['data_sources'])
            else:
                status_info['status'] = 'not_found'
                status_info['data_sources'] = []
        
        return company_status

    def _calculate_quality_statistics(self, quality_scores: List[float]) -> Dict[str, Any]:
        """è¨ˆç®—å“è³ªçµ±è¨ˆ"""
        if not quality_scores:
            return {
                'average_quality_score': 0,
                'median_quality_score': 0,
                'companies_above_8': 0,
                'companies_above_6': 0,
                'companies_below_3': 0,
                'zero_score_companies': 0,
                'quality_distribution': {}
            }
        
        quality_distribution = {
            'excellent': len([s for s in quality_scores if s >= self.quality_thresholds['excellent']]),
            'good': len([s for s in quality_scores if self.quality_thresholds['good'] <= s < self.quality_thresholds['excellent']]),
            'average': len([s for s in quality_scores if self.quality_thresholds['average'] <= s < self.quality_thresholds['good']]),
            'poor': len([s for s in quality_scores if s < self.quality_thresholds['average']])
        }
        
        return {
            'average_quality_score': round(statistics.mean(quality_scores), 2),
            'median_quality_score': round(statistics.median(quality_scores), 2),
            'companies_above_8': len([s for s in quality_scores if s >= 8.0]),
            'companies_above_6': len([s for s in quality_scores if s >= 6.0]),
            'companies_below_3': len([s for s in quality_scores if s <= 3.0]),
            'zero_score_companies': len([s for s in quality_scores if s == 0]),
            'quality_distribution': quality_distribution,
            'quality_std': round(statistics.stdev(quality_scores), 2) if len(quality_scores) > 1 else 0
        }

    def _calculate_file_statistics(self, company_processing_status: Dict) -> Dict[str, Any]:
        """è¨ˆç®—æª”æ¡ˆçµ±è¨ˆ"""
        file_counts = [c['file_count'] for c in company_processing_status.values() if c['file_count'] > 0]
        companies_with_multiple = len([c for c in company_processing_status.values() if c['file_count'] > 1])
        
        # æ‰¾å‡ºæª”æ¡ˆæœ€å¤šçš„å…¬å¸
        max_files_company = max(company_processing_status.items(), 
                              key=lambda x: x[1]['file_count'], default=(None, {'file_count': 0}))
        
        # è³‡æ–™ä¾†æºçµ±è¨ˆ
        all_sources = set()
        for status_info in company_processing_status.values():
            all_sources.update(status_info.get('data_sources', []))
        
        return {
            'total_md_files': sum(file_counts),
            'files_per_company_avg': round(statistics.mean(file_counts), 2) if file_counts else 0,
            'companies_with_multiple_files': companies_with_multiple,
            'most_files_company': max_files_company[1].get('company_name', ''),
            'most_files_count': max_files_company[1]['file_count'],
            'unique_data_sources': list(all_sources),
            'data_source_count': len(all_sources)
        }

    def _analyze_missing_companies(self, company_processing_status: Dict) -> Dict[str, Any]:
        """åˆ†æç¼ºå¤±å…¬å¸"""
        missing_companies = []
        low_quality_companies = []
        
        for company_code, status_info in company_processing_status.items():
            if status_info['status'] == 'not_found':
                missing_companies.append({
                    'company_code': company_code,
                    'company_name': status_info['company_name'],
                    'priority': self._calculate_missing_priority(company_code, status_info['company_name'])
                })
            elif status_info['status'] == 'low_quality':
                low_quality_companies.append({
                    'company_code': company_code,
                    'company_name': status_info['company_name'],
                    'average_quality': status_info['average_quality_score']
                })
        
        # æŒ‰å„ªå…ˆç´šæ’åº
        missing_companies.sort(key=lambda x: x['priority'], reverse=True)
        low_quality_companies.sort(key=lambda x: x['average_quality'])
        
        return {
            'missing_companies': missing_companies,
            'missing_count': len(missing_companies),
            'low_quality_companies': low_quality_companies,
            'low_quality_count': len(low_quality_companies),
            'improvement_suggestions': self._generate_improvement_suggestions(missing_companies, low_quality_companies)
        }

    def _calculate_health_indicators(self, total_companies: int, processed_companies: int, 
                                   quality_scores: List[float], coverage_rate: float) -> Dict[str, Any]:
        """è¨ˆç®—å¥åº·åº¦æŒ‡æ¨™"""
        if total_companies == 0:
            return {'overall_health': 0, 'health_grade': 'F'}
        
        # è¦†è“‹ç‡å¥åº·åº¦ (40%)
        coverage_health = coverage_rate / 100 * 40
        
        # è™•ç†æˆåŠŸç‡å¥åº·åº¦ (30%)
        success_rate = (processed_companies / total_companies) * 100
        success_health = success_rate / 100 * 30
        
        # å“è³ªå¥åº·åº¦ (30%)
        if quality_scores:
            avg_quality = statistics.mean(quality_scores)
            quality_health = (avg_quality / 10) * 30
        else:
            quality_health = 0
        
        overall_health = coverage_health + success_health + quality_health
        
        # å¥åº·åº¦ç­‰ç´š
        if overall_health >= 90:
            health_grade = 'A+'
        elif overall_health >= 80:
            health_grade = 'A'
        elif overall_health >= 70:
            health_grade = 'B'
        elif overall_health >= 60:
            health_grade = 'C'
        elif overall_health >= 50:
            health_grade = 'D'
        else:
            health_grade = 'F'
        
        return {
            'overall_health': round(overall_health, 1),
            'health_grade': health_grade,
            'coverage_health': round(coverage_health, 1),
            'success_health': round(success_health, 1),
            'quality_health': round(quality_health, 1),
            'health_components': {
                'coverage_rate': coverage_rate,
                'success_rate': success_rate,
                'average_quality': round(statistics.mean(quality_scores), 2) if quality_scores else 0
            }
        }

    def analyze_search_patterns(self, processed_companies: List[Dict]) -> Dict[str, Any]:
        """åˆ†ææœå°‹é—œéµå­—æ¨¡å¼ - å¢å¼·ç‰ˆæœ¬"""
        # æå–æ‰€æœ‰é—œéµå­—
        all_keywords = []
        keyword_quality_scores = {}
        keyword_company_mapping = {}
        
        for company_data in processed_companies:
            search_keywords = company_data.get('search_keywords', [])
            quality_score = company_data.get('quality_score', 0)
            company_code = company_data.get('company_code', '')
            company_name = company_data.get('company_name', '')
            
            # åªè™•ç†è§€å¯Ÿåå–®ä¸­çš„å…¬å¸
            if company_code in self.watchlist_mapping:
                for keyword in search_keywords:
                    if keyword and keyword.lower() not in self.stop_words:
                        all_keywords.append(keyword)
                        
                        if keyword not in keyword_quality_scores:
                            keyword_quality_scores[keyword] = []
                        keyword_quality_scores[keyword].append(quality_score)
                        
                        if keyword not in keyword_company_mapping:
                            keyword_company_mapping[keyword] = set()
                        keyword_company_mapping[keyword].add(f"{company_name}({company_code})")
        
        # çµ±è¨ˆæœ€å¸¸ç”¨é—œéµå­—
        keyword_counts = Counter(all_keywords)
        most_common_keywords = keyword_counts.most_common(20)
        
        # è¨ˆç®—é—œéµå­—èˆ‡å“è³ªçš„é—œè¯
        keyword_quality_correlation = {}
        for keyword, scores in keyword_quality_scores.items():
            if scores:
                keyword_quality_correlation[keyword] = round(statistics.mean(scores), 2)
        
        # æŒ‰é—œéµå­—æ•¸é‡åˆ†çµ„å…¬å¸
        companies_by_keyword_count = self._group_companies_by_keyword_count(processed_companies)
        
        # æ‰¾å‡ºæ•ˆæœæœ€å¥½çš„æœå°‹æ¨¡å¼
        effective_patterns = self._identify_effective_patterns(keyword_quality_correlation, keyword_counts)
        
        # é—œéµå­—åˆ†é¡çµ±è¨ˆ
        keyword_category_stats = self._analyze_keyword_categories(keyword_counts, keyword_quality_correlation)
        
        return {
            'most_common_keywords': most_common_keywords,
            'keyword_quality_correlation': keyword_quality_correlation,
            'companies_by_keyword_count': companies_by_keyword_count,
            'effective_search_patterns': effective_patterns,
            'keyword_category_statistics': keyword_category_stats,
            'total_unique_keywords': len(keyword_counts),
            'average_keywords_per_company': round(len(all_keywords) / len([c for c in processed_companies if c.get('company_code') in self.watchlist_mapping]), 2) if processed_companies else 0
        }

    def _group_companies_by_keyword_count(self, processed_companies: List[Dict]) -> Dict[str, List[str]]:
        """æŒ‰é—œéµå­—æ•¸é‡åˆ†çµ„å…¬å¸"""
        companies_by_keyword_count = {'0': [], '1-2': [], '3-5': [], '6+': []}
        
        for company_code in self.watchlist_mapping.keys():
            # æ‰¾åˆ°è©²å…¬å¸çš„é—œéµå­—
            company_keywords = []
            company_name = self.watchlist_mapping[company_code]
            
            for company_data in processed_companies:
                if company_data.get('company_code') == company_code:
                    company_keywords.extend(company_data.get('search_keywords', []))
            
            keyword_count = len(set(company_keywords))
            
            if keyword_count == 0:
                companies_by_keyword_count['0'].append(company_name)
            elif keyword_count <= 2:
                companies_by_keyword_count['1-2'].append(company_name)
            elif keyword_count <= 5:
                companies_by_keyword_count['3-5'].append(company_name)
            else:
                companies_by_keyword_count['6+'].append(company_name)
        
        return companies_by_keyword_count

    def _identify_effective_patterns(self, keyword_quality_correlation: Dict, keyword_counts: Counter) -> List[str]:
        """è­˜åˆ¥æ•ˆæœæœ€å¥½çš„æœå°‹æ¨¡å¼"""
        effective_patterns = []
        
        for keyword, avg_score in keyword_quality_correlation.items():
            usage_count = keyword_counts[keyword]
            
            # æ•ˆæœå¥½ä¸”ä½¿ç”¨é »ç‡é©ä¸­çš„é—œéµå­—
            if avg_score >= 7.0 and usage_count >= 2:
                effectiveness_score = avg_score * (1 + min(usage_count / 10, 0.5))
                effective_patterns.append({
                    'pattern': keyword,
                    'avg_quality': avg_score,
                    'usage_count': usage_count,
                    'effectiveness_score': round(effectiveness_score, 2)
                })
        
        # æŒ‰æ•ˆæœè©•åˆ†æ’åº
        effective_patterns.sort(key=lambda x: x['effectiveness_score'], reverse=True)
        
        return [f"{p['pattern']} (å“è³ª: {p['avg_quality']}, ä½¿ç”¨: {p['usage_count']}æ¬¡)" 
                for p in effective_patterns[:10]]

    def _analyze_keyword_categories(self, keyword_counts: Counter, keyword_quality_correlation: Dict) -> Dict[str, Any]:
        """åˆ†æé—œéµå­—åˆ†é¡çµ±è¨ˆ"""
        category_stats = {}
        
        for category, keywords in self.keyword_categories.items():
            matching_keywords = []
            
            for keyword in keyword_counts.keys():
                keyword_lower = keyword.lower()
                if any(cat_keyword in keyword_lower for cat_keyword in keywords):
                    matching_keywords.append(keyword)
            
            if matching_keywords:
                total_usage = sum(keyword_counts[k] for k in matching_keywords)
                avg_quality = statistics.mean([keyword_quality_correlation.get(k, 0) for k in matching_keywords])
                
                category_stats[category] = {
                    'keyword_count': len(matching_keywords),
                    'total_usage': total_usage,
                    'average_quality': round(avg_quality, 2),
                    'keywords': matching_keywords[:5]  # åªä¿ç•™å‰5å€‹
                }
        
        return category_stats

    def _categorize_company_status(self, company_code: str, company_files: List[Dict]) -> str:
        """åˆ†é¡å…¬å¸è™•ç†ç‹€æ…‹ - å¢å¼·ç‰ˆæœ¬"""
        if not company_files:
            return 'not_found'
        
        # æª¢æŸ¥é©—è­‰ç‹€æ…‹
        validation_failed = any(not f.get('content_validation_passed', True) for f in company_files)
        if validation_failed:
            return 'validation_failed'
        
        # æª¢æŸ¥å“è³ªè©•åˆ†
        quality_scores = [f.get('quality_score', 0) for f in company_files]
        avg_quality = statistics.mean(quality_scores) if quality_scores else 0
        
        if avg_quality < 3.0:
            return 'low_quality'
        
        # æª¢æŸ¥æª”æ¡ˆæ•¸é‡
        if len(company_files) > 1:
            return 'multiple_files'
        
        return 'processed'

    def calculate_keyword_effectiveness_by_company(self, processed_companies: List[Dict]) -> Dict[str, Dict]:
        """è¨ˆç®—æ¯å®¶å…¬å¸çš„é—œéµå­—æ•ˆæœ - å¢å¼·ç‰ˆæœ¬"""
        company_keyword_effectiveness = {}
        
        for company_data in processed_companies:
            company_code = company_data.get('company_code', '')
            company_name = company_data.get('company_name', '')
            
            if company_code in self.watchlist_mapping:
                search_keywords = company_data.get('search_keywords', [])
                quality_score = company_data.get('quality_score', 0)
                
                if company_code not in company_keyword_effectiveness:
                    company_keyword_effectiveness[company_code] = {
                        'company_name': company_name,
                        'keywords': [],
                        'quality_scores': [],
                        'avg_effectiveness': 0,
                        'keyword_diversity': 0,
                        'best_keywords': [],
                        'improvement_suggestions': []
                    }
                
                effectiveness_data = company_keyword_effectiveness[company_code]
                effectiveness_data['keywords'].extend(search_keywords)
                effectiveness_data['quality_scores'].append(quality_score)
        
        # è¨ˆç®—æ•ˆæœçµ±è¨ˆ
        for company_code, effectiveness_data in company_keyword_effectiveness.items():
            quality_scores = effectiveness_data['quality_scores']
            keywords = effectiveness_data['keywords']
            
            if quality_scores:
                effectiveness_data['avg_effectiveness'] = round(statistics.mean(quality_scores), 2)
                effectiveness_data['keyword_diversity'] = len(set(keywords))
                
                # æ‰¾å‡ºæœ€ä½³é—œéµå­—çµ„åˆ
                if keywords:
                    keyword_counter = Counter(keywords)
                    effectiveness_data['best_keywords'] = [k for k, count in keyword_counter.most_common(3)]
                
                # ç”Ÿæˆæ”¹é€²å»ºè­°
                effectiveness_data['improvement_suggestions'] = self._generate_keyword_improvement_suggestions(
                    company_code, effectiveness_data
                )
        
        return company_keyword_effectiveness

    def _generate_keyword_improvement_suggestions(self, company_code: str, effectiveness_data: Dict) -> List[str]:
        """ç”Ÿæˆé—œéµå­—æ”¹é€²å»ºè­°"""
        suggestions = []
        avg_effectiveness = effectiveness_data['avg_effectiveness']
        keyword_diversity = effectiveness_data['keyword_diversity']
        
        if avg_effectiveness < 5.0:
            suggestions.append("å»ºè­°å¢åŠ æ›´å¤šé«˜å“è³ªé—œéµå­—")
            suggestions.append("è€ƒæ…®ä½¿ç”¨ 'factset' æˆ– 'eps' ç­‰é«˜æ•ˆé—œéµå­—")
        
        if keyword_diversity < 3:
            suggestions.append("å»ºè­°å¢åŠ é—œéµå­—å¤šæ¨£æ€§")
            suggestions.append("å¯å˜—è©¦æ·»åŠ ç”¢æ¥­ç›¸é—œæˆ–åˆ†æç›¸é—œé—œéµå­—")
        
        # åŸºæ–¼å…¬å¸ä»£è™Ÿçš„å»ºè­°
        try:
            code_num = int(company_code)
            if code_num <= 3000:
                suggestions.append("å¤§å‹è‚¡å»ºè­°ä½¿ç”¨: åˆ†æå¸«, ç›®æ¨™åƒ¹, é ä¼°")
            elif code_num <= 6000:
                suggestions.append("ä¸­å‹è‚¡å»ºè­°ä½¿ç”¨: ç‡Ÿæ”¶, ç²åˆ©, å±•æœ›")
            else:
                suggestions.append("å°å‹è‚¡å»ºè­°ä½¿ç”¨: ç”¢æ¥­, è¶¨å‹¢, æˆé•·")
        except ValueError:
            pass
        
        return suggestions[:3]  # é™åˆ¶å»ºè­°æ•¸é‡

    def generate_missing_companies_report(self, processed_companies: List[Dict]) -> List[Dict]:
        """ç”Ÿæˆç¼ºå¤±å…¬å¸å ±å‘Š - å¢å¼·ç‰ˆæœ¬"""
        processed_codes = set()
        for company_data in processed_companies:
            company_code = company_data.get('company_code', '')
            if company_code:
                processed_codes.add(company_code)
        
        missing_companies = []
        for company_code, company_name in self.watchlist_mapping.items():
            if company_code not in processed_codes:
                priority = self._calculate_missing_priority(company_code, company_name)
                suggested_keywords = self._suggest_search_keywords(company_code, company_name)
                
                missing_companies.append({
                    'company_code': company_code,
                    'company_name': company_name,
                    'status': 'ç¼ºå¤±MDæª”æ¡ˆ',
                    'priority': priority,
                    'priority_level': self._get_priority_level(priority),
                    'suggested_keywords': suggested_keywords,
                    'search_strategy': self._generate_search_strategy(company_code, company_name),
                    'estimated_difficulty': self._estimate_search_difficulty(company_code, company_name)
                })
        
        # æŒ‰å„ªå…ˆç´šæ’åº
        missing_companies.sort(key=lambda x: x['priority'], reverse=True)
        
        return missing_companies

    def _calculate_missing_priority(self, company_code: str, company_name: str) -> int:
        """è¨ˆç®—ç¼ºå¤±å…¬å¸çš„å„ªå…ˆç´š (1-10) - å¢å¼·ç‰ˆæœ¬"""
        priority = 5  # åŸºæº–åˆ†æ•¸
        
        # å…¬å¸ä»£è™Ÿè¶Šå°ï¼Œé€šå¸¸è¶Šé‡è¦ (å¤§å‹è‚¡)
        try:
            code_num = int(company_code)
            if code_num <= 1500:
                priority += 4  # è¶…å¤§å‹è‚¡
            elif code_num <= 2500:
                priority += 3  # å¤§å‹è‚¡
            elif code_num <= 4000:
                priority += 2  # ä¸­å¤§å‹è‚¡
            elif code_num <= 6000:
                priority += 1  # ä¸­å‹è‚¡
            # å°å‹è‚¡ä¸åŠ åˆ†
        except ValueError:
            priority -= 1
        
        # çŸ¥åå…¬å¸åç¨±é—œéµå­—
        important_keywords = [
            'å°ç©', 'é´»æµ·', 'è¯ç™¼', 'å°é”', 'ä¸­è¯é›»', 'å…†è±', 'ç¬¬ä¸€é‡‘',
            'åœ‹æ³°', 'å¯Œé‚¦', 'ä¸­ä¿¡', 'ç‰å±±', 'æ°¸è±', 'å…ƒå¤§', 'å°æ–°'
        ]
        
        if any(keyword in company_name for keyword in important_keywords):
            priority += 2
        
        # ç‰¹æ®Šç”¢æ¥­é—œéµå­—
        tech_keywords = ['ç§‘æŠ€', 'é›»å­', 'åŠå°é«”', 'å…‰é›»', 'é€šè¨Š']
        finance_keywords = ['é‡‘', 'éŠ€', 'ä¿éšª', 'è­‰åˆ¸', 'æŠ•ä¿¡']
        
        if any(keyword in company_name for keyword in tech_keywords):
            priority += 1
        elif any(keyword in company_name for keyword in finance_keywords):
            priority += 1
        
        return min(10, max(1, priority))

    def _get_priority_level(self, priority: int) -> str:
        """å–å¾—å„ªå…ˆç´šç­‰ç´š"""
        if priority >= 9:
            return "æ¥µé«˜"
        elif priority >= 7:
            return "é«˜"
        elif priority >= 5:
            return "ä¸­"
        elif priority >= 3:
            return "ä½"
        else:
            return "æ¥µä½"

    def _suggest_search_keywords(self, company_code: str, company_name: str) -> List[str]:
        """å»ºè­°æœå°‹é—œéµå­— - å¢å¼·ç‰ˆæœ¬"""
        keywords = []
        
        # å…¬å¸åç¨± (å¿…é ˆ)
        keywords.append(company_name)
        
        # å…¬å¸ä»£è™Ÿ
        keywords.append(company_code)
        
        # åŸºæ–¼å…¬å¸ä»£è™Ÿçš„å»ºè­°
        try:
            code_num = int(company_code)
            if code_num <= 3000:
                keywords.extend(['factset', 'eps', 'ç›®æ¨™åƒ¹', 'åˆ†æå¸«'])
            elif code_num <= 6000:
                keywords.extend(['factset', 'ç‡Ÿæ”¶', 'ç²åˆ©', 'é ä¼°'])
            else:
                keywords.extend(['factset', 'é ä¼°', 'å±•æœ›'])
        except ValueError:
            keywords.extend(['factset', 'eps'])
        
        # åŸºæ–¼å…¬å¸åç¨±çš„å»ºè­°
        if 'ç§‘æŠ€' in company_name or 'é›»å­' in company_name:
            keywords.extend(['åŠå°é«”', 'æŠ€è¡“'])
        elif 'é‡‘' in company_name or 'éŠ€' in company_name:
            keywords.extend(['é‡‘è', 'éŠ€è¡Œ'])
        elif 'ç”ŸæŠ€' in company_name or 'é†«' in company_name:
            keywords.extend(['ç”ŸæŠ€', 'é†«ç™‚'])
        
        # å»é‡ä¸¦é™åˆ¶æ•¸é‡
        unique_keywords = list(dict.fromkeys(keywords))[:8]
        
        return unique_keywords

    def _generate_search_strategy(self, company_code: str, company_name: str) -> str:
        """ç”Ÿæˆæœå°‹ç­–ç•¥å»ºè­°"""
        strategies = []
        
        try:
            code_num = int(company_code)
            if code_num <= 2000:
                strategies.append("å»ºè­°å„ªå…ˆæœå°‹å¤§å‹åˆ¸å•†ç ”ç©¶å ±å‘Š")
            elif code_num <= 4000:
                strategies.append("å»ºè­°æœå°‹ FactSet å’Œ Bloomberg è³‡æ–™")
            else:
                strategies.append("å»ºè­°æ“´å¤§æœå°‹ç¯„åœï¼ŒåŒ…å«ç”¢æ¥­å ±å‘Š")
        except ValueError:
            strategies.append("å»ºè­°ä½¿ç”¨å¤šé‡é—œéµå­—çµ„åˆæœå°‹")
        
        if len(company_name) <= 3:
            strategies.append("å…¬å¸åç¨±è¼ƒçŸ­ï¼Œå»ºè­°åŠ å…¥ä»£è™Ÿæœå°‹")
        
        return "; ".join(strategies)

    def _estimate_search_difficulty(self, company_code: str, company_name: str) -> str:
        """ä¼°è¨ˆæœå°‹é›£åº¦"""
        try:
            code_num = int(company_code)
            if code_num <= 2000:
                return "å®¹æ˜“"
            elif code_num <= 4000:
                return "ä¸­ç­‰"
            elif code_num <= 6000:
                return "å›°é›£"
            else:
                return "éå¸¸å›°é›£"
        except ValueError:
            return "æœªçŸ¥"

    def _generate_improvement_suggestions(self, missing_companies: List[Dict], 
                                        low_quality_companies: List[Dict]) -> List[str]:
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        suggestions = []
        
        if missing_companies:
            high_priority_missing = [c for c in missing_companies if c['priority'] >= 7]
            if high_priority_missing:
                suggestions.append(f"å„ªå…ˆè™•ç† {len(high_priority_missing)} å®¶é«˜å„ªå…ˆç´šç¼ºå¤±å…¬å¸")
        
        if low_quality_companies:
            suggestions.append(f"æ”¹å–„ {len(low_quality_companies)} å®¶ä½å“è³ªå…¬å¸çš„æœå°‹é—œéµå­—")
        
        if len(missing_companies) > len(self.watchlist_mapping) * 0.3:
            suggestions.append("è¦†è“‹ç‡åä½ï¼Œå»ºè­°æª¢æŸ¥æœå°‹ç³»çµ±é‹ä½œç‹€æ³")
        
        suggestions.append("å®šæœŸæ›´æ–°è§€å¯Ÿåå–®ï¼Œç¢ºä¿æ¶µè“‹é‡è¦å…¬å¸")
        suggestions.append("è€ƒæ…®ä½¿ç”¨å¤šæ¨£åŒ–çš„é—œéµå­—çµ„åˆæé«˜æœå°‹æ•ˆæœ")
        
        return suggestions


# æ¸¬è©¦åŠŸèƒ½
if __name__ == "__main__":
    analyzer = WatchlistAnalyzer()
    
    print("=== è§€å¯Ÿåå–®åˆ†æå™¨æ¸¬è©¦ v3.6.1 (å¢å¼·ç‰ˆ) ===")
    print(f"ğŸ“‹ è§€å¯Ÿåå–®è¼‰å…¥: {len(analyzer.watchlist_mapping)} å®¶å…¬å¸")
    print(f"ğŸ”§ é©—è­‰åŠŸèƒ½: {'å•Ÿç”¨' if analyzer.validation_enabled else 'åœç”¨'}")
    
    if analyzer.validation_enabled:
        # æ¸¬è©¦è³‡æ–™
        test_companies = [
            {
                'company_code': '2330',
                'company_name': 'å°ç©é›»',
                'search_keywords': ['å°ç©é›»', 'factset', 'eps', 'é ä¼°', 'åŠå°é«”'],
                'quality_score': 9.5,
                'content_validation_passed': True,
                'content_date': '2025/6/24',
                'has_eps_data': True,
                'has_target_price': True,
                'analyst_count': 42,
                'data_source': 'factset'
            },
            {
                'company_code': '6462', 
                'company_name': 'ç¥ç›¾',
                'search_keywords': ['ç¥ç›¾', 'factset', 'ç›®æ¨™åƒ¹', 'æŒ‡ç´‹è¾¨è­˜'],
                'quality_score': 7.2,
                'content_validation_passed': True,
                'content_date': '2025/6/23',
                'has_eps_data': False,
                'has_target_price': True,
                'analyst_count': 8,
                'data_source': 'factset'
            },
            {
                'company_code': '2317',
                'company_name': 'é´»æµ·',
                'search_keywords': ['é´»æµ·', 'factset', 'ä»£å·¥', 'eps'],
                'quality_score': 8.1,
                'content_validation_passed': True,
                'content_date': '2025/6/22',
                'has_eps_data': True,
                'has_target_price': False,
                'analyst_count': 35,
                'data_source': 'bloomberg'
            }
        ]
        
        print(f"\nğŸ§ª æ¸¬è©¦è§€å¯Ÿåå–®è¦†è“‹åˆ†æ (å¢å¼·ç‰ˆ):")
        coverage_analysis = analyzer.analyze_watchlist_coverage(test_companies)
        
        if 'error' not in coverage_analysis:
            print(f"âœ… åˆ†ææˆåŠŸå®Œæˆ")
            print(f"   è§€å¯Ÿåå–®å…¬å¸ç¸½æ•¸: {coverage_analysis['total_watchlist_companies']}")
            print(f"   æœ‰MDæª”æ¡ˆå…¬å¸æ•¸: {coverage_analysis['companies_with_md_files']}")
            print(f"   è¦†è“‹ç‡: {coverage_analysis['coverage_rate']}%")
            print(f"   æˆåŠŸè™•ç†ç‡: {coverage_analysis['success_rate']}%")
            
            # é¡¯ç¤ºå¥åº·åº¦æŒ‡æ¨™
            health_indicators = coverage_analysis['health_indicators']
            print(f"\nğŸ¥ ç³»çµ±å¥åº·åº¦:")
            print(f"   æ•´é«”å¥åº·åº¦: {health_indicators['overall_health']} ({health_indicators['health_grade']})")
            print(f"   è¦†è“‹ç‡å¥åº·åº¦: {health_indicators['coverage_health']}")
            print(f"   æˆåŠŸç‡å¥åº·åº¦: {health_indicators['success_health']}")
            print(f"   å“è³ªå¥åº·åº¦: {health_indicators['quality_health']}")
            
            # é¡¯ç¤ºå“è³ªçµ±è¨ˆ
            quality_stats = coverage_analysis['quality_statistics']
            print(f"\nğŸ“Š å¢å¼·å“è³ªçµ±è¨ˆ:")
            print(f"   å¹³å‡å“è³ªè©•åˆ†: {quality_stats['average_quality_score']}")
            print(f"   å“è³ªæ¨™æº–å·®: {quality_stats['quality_std']}")
            quality_dist = quality_stats['quality_distribution']
            print(f"   å“è³ªåˆ†å¸ƒ: å„ªç§€ {quality_dist['excellent']}, è‰¯å¥½ {quality_dist['good']}, æ™®é€š {quality_dist['average']}, å·® {quality_dist['poor']}")
            
            # é¡¯ç¤ºæœå°‹æ¨¡å¼åˆ†æ
            search_analysis = coverage_analysis['search_pattern_analysis']
            print(f"\nğŸ” å¢å¼·æœå°‹æ¨¡å¼åˆ†æ:")
            print(f"   å”¯ä¸€é—œéµå­—æ•¸: {search_analysis['total_unique_keywords']}")
            print(f"   å¹³å‡é—œéµå­—æ•¸/å…¬å¸: {search_analysis['average_keywords_per_company']}")
            
            # é¡¯ç¤ºç¼ºå¤±å…¬å¸åˆ†æ
            missing_analysis = coverage_analysis['missing_companies_analysis']
            print(f"\nâŒ ç¼ºå¤±å…¬å¸åˆ†æ:")
            print(f"   ç¼ºå¤±å…¬å¸æ•¸: {missing_analysis['missing_count']}")
            print(f"   ä½å“è³ªå…¬å¸æ•¸: {missing_analysis['low_quality_count']}")
            
        else:
            print(f"âŒ åˆ†æå¤±æ•—: {coverage_analysis['error']}")
    
    else:
        print("âš ï¸ è§€å¯Ÿåå–®åˆ†æå·²åœç”¨")
    
    print(f"\nâœ… v3.6.1 å¢å¼·ç‰ˆè§€å¯Ÿåå–®åˆ†æå™¨æ¸¬è©¦å®Œæˆï¼")
    print(f"ğŸ†• å¢å¼·åŠŸèƒ½: å¥åº·åº¦æŒ‡æ¨™ã€è©³ç´°å“è³ªçµ±è¨ˆã€æ”¹é€²å»ºè­°ã€æœå°‹ç­–ç•¥")