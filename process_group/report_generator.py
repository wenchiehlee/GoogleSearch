#!/usr/bin/env python3
"""
Report Generator - FactSet Pipeline v3.6.1 (æ–°å¢è§€å¯Ÿåå–®çµ±è¨ˆå ±å‘Š)
ç”ŸæˆæŠ•è³‡çµ„åˆå ±å‘Šã€æŸ¥è©¢æ¨¡å¼çµ±è¨ˆå ±å‘Šå’Œè§€å¯Ÿåå–®çµ±è¨ˆå ±å‘Š
"""

import os
import re
import pandas as pd
import urllib.parse
from datetime import datetime
from typing import Dict, Any, List, Optional
import pytz

class ReportGenerator:
    """å ±å‘Šç”Ÿæˆå™¨ v3.6.1 - æ”¯æ´æ¨™æº–åŒ–æŸ¥è©¢æ¨¡å¼å ±å‘Š"""
    
    def __init__(self, github_repo_base="https://raw.githubusercontent.com/wenchiehlee/GoogleSearch/refs/heads/main"):
        self.github_repo_base = github_repo_base
        self.output_dir = "data/reports"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # è¨­å®šå°åŒ—æ™‚å€
        self.taipei_tz = pytz.timezone('Asia/Taipei')
        
        # æŠ•è³‡çµ„åˆæ‘˜è¦æ¬„ä½ (14 æ¬„ä½)
        self.portfolio_summary_columns = [
            'ä»£è™Ÿ', 'åç¨±', 'è‚¡ç¥¨ä»£è™Ÿ', 'MDæœ€èˆŠæ—¥æœŸ', 'MDæœ€æ–°æ—¥æœŸ', 'MDè³‡æ–™ç­†æ•¸',
            'åˆ†æå¸«æ•¸é‡', 'ç›®æ¨™åƒ¹', '2025EPSå¹³å‡å€¼', '2026EPSå¹³å‡å€¼', '2027EPSå¹³å‡å€¼',
            'å“è³ªè©•åˆ†', 'ç‹€æ…‹', 'æ›´æ–°æ—¥æœŸ'
        ]

        # è©³ç´°å ±å‘Šæ¬„ä½ (22 æ¬„ä½ - åŒ…å«é©—è­‰ç‹€æ…‹)
        self.detailed_report_columns = [
            'ä»£è™Ÿ', 'åç¨±', 'è‚¡ç¥¨ä»£è™Ÿ', 'MDæ—¥æœŸ', 'åˆ†æå¸«æ•¸é‡', 'ç›®æ¨™åƒ¹',
            '2025EPSæœ€é«˜å€¼', '2025EPSæœ€ä½å€¼', '2025EPSå¹³å‡å€¼',
            '2026EPSæœ€é«˜å€¼', '2026EPSæœ€ä½å€¼', '2026EPSå¹³å‡å€¼',
            '2027EPSæœ€é«˜å€¼', '2027EPSæœ€ä½å€¼', '2027EPSå¹³å‡å€¼',
            'å“è³ªè©•åˆ†', 'ç‹€æ…‹', 'é©—è­‰ç‹€æ…‹', 'MD File', 'æ›´æ–°æ—¥æœŸ'
        ]

        # é—œéµå­—å ±å‘Šæ¬„ä½ (10 æ¬„ä½) - ä¿ç•™å‚³çµ±é—œéµå­—åˆ†æç”¨
        self.keyword_summary_columns = [
            'é—œéµå­—', 'ä½¿ç”¨æ¬¡æ•¸', 'å¹³å‡å“è³ªè©•åˆ†', 'æœ€é«˜å“è³ªè©•åˆ†', 'æœ€ä½å“è³ªè©•åˆ†',
            'ç›¸é—œå…¬å¸æ•¸é‡', 'å“è³ªç‹€æ…‹', 'åˆ†é¡', 'æ•ˆæœè©•ç´š', 'æ›´æ–°æ—¥æœŸ'
        ]
        
        # ğŸ†• æŸ¥è©¢æ¨¡å¼å ±å‘Šæ¬„ä½ (10 æ¬„ä½) - æ–°å¢æ¨™æº–åŒ–æŸ¥è©¢æ¨¡å¼
        self.query_pattern_summary_columns = [
            'Query pattern', 'ä½¿ç”¨æ¬¡æ•¸', 'å¹³å‡å“è³ªè©•åˆ†', 'æœ€é«˜å“è³ªè©•åˆ†', 'æœ€ä½å“è³ªè©•åˆ†',
            'ç›¸é—œå…¬å¸æ•¸é‡', 'å“è³ªç‹€æ…‹', 'åˆ†é¡', 'æ•ˆæœè©•ç´š', 'æ›´æ–°æ—¥æœŸ'
        ]
        
        # ğŸ†• è§€å¯Ÿåå–®å ±å‘Šæ¬„ä½ (12 æ¬„ä½) - v3.6.1 æ–°å¢
        self.watchlist_summary_columns = [
            'å…¬å¸ä»£è™Ÿ', 'å…¬å¸åç¨±', 'MDæª”æ¡ˆæ•¸é‡', 'è™•ç†ç‹€æ…‹', 'å¹³å‡å“è³ªè©•åˆ†', 'æœ€é«˜å“è³ªè©•åˆ†',
            'æœå°‹é—œéµå­—æ•¸é‡', 'ä¸»è¦é—œéµå­—', 'é—œéµå­—å¹³å‡å“è³ª', 'æœ€æ–°æª”æ¡ˆæ—¥æœŸ', 'é©—è­‰ç‹€æ…‹', 'æ›´æ–°æ—¥æœŸ'
        ]

    def _get_taipei_time(self) -> str:
        """å–å¾—å°åŒ—æ™‚é–“çš„å­—ä¸²æ ¼å¼"""
        taipei_time = datetime.now(self.taipei_tz)
        return taipei_time.strftime('%Y-%m-%d %H:%M:%S')

    def _should_include_in_report_v351(self, company_data: Dict[str, Any]) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²å°‡æ­¤è³‡æ–™åŒ…å«åœ¨å ±å‘Šä¸­"""
        
        # æª¢æŸ¥ 1: åŸºæœ¬è³‡æ–™å®Œæ•´æ€§
        company_code = company_data.get('company_code')
        company_name = company_data.get('company_name')
        
        if not company_code or company_code == 'Unknown':
            return False
        
        if not company_name or company_name == 'Unknown':
            return False
        
        # æª¢æŸ¥ 2: é©—è­‰çµæœæª¢æŸ¥
        validation_result = company_data.get('validation_result', {})
        validation_status = validation_result.get('overall_status', 'unknown')
        
        if validation_status == 'error':
            return False
        
        # æª¢æŸ¥ 3: content_validation_passed å­—æ®µ
        validation_passed = company_data.get('content_validation_passed', True)
        if not validation_passed:
            return False
        
        # æª¢æŸ¥ 4: æª¢æŸ¥é—œéµé©—è­‰éŒ¯èª¤
        validation_errors = company_data.get('validation_errors', [])
        if validation_errors:
            critical_error_keywords = [
                'ä¸åœ¨è§€å¯Ÿåå–®ä¸­',
                'å…¬å¸åç¨±ä¸ç¬¦è§€å¯Ÿåå–®',
                'è§€å¯Ÿåå–®é¡¯ç¤ºæ‡‰ç‚º',
                'æ„›æ´¾å¸.*æ„›ç«‹ä¿¡',
                'æ„›ç«‹ä¿¡.*æ„›æ´¾å¸',
                'å…¬å¸ä»£è™Ÿæ ¼å¼ç„¡æ•ˆ'
            ]
            
            for error in validation_errors:
                error_str = str(error)
                if any(re.search(keyword, error_str, re.IGNORECASE) for keyword in critical_error_keywords):
                    return False
        
        # æª¢æŸ¥ 5: å“è³ªè©•åˆ†ç‚º 0 ä¸”æœ‰ âŒ é©—è­‰å¤±æ•—ç‹€æ…‹
        quality_score = company_data.get('quality_score', 0)
        quality_status = company_data.get('quality_status', '')
        
        if quality_score == 0 and 'âŒ é©—è­‰å¤±æ•—' in quality_status:
            return False
        
        return True

    def generate_portfolio_summary(self, processed_companies: List[Dict], filter_invalid=True) -> pd.DataFrame:
        """ç”ŸæˆæŠ•è³‡çµ„åˆæ‘˜è¦ (14 æ¬„ä½)"""
        try:
            # éæ¿¾é‚è¼¯
            if filter_invalid:
                original_count = len(processed_companies)
                valid_companies = [c for c in processed_companies if self._should_include_in_report_v351(c)]
                filtered_count = original_count - len(valid_companies)
                
                print(f"ğŸ“Š æŠ•è³‡çµ„åˆæ‘˜è¦éæ¿¾çµæœ:")
                print(f"   åŸå§‹å…¬å¸æ•¸: {original_count}")
                print(f"   ä¿ç•™å…¬å¸æ•¸: {len(valid_companies)}")
                print(f"   éæ¿¾å…¬å¸æ•¸: {filtered_count}")
            else:
                valid_companies = processed_companies
                print(f"ğŸ“Š æŠ•è³‡çµ„åˆæ‘˜è¦ï¼šæœªå•Ÿç”¨éæ¿¾ï¼ŒåŒ…å«æ‰€æœ‰ {len(valid_companies)} å®¶å…¬å¸")
            
            # æŒ‰å…¬å¸åˆ†çµ„ï¼Œå–å¾—æ¯å®¶å…¬å¸çš„æœ€ä½³å“è³ªè³‡æ–™
            company_summary = {}
            
            for company_data in valid_companies:
                company_code = company_data.get('company_code', 'Unknown')
                
                if company_code not in company_summary:
                    company_summary[company_code] = {
                        'files': [],
                        'best_quality_data': None
                    }
                
                company_summary[company_code]['files'].append(company_data)
                
                # é¸æ“‡æœ€ä½³å“è³ªè³‡æ–™
                current_best = company_summary[company_code]['best_quality_data']
                
                if current_best is None:
                    company_summary[company_code]['best_quality_data'] = company_data
                else:
                    current_quality = company_data.get('quality_score', 0)
                    best_quality = current_best.get('quality_score', 0)
                    
                    if current_quality > best_quality:
                        company_summary[company_code]['best_quality_data'] = company_data
            
            # ç”Ÿæˆæ‘˜è¦è³‡æ–™
            summary_rows = []
            
            for company_code, company_info in company_summary.items():
                best_data = company_info['best_quality_data']
                all_files = company_info['files']
                
                # è¨ˆç®—æ—¥æœŸç¯„åœ
                oldest_date, newest_date = self._calculate_date_range_content_date_only(all_files)
                
                # ä½¿ç”¨æœ€ä½³å“è³ªè³‡æ–™ç”Ÿæˆæ‘˜è¦
                clean_code = self._clean_stock_code_for_display(company_code)
                
                summary_row = {
                    'ä»£è™Ÿ': clean_code,
                    'åç¨±': best_data.get('company_name', 'Unknown'),
                    'è‚¡ç¥¨ä»£è™Ÿ': f"{clean_code}-TW",
                    'MDæœ€èˆŠæ—¥æœŸ': oldest_date,
                    'MDæœ€æ–°æ—¥æœŸ': newest_date,
                    'MDè³‡æ–™ç­†æ•¸': len(all_files),
                    'åˆ†æå¸«æ•¸é‡': best_data.get('analyst_count', 0),
                    'ç›®æ¨™åƒ¹': best_data.get('target_price', ''),
                    '2025EPSå¹³å‡å€¼': self._format_eps_value(best_data.get('eps_2025_avg')),
                    '2026EPSå¹³å‡å€¼': self._format_eps_value(best_data.get('eps_2026_avg')),
                    '2027EPSå¹³å‡å€¼': self._format_eps_value(best_data.get('eps_2027_avg')),
                    'å“è³ªè©•åˆ†': best_data.get('quality_score', 0),
                    'ç‹€æ…‹': best_data.get('quality_status', 'ğŸ”´ ä¸è¶³'),
                    'æ›´æ–°æ—¥æœŸ': self._get_taipei_time()
                }
                
                summary_rows.append(summary_row)
            
            # å»ºç«‹ DataFrame
            df = pd.DataFrame(summary_rows, columns=self.portfolio_summary_columns)
            df = df.sort_values('ä»£è™Ÿ')
            
            print("âœ… æŠ•è³‡çµ„åˆæ‘˜è¦å·²ä½¿ç”¨æœ€ä½³å“è³ªè³‡æ–™ç”Ÿæˆ")
            
            return df
            
        except Exception as e:
            print(f"âŒ ç”ŸæˆæŠ•è³‡çµ„åˆæ‘˜è¦å¤±æ•—: {e}")
            return pd.DataFrame(columns=self.portfolio_summary_columns)

    def generate_detailed_report(self, processed_companies: List[Dict], filter_invalid=True) -> pd.DataFrame:
        """ç”Ÿæˆè©³ç´°å ±å‘Š (22 æ¬„ä½)"""
        try:
            detailed_rows = []
            filtered_count = 0
            
            for company_data in processed_companies:
                # æª¢æŸ¥æ˜¯å¦æ‡‰è©²éæ¿¾æ­¤è³‡æ–™
                if filter_invalid and not self._should_include_in_report_v351(company_data):
                    filtered_count += 1
                    continue
                
                # ç”Ÿæˆé©—è­‰ç‹€æ…‹æ¨™è¨˜
                validation_status = self._generate_validation_status_marker_v351(company_data)
                
                # è™•ç† MD æª”æ¡ˆé€£çµ
                md_file_url = self._format_md_file_url_with_warning(company_data)
                
                detailed_row = {
                    'ä»£è™Ÿ': company_data.get('company_code', 'Unknown'),
                    'åç¨±': company_data.get('company_name', 'Unknown'),
                    'è‚¡ç¥¨ä»£è™Ÿ': f"{company_data.get('company_code', 'Unknown')}-TW",
                    'MDæ—¥æœŸ': self._get_content_date_only(company_data),
                    'åˆ†æå¸«æ•¸é‡': company_data.get('analyst_count', 0),
                    'ç›®æ¨™åƒ¹': company_data.get('target_price', ''),
                    '2025EPSæœ€é«˜å€¼': self._format_eps_value(company_data.get('eps_2025_high')),
                    '2025EPSæœ€ä½å€¼': self._format_eps_value(company_data.get('eps_2025_low')),
                    '2025EPSå¹³å‡å€¼': self._format_eps_value(company_data.get('eps_2025_avg')),
                    '2026EPSæœ€é«˜å€¼': self._format_eps_value(company_data.get('eps_2026_high')),
                    '2026EPSæœ€ä½å€¼': self._format_eps_value(company_data.get('eps_2026_low')),
                    '2026EPSå¹³å‡å€¼': self._format_eps_value(company_data.get('eps_2026_avg')),
                    '2027EPSæœ€é«˜å€¼': self._format_eps_value(company_data.get('eps_2027_high')),
                    '2027EPSæœ€ä½å€¼': self._format_eps_value(company_data.get('eps_2027_low')),
                    '2027EPSå¹³å‡å€¼': self._format_eps_value(company_data.get('eps_2027_avg')),
                    'å“è³ªè©•åˆ†': company_data.get('quality_score', 0),
                    'ç‹€æ…‹': company_data.get('quality_status', 'ğŸ”´ ä¸è¶³'),
                    'é©—è­‰ç‹€æ…‹': validation_status,
                    'MD File': md_file_url,
                    'æ›´æ–°æ—¥æœŸ': self._get_taipei_time()
                }
                
                detailed_rows.append(detailed_row)
            
            # å»ºç«‹ DataFrame
            df = pd.DataFrame(detailed_rows, columns=self.detailed_report_columns)
            df = df.sort_values(['ä»£è™Ÿ', 'MDæ—¥æœŸ'], ascending=[True, False])
            
            if filter_invalid and filtered_count > 0:
                print(f"ğŸ“Š è©³ç´°å ±å‘Šéæ¿¾äº† {filtered_count} ç­†é©—è­‰å¤±æ•—çš„è³‡æ–™")
            
            print(f"ğŸ“Š è©³ç´°å ±å‘ŠåŒ…å« {len(detailed_rows)} ç­†æœ‰æ•ˆè³‡æ–™")
            
            return df
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆè©³ç´°å ±å‘Šå¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame(columns=self.detailed_report_columns)

    def generate_keyword_summary(self, keyword_analysis: Dict[str, Any]) -> pd.DataFrame:
        """ğŸ”§ ç”Ÿæˆé—œéµå­—/æŸ¥è©¢æ¨¡å¼çµ±è¨ˆå ±å‘Šï¼ˆæ”¯æ´æ¨™æº–åŒ–æŸ¥è©¢æ¨¡å¼ï¼‰"""
        try:
            # æª¢æŸ¥æ˜¯å¦ç‚ºæŸ¥è©¢æ¨¡å¼åˆ†æ
            analysis_type = keyword_analysis.get('analysis_type', 'keywords')
            
            if analysis_type in ['search_query_patterns_normalized', 'search_query_patterns']:
                return self._generate_query_pattern_summary(keyword_analysis)
            else:
                return self._generate_traditional_keyword_summary(keyword_analysis)
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆé—œéµå­—/æŸ¥è©¢æ¨¡å¼å ±å‘Šå¤±æ•—: {e}")
            return pd.DataFrame(columns=self.keyword_summary_columns)

    def _generate_query_pattern_summary(self, pattern_analysis: Dict[str, Any]) -> pd.DataFrame:
        """ğŸ†• ç”Ÿæˆæ¨™æº–åŒ–æŸ¥è©¢æ¨¡å¼çµ±è¨ˆå ±å‘Š"""
        pattern_data = []
        pattern_stats = pattern_analysis.get('pattern_stats', {})
        
        for pattern, stats in pattern_stats.items():
            # è¨ˆç®—æ•ˆæœè©•ç´š
            avg_score = stats['avg_quality_score']
            if avg_score >= 9:
                effect_rating = "å„ªç§€ â­â­â­"
            elif avg_score >= 7:
                effect_rating = "è‰¯å¥½ â­â­"
            elif avg_score >= 5:
                effect_rating = "æ™®é€š â­"
            else:
                effect_rating = "éœ€æ”¹å–„"
            
            # å“è³ªç‹€æ…‹
            quality_status = self._get_quality_status_by_score(avg_score)
            
            # æ¨¡å¼é¡å‹è½‰æ›ç‚ºä¸­æ–‡
            pattern_type = stats.get('pattern_type', 'other')
            category_mapping = {
                'factset_direct': 'FactSetç›´æ¥',
                'cnyes_factset': 'é‰…äº¨ç¶²FactSet',
                'eps_forecast': 'EPSé ä¼°',
                'analyst_consensus': 'åˆ†æå¸«å…±è­˜',
                'taiwan_financial_simple': 'å°ç£è²¡ç¶“',
                'other': 'å…¶ä»–'
            }
            category_display = category_mapping.get(pattern_type, 'other')
            
            pattern_data.append({
                'Query pattern': pattern,  # ğŸ”§ é—œéµæ¬„ä½åç¨±
                'ä½¿ç”¨æ¬¡æ•¸': stats['usage_count'],
                'å¹³å‡å“è³ªè©•åˆ†': round(avg_score, 2),
                'æœ€é«˜å“è³ªè©•åˆ†': stats['max_quality_score'],
                'æœ€ä½å“è³ªè©•åˆ†': stats['min_quality_score'],
                'ç›¸é—œå…¬å¸æ•¸é‡': stats['company_count'],
                'å“è³ªç‹€æ…‹': quality_status,
                'åˆ†é¡': category_display,
                'æ•ˆæœè©•ç´š': effect_rating,
                'æ›´æ–°æ—¥æœŸ': self._get_taipei_time()
            })
        
        # æŒ‰å¹³å‡å“è³ªè©•åˆ†æ’åº
        pattern_data.sort(key=lambda x: x['å¹³å‡å“è³ªè©•åˆ†'], reverse=True)
        
        return pd.DataFrame(pattern_data, columns=self.query_pattern_summary_columns)

    def _generate_traditional_keyword_summary(self, keyword_analysis: Dict[str, Any]) -> pd.DataFrame:
        """ç”Ÿæˆå‚³çµ±é—œéµå­—çµ±è¨ˆå ±å‘Šï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰"""
        keyword_data = []
        keyword_stats = keyword_analysis.get('keyword_stats', {})
        
        for keyword, stats in keyword_stats.items():
            # è¨ˆç®—æ•ˆæœè©•ç´š
            avg_score = stats['avg_quality_score']
            if avg_score >= 9:
                effect_rating = "å„ªç§€ â­â­â­"
            elif avg_score >= 7:
                effect_rating = "è‰¯å¥½ â­â­"
            elif avg_score >= 5:
                effect_rating = "æ™®é€š â­"
            else:
                effect_rating = "éœ€æ”¹å–„"
            
            # å“è³ªç‹€æ…‹
            quality_status = self._get_quality_status_by_score(avg_score)
            
            keyword_data.append({
                'é—œéµå­—': keyword,
                'ä½¿ç”¨æ¬¡æ•¸': stats['usage_count'],
                'å¹³å‡å“è³ªè©•åˆ†': round(avg_score, 2),
                'æœ€é«˜å“è³ªè©•åˆ†': stats['max_quality_score'],
                'æœ€ä½å“è³ªè©•åˆ†': stats['min_quality_score'],
                'ç›¸é—œå…¬å¸æ•¸é‡': stats['company_count'],
                'å“è³ªç‹€æ…‹': quality_status,
                'åˆ†é¡': stats['category'],
                'æ•ˆæœè©•ç´š': effect_rating,
                'æ›´æ–°æ—¥æœŸ': self._get_taipei_time()
            })
        
        # æŒ‰å¹³å‡å“è³ªè©•åˆ†æ’åº
        keyword_data.sort(key=lambda x: x['å¹³å‡å“è³ªè©•åˆ†'], reverse=True)
        
        return pd.DataFrame(keyword_data, columns=self.keyword_summary_columns)

    def generate_watchlist_summary(self, watchlist_analysis: Dict[str, Any]) -> pd.DataFrame:
        """ğŸ†• v3.6.1 ç”Ÿæˆè§€å¯Ÿåå–®çµ±è¨ˆå ±å‘Š"""
        try:
            watchlist_data = []
            company_processing_status = watchlist_analysis.get('company_processing_status', {})
            search_pattern_analysis = watchlist_analysis.get('search_pattern_analysis', {})
            keyword_quality_correlation = search_pattern_analysis.get('keyword_quality_correlation', {})
            
            # è™•ç†ç‹€æ…‹ä¸­æ–‡åŒ–æ˜ å°„
            status_mapping = {
                'processed': 'âœ… å·²è™•ç†',
                'not_found': 'âŒ æœªæ‰¾åˆ°',
                'validation_failed': 'ğŸš« é©—è­‰å¤±æ•—',
                'low_quality': 'ğŸ”´ å“è³ªéä½',
                'multiple_files': 'ğŸ“„ å¤šå€‹æª”æ¡ˆ'
            }
            
            for company_code, status_info in company_processing_status.items():
                # è¨ˆç®—ä¸»è¦é—œéµå­—
                company_keywords = status_info.get('search_keywords', [])
                main_keywords = ', '.join(company_keywords[:3]) if company_keywords else ''
                
                # è¨ˆç®—é—œéµå­—å¹³å‡å“è³ª
                keyword_avg_quality = 0
                if company_keywords:
                    keyword_qualities = [keyword_quality_correlation.get(kw, 0) for kw in company_keywords]
                    keyword_avg_quality = sum(keyword_qualities) / len(keyword_qualities) if keyword_qualities else 0
                
                # è™•ç†ç‹€æ…‹é¡¯ç¤º
                status_display = status_mapping.get(status_info['status'], status_info['status'])
                
                # æ ¼å¼åŒ–æœ€æ–°æª”æ¡ˆæ—¥æœŸ
                latest_file_date = status_info.get('latest_file_date', '')
                if latest_file_date:
                    # çµ±ä¸€æ—¥æœŸæ ¼å¼
                    if '/' in latest_file_date:
                        try:
                            parts = latest_file_date.split('/')
                            if len(parts) == 3:
                                year, month, day = parts
                                latest_file_date = f"{year}-{int(month):02d}-{int(day):02d}"
                        except:
                            pass
                
                watchlist_data.append({
                    'å…¬å¸ä»£è™Ÿ': company_code,
                    'å…¬å¸åç¨±': status_info.get('company_name', ''),
                    'MDæª”æ¡ˆæ•¸é‡': status_info.get('file_count', 0),
                    'è™•ç†ç‹€æ…‹': status_display,
                    'å¹³å‡å“è³ªè©•åˆ†': round(status_info.get('average_quality_score', 0), 2),
                    'æœ€é«˜å“è³ªè©•åˆ†': status_info.get('max_quality_score', 0),
                    'æœå°‹é—œéµå­—æ•¸é‡': len(company_keywords),
                    'ä¸»è¦é—œéµå­—': main_keywords,
                    'é—œéµå­—å¹³å‡å“è³ª': round(keyword_avg_quality, 2),
                    'æœ€æ–°æª”æ¡ˆæ—¥æœŸ': latest_file_date,
                    'é©—è­‰ç‹€æ…‹': self._get_validation_status_display(status_info),
                    'æ›´æ–°æ—¥æœŸ': self._get_taipei_time()
                })
            
            # æŒ‰è™•ç†ç‹€æ…‹å’Œå“è³ªè©•åˆ†æ’åº
            watchlist_data.sort(key=lambda x: (
                x['è™•ç†ç‹€æ…‹'] == 'âœ… å·²è™•ç†',  # å·²è™•ç†çš„æ’åœ¨å‰é¢
                x['å¹³å‡å“è³ªè©•åˆ†']             # ç„¶å¾ŒæŒ‰å“è³ªè©•åˆ†æ’åº
            ), reverse=True)
            
            df = pd.DataFrame(watchlist_data, columns=self.watchlist_summary_columns)
            
            print(f"ğŸ“Š è§€å¯Ÿåå–®çµ±è¨ˆå ±å‘Šå·²ç”Ÿæˆ: {len(watchlist_data)} å®¶å…¬å¸")
            
            return df
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆè§€å¯Ÿåå–®çµ±è¨ˆå ±å‘Šå¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame(columns=self.watchlist_summary_columns)

    def save_keyword_summary(self, keyword_df: pd.DataFrame) -> str:
        """ğŸ”§ å„²å­˜é—œéµå­—/æŸ¥è©¢æ¨¡å¼çµ±è¨ˆå ±å‘Š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # æ ¹æ“šç¬¬ä¸€æ¬„çš„åç¨±æ±ºå®šæª”æ¡ˆåç¨±
        if len(keyword_df.columns) > 0 and keyword_df.columns[0] == 'Query pattern':
            keyword_path = os.path.join(self.output_dir, f'query_pattern_summary_{timestamp}.csv')
            latest_keyword_path = os.path.join(self.output_dir, 'query_pattern_summary_latest.csv')
            report_type = "æŸ¥è©¢æ¨¡å¼"
        else:
            keyword_path = os.path.join(self.output_dir, f'keyword_summary_{timestamp}.csv')
            latest_keyword_path = os.path.join(self.output_dir, 'keyword_summary_latest.csv')
            report_type = "é—œéµå­—"
        
        try:
            keyword_df_clean = keyword_df.replace('', pd.NA)
            keyword_df_clean.to_csv(keyword_path, index=False, encoding='utf-8-sig')
            keyword_df_clean.to_csv(latest_keyword_path, index=False, encoding='utf-8-sig')
            
            print(f"ğŸ“ {report_type}çµ±è¨ˆå ±å‘Šå·²å„²å­˜: {keyword_path}")
            return keyword_path
            
        except Exception as e:
            print(f"âŒ å„²å­˜{report_type}å ±å‘Šå¤±æ•—: {e}")
            return ""

    def save_all_reports(self, portfolio_df: pd.DataFrame, detailed_df: pd.DataFrame, 
                        keyword_df: pd.DataFrame = None, watchlist_df: pd.DataFrame = None) -> Dict[str, str]:
        """ğŸ”§ v3.6.1 å„²å­˜æ‰€æœ‰å ±å‘Šç‚º CSV - åŒ…å«æŸ¥è©¢æ¨¡å¼å’Œè§€å¯Ÿåå–®å ±å‘Š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # æª”æ¡ˆè·¯å¾‘
        portfolio_path = os.path.join(self.output_dir, f'portfolio_summary_v361_{timestamp}.csv')
        detailed_path = os.path.join(self.output_dir, f'detailed_report_v361_{timestamp}.csv')
        latest_portfolio_path = os.path.join(self.output_dir, 'portfolio_summary_latest.csv')
        latest_detailed_path = os.path.join(self.output_dir, 'detailed_report_latest.csv')
        
        try:
            # ç¢ºä¿ç©ºå­—ç¬¦ä¸²åœ¨ CSV ä¸­æ­£ç¢ºè™•ç†
            portfolio_df_clean = portfolio_df.copy().replace('', pd.NA)
            detailed_df_clean = detailed_df.copy().replace('', pd.NA)
            
            # å„²å­˜ä¸»è¦å ±å‘Š
            portfolio_df_clean.to_csv(portfolio_path, index=False, encoding='utf-8-sig')
            detailed_df_clean.to_csv(detailed_path, index=False, encoding='utf-8-sig')
            portfolio_df_clean.to_csv(latest_portfolio_path, index=False, encoding='utf-8-sig')
            detailed_df_clean.to_csv(latest_detailed_path, index=False, encoding='utf-8-sig')
            
            saved_files = {
                'portfolio_summary': portfolio_path,
                'detailed_report': detailed_path,
                'portfolio_summary_latest': latest_portfolio_path,
                'detailed_report_latest': latest_detailed_path
            }
            
            # ğŸ†• å„²å­˜é—œéµå­—/æŸ¥è©¢æ¨¡å¼å ±å‘Š
            if keyword_df is not None and not keyword_df.empty:
                # æª¢æŸ¥æ˜¯å¦ç‚ºæŸ¥è©¢æ¨¡å¼å ±å‘Š
                if len(keyword_df.columns) > 0 and keyword_df.columns[0] == 'Query pattern':
                    pattern_path = os.path.join(self.output_dir, f'query_pattern_summary_v361_{timestamp}.csv')
                    latest_pattern_path = os.path.join(self.output_dir, 'query_pattern_summary_latest.csv')
                    report_key = 'query_pattern_summary'
                    latest_key = 'query_pattern_summary_latest'
                else:
                    pattern_path = os.path.join(self.output_dir, f'keyword_summary_v361_{timestamp}.csv')
                    latest_pattern_path = os.path.join(self.output_dir, 'keyword_summary_latest.csv')
                    report_key = 'keyword_summary'
                    latest_key = 'keyword_summary_latest'
                
                keyword_df_clean = keyword_df.replace('', pd.NA)
                keyword_df_clean.to_csv(pattern_path, index=False, encoding='utf-8-sig')
                keyword_df_clean.to_csv(latest_pattern_path, index=False, encoding='utf-8-sig')
                
                saved_files[report_key] = pattern_path
                saved_files[latest_key] = latest_pattern_path
            
            # ğŸ†• å„²å­˜è§€å¯Ÿåå–®å ±å‘Š
            if watchlist_df is not None and not watchlist_df.empty:
                watchlist_path = os.path.join(self.output_dir, f'watchlist_summary_v361_{timestamp}.csv')
                latest_watchlist_path = os.path.join(self.output_dir, 'watchlist_summary_latest.csv')
                
                watchlist_df_clean = watchlist_df.replace('', pd.NA)
                watchlist_df_clean.to_csv(watchlist_path, index=False, encoding='utf-8-sig')
                watchlist_df_clean.to_csv(latest_watchlist_path, index=False, encoding='utf-8-sig')
                
                saved_files['watchlist_summary'] = watchlist_path
                saved_files['watchlist_summary_latest'] = latest_watchlist_path
            
            print(f"ğŸ“ v3.6.1 å ±å‘Šå·²å„²å­˜:")
            print(f"   æŠ•è³‡çµ„åˆæ‘˜è¦: {portfolio_path}")
            print(f"   è©³ç´°å ±å‘Š: {detailed_path}")
            if keyword_df is not None:
                report_type = "æŸ¥è©¢æ¨¡å¼" if 'query_pattern_summary' in saved_files else "é—œéµå­—"
                print(f"   {report_type}å ±å‘Š: {saved_files.get('query_pattern_summary', saved_files.get('keyword_summary', 'N/A'))}")
            if watchlist_df is not None:
                print(f"   è§€å¯Ÿåå–®å ±å‘Š: {saved_files.get('watchlist_summary', 'N/A')}")
            
            return saved_files
            
        except Exception as e:
            print(f"âŒ å„²å­˜å ±å‘Šå¤±æ•—: {e}")
            return {}

    def save_watchlist_summary(self, watchlist_df: pd.DataFrame) -> str:
        """ğŸ†• v3.6.1 å–®ç¨å„²å­˜è§€å¯Ÿåå–®çµ±è¨ˆå ±å‘Š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        watchlist_path = os.path.join(self.output_dir, f'watchlist_summary_{timestamp}.csv')
        latest_watchlist_path = os.path.join(self.output_dir, 'watchlist_summary_latest.csv')
        
        try:
            watchlist_df_clean = watchlist_df.replace('', pd.NA)
            watchlist_df_clean.to_csv(watchlist_path, index=False, encoding='utf-8-sig')
            watchlist_df_clean.to_csv(latest_watchlist_path, index=False, encoding='utf-8-sig')
            
            print(f"ğŸ“ è§€å¯Ÿåå–®çµ±è¨ˆå ±å‘Šå·²å„²å­˜: {watchlist_path}")
            return watchlist_path
            
        except Exception as e:
            print(f"âŒ å„²å­˜è§€å¯Ÿåå–®å ±å‘Šå¤±æ•—: {e}")
            return ""

    def generate_statistics_report(self, processed_companies: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ğŸ”§ v3.6.1 ç”Ÿæˆçµ±è¨ˆå ±å‘Š - åŒ…å«è§€å¯Ÿåå–®çµ±è¨ˆ"""
        total_companies = len(processed_companies)
        
        if total_companies == 0:
            return {
                'total_companies': 0,
                'companies_with_data': 0,
                'success_rate': 0
            }
        
        # åŸºæœ¬çµ±è¨ˆ
        companies_with_data = len([c for c in processed_companies if c.get('quality_score', 0) > 0])
        success_rate = (companies_with_data / total_companies) * 100
        
        # å“è³ªåˆ†æ
        quality_scores = [c.get('quality_score', 0) for c in processed_companies]
        
        # content_date æå–åˆ†æ
        companies_with_content_date = len([c for c in processed_companies if self._get_content_date_only(c)])
        content_date_success_rate = (companies_with_content_date / total_companies) * 100
        
        # é©—è­‰çµ±è¨ˆ
        validation_passed = 0
        validation_failed = 0
        validation_disabled = 0
        
        for company in processed_companies:
            validation_result = company.get('validation_result', {})
            validation_method = validation_result.get('validation_method', 'unknown')
            
            if validation_method == 'disabled':
                validation_disabled += 1
            elif self._should_include_in_report_v351(company):
                validation_passed += 1
            else:
                validation_failed += 1
        
        validation_success_rate = (validation_passed / total_companies) * 100
        
        # éæ¿¾çµ±è¨ˆ
        companies_included_in_report = len([c for c in processed_companies if self._should_include_in_report_v351(c)])
        inclusion_rate = (companies_included_in_report / total_companies) * 100
        
        # ğŸ†• è§€å¯Ÿåå–®é—œè¯çµ±è¨ˆ
        watchlist_companies = len([c for c in processed_companies 
                                 if c.get('company_code', '') and 
                                 self._is_watchlist_company(c.get('company_code', ''))])
        
        statistics = {
            'version': '3.6.1',
            'report_type': 'comprehensive_with_watchlist',
            'timestamp': datetime.now().isoformat(),
            
            # åŸºæœ¬çµ±è¨ˆ
            'total_companies': total_companies,
            'companies_with_data': companies_with_data,
            'success_rate': round(success_rate, 1),
            'companies_with_content_date': companies_with_content_date,
            'content_date_success_rate': round(content_date_success_rate, 1),
            
            # é©—è­‰çµ±è¨ˆ
            'validation_statistics': {
                'validation_passed': validation_passed,
                'validation_failed': validation_failed,
                'validation_disabled': validation_disabled,
                'validation_success_rate': round(validation_success_rate, 1),
                'companies_included_in_report': companies_included_in_report,
                'inclusion_rate': round(inclusion_rate, 1),
                'filtered_out': total_companies - companies_included_in_report
            },
            
            # å“è³ªåˆ†æ
            'quality_analysis': {
                'average_quality_score': round(sum(quality_scores) / len(quality_scores), 1) if quality_scores else 0,
                'highest_quality_score': max(quality_scores) if quality_scores else 0,
                'lowest_quality_score': min(quality_scores) if quality_scores else 0
            },
            
            # ğŸ†• è§€å¯Ÿåå–®ç›¸é—œçµ±è¨ˆ
            'watchlist_statistics': {
                'watchlist_companies_in_data': watchlist_companies,
                'watchlist_coverage_in_data': round((watchlist_companies / total_companies) * 100, 1) if total_companies > 0 else 0
            }
        }
        
        return statistics

    def save_statistics_report(self, statistics: Dict[str, Any]) -> str:
        """å„²å­˜çµ±è¨ˆå ±å‘Šç‚º JSON æª”æ¡ˆ"""
        import json
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        stats_path = os.path.join(self.output_dir, f'statistics_v361_{timestamp}.json')
        latest_stats_path = os.path.join(self.output_dir, 'statistics_latest.json')
        
        try:
            with open(stats_path, 'w', encoding='utf-8') as f:
                json.dump(statistics, f, ensure_ascii=False, indent=2, default=str)
            
            with open(latest_stats_path, 'w', encoding='utf-8') as f:
                json.dump(statistics, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"ğŸ“Š v3.6.1 çµ±è¨ˆå ±å‘Šå·²å„²å­˜: {stats_path}")
            
            # é¡¯ç¤ºé‡è¦çµ±è¨ˆ
            validation_stats = statistics.get('validation_statistics', {})
            watchlist_stats = statistics.get('watchlist_statistics', {})
            
            print(f"ğŸ“Š é©—è­‰æˆåŠŸç‡: {validation_stats.get('validation_success_rate', 0)}%")
            print(f"ğŸ“Š å ±å‘ŠåŒ…å«ç‡: {validation_stats.get('inclusion_rate', 0)}%")
            print(f"ğŸ“Š è§€å¯Ÿåå–®è¦†è“‹: {watchlist_stats.get('watchlist_coverage_in_data', 0)}%")
            
            return stats_path
            
        except Exception as e:
            print(f"âŒ å„²å­˜çµ±è¨ˆå ±å‘Šå¤±æ•—: {e}")
            return ""

    # è¼”åŠ©æ–¹æ³•
    def _is_watchlist_company(self, company_code: str) -> bool:
        """ğŸ†• v3.6.1 æª¢æŸ¥æ˜¯å¦ç‚ºè§€å¯Ÿåå–®å…¬å¸ï¼ˆç°¡åŒ–æª¢æŸ¥ï¼‰"""
        # é€™è£¡å¯ä»¥åŠ è¼‰è§€å¯Ÿåå–®é€²è¡Œæª¢æŸ¥ï¼Œæˆ–è€…æ ¹æ“šä»£è™Ÿç¯„åœé€²è¡Œç°¡å–®åˆ¤æ–·
        if company_code and company_code.isdigit() and len(company_code) == 4:
            return True
        return False

    def _get_validation_status_display(self, status_info: Dict) -> str:
        """ğŸ†• v3.6.1 å–å¾—é©—è­‰ç‹€æ…‹é¡¯ç¤º"""
        validation_passed = status_info.get('validation_passed', True)
        validation_errors = status_info.get('validation_errors', [])
        
        if not validation_passed:
            if validation_errors:
                main_error = str(validation_errors[0])
                if "ä¸åœ¨è§€å¯Ÿåå–®" in main_error:
                    return "ğŸš« ä¸åœ¨è§€å¯Ÿåå–®"
                elif "åç¨±ä¸ç¬¦" in main_error:
                    return "ğŸ“ åç¨±ä¸ç¬¦"
                else:
                    return "âŒ é©—è­‰å¤±æ•—"
            else:
                return "âŒ é©—è­‰å¤±æ•—"
        else:
            return "âœ… é©—è­‰é€šé"

    def _clean_stock_code_for_display(self, code):
        """æ¸…ç†è‚¡ç¥¨ä»£è™Ÿï¼Œç¢ºä¿é¡¯ç¤ºç‚ºç´”æ•¸å­—ï¼ˆç„¡å¼•è™Ÿï¼‰"""
        if pd.isna(code) or code is None or code == '':
            return ''
        
        code_str = str(code).strip()
        
        if code_str.startswith("'"):
            code_str = code_str[1:]
        
        if code_str.isdigit():
            return code_str
        
        return code_str

    def _format_md_file_url_with_warning(self, company_data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ– MD æª”æ¡ˆé€£çµ"""
        filename = company_data.get('filename', '')
        
        if not filename:
            return ""
        
        if not filename.endswith('.md'):
            filename += '.md'
        
        encoded_filename = urllib.parse.quote(filename, safe='')
        raw_url = f"{self.github_repo_base}/data/md/{encoded_filename}"
        
        return raw_url

    def _get_content_date_only(self, company_data: Dict[str, Any]) -> str:
        """çµ•å°åªä½¿ç”¨ content_dateï¼Œç„¡ä»»ä½• fallback"""
        content_date = company_data.get('content_date')
        
        if content_date and str(content_date).strip() and str(content_date).strip().lower() != 'none':
            try:
                if isinstance(content_date, str) and '/' in content_date:
                    parts = content_date.split('/')
                    if len(parts) == 3:
                        year, month, day = parts
                        if (year.isdigit() and month.isdigit() and day.isdigit() and
                            1900 <= int(year) <= 2100 and 1 <= int(month) <= 12 and 1 <= int(day) <= 31):
                            return f"{year}-{int(month):02d}-{int(day):02d}"
                
                elif isinstance(content_date, datetime):
                    return content_date.strftime('%Y-%m-%d')
                    
            except Exception as e:
                pass
        
        return ""

    def _calculate_date_range_content_date_only(self, all_files: List[Dict]) -> tuple:
        """è¨ˆç®—æ—¥æœŸç¯„åœ - åªè€ƒæ…®æœ‰ content_date çš„æª”æ¡ˆ"""
        valid_dates = []
        
        for file_data in all_files:
            content_date = self._get_content_date_only(file_data)
            if content_date:
                valid_dates.append(content_date)
        
        if valid_dates:
            valid_dates.sort()
            return valid_dates[0], valid_dates[-1]
        
        return "", ""

    def _format_eps_value(self, eps_value) -> str:
        """æ ¼å¼åŒ– EPS æ•¸å€¼"""
        if eps_value is None or eps_value == '':
            return ''
        try:
            eps = float(eps_value)
            return f"{eps:.2f}"
        except (ValueError, TypeError):
            return str(eps_value)

    def _generate_validation_status_marker_v351(self, company_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆé©—è­‰ç‹€æ…‹æ¨™è¨˜"""
        validation_result = company_data.get('validation_result', {})
        validation_status = validation_result.get('overall_status', 'unknown')
        validation_method = validation_result.get('validation_method', 'unknown')
        validation_passed = company_data.get('content_validation_passed', True)
        validation_errors = company_data.get('validation_errors', [])
        validation_warnings = company_data.get('validation_warnings', [])
        validation_enabled = company_data.get('validation_enabled', False)
        
        # å¤šå±¤é©—è­‰ç‹€æ…‹åˆ¤æ–·
        if validation_status == 'error' or not validation_passed:
            if validation_errors:
                main_error = str(validation_errors[0])
                if "ä¸åœ¨è§€å¯Ÿåå–®" in main_error:
                    return "ğŸš« ä¸åœ¨è§€å¯Ÿåå–®"
                elif "å…¬å¸åç¨±ä¸ç¬¦è§€å¯Ÿåå–®" in main_error:
                    return "ğŸ“ åç¨±ä¸ç¬¦"
                elif "æ„›æ´¾å¸" in main_error or "æ„›ç«‹ä¿¡" in main_error:
                    return "ğŸ”„ åç¨±æ··äº‚"
                else:
                    return "âŒ é©—è­‰å¤±æ•—"
            else:
                return "âŒ é©—è­‰å¤±æ•—"
        
        elif validation_method == 'disabled' or not validation_enabled:
            return "âš ï¸ é©—è­‰åœç”¨"
        
        elif validation_warnings:
            return "âš ï¸ æœ‰è­¦å‘Š"
        
        else:
            return "âœ… é€šé"

    def _get_quality_status_by_score(self, score: float) -> str:
        """å–å¾—å“è³ªç‹€æ…‹æŒ‡æ¨™"""
        if score >= 9:
            return "ğŸŸ¢ å„ªç§€"
        elif score >= 7:
            return "ğŸŸ¡ è‰¯å¥½"
        elif score >= 5:
            return "ğŸŸ  æ™®é€š"
        else:
            return "ğŸ”´ éœ€æ”¹å–„"


# æ¸¬è©¦åŠŸèƒ½
if __name__ == "__main__":
    generator = ReportGenerator()
    
    print("=== ğŸ†• v3.6.1 æ¨™æº–åŒ–æŸ¥è©¢æ¨¡å¼å ±å‘Šç”Ÿæˆå™¨æ¸¬è©¦ ===")
    
    # æ¸¬è©¦æ¨™æº–åŒ–æŸ¥è©¢æ¨¡å¼åˆ†ææ•¸æ“š
    test_pattern_analysis = {
        'analysis_type': 'search_query_patterns_normalized',
        'pattern_stats': {
            '{name} {symbol} factset åˆ†æå¸«': {
                'usage_count': 30,
                'avg_quality_score': 7.89,
                'max_quality_score': 8.7,
                'min_quality_score': 4.3,
                'company_count': 15,
                'pattern_type': 'factset_direct'
            },
            '{name} {symbol} factset eps': {
                'usage_count': 25,
                'avg_quality_score': 8.5,
                'max_quality_score': 9.2,
                'min_quality_score': 7.1,
                'company_count': 12,
                'pattern_type': 'factset_direct'
            },
            'site:cnyes.com {symbol} factset': {
                'usage_count': 18,
                'avg_quality_score': 7.2,
                'max_quality_score': 8.1,
                'min_quality_score': 6.3,
                'company_count': 10,
                'pattern_type': 'cnyes_factset'
            }
        }
    }
    
    print("æ¸¬è©¦ 1: ç”Ÿæˆæ¨™æº–åŒ–æŸ¥è©¢æ¨¡å¼å ±å‘Š")
    pattern_summary = generator.generate_keyword_summary(test_pattern_analysis)
    
    print(f"   ç”Ÿæˆçš„æŸ¥è©¢æ¨¡å¼å ±å‘ŠåŒ…å« {len(pattern_summary)} å€‹æ¨¡å¼")
    print("   å‰å¹¾è¡Œæ•¸æ“š:")
    for i, row in pattern_summary.head().iterrows():
        pattern = row['Query pattern']
        usage = row['ä½¿ç”¨æ¬¡æ•¸']
        quality = row['å¹³å‡å“è³ªè©•åˆ†']
        category = row['åˆ†é¡']
        print(f"     {pattern}: ä½¿ç”¨ {usage} æ¬¡, å¹³å‡å“è³ª {quality}, åˆ†é¡: {category}")
    
    print("\næ¸¬è©¦ 2: å„²å­˜æŸ¥è©¢æ¨¡å¼ CSV")
    csv_file = generator.save_keyword_summary(pattern_summary)
    if csv_file:
        print(f"   âœ… æŸ¥è©¢æ¨¡å¼ CSV å·²å„²å­˜: {csv_file}")
    
    print(f"\nğŸ‰ v3.6.1 æ¨™æº–åŒ–æŸ¥è©¢æ¨¡å¼å ±å‘Šç”Ÿæˆå™¨æ¸¬è©¦å®Œæˆ!")
    print(f"âœ… ç”Ÿæˆæ¨™æº–åŒ–æŸ¥è©¢æ¨¡å¼çµ±è¨ˆå ±å‘Š (Query pattern æ¬„ä½)")
    print(f"âœ… æ”¯æ´ {{name}} {{symbol}} æ ¼å¼çš„æ¨¡å¼èšåˆ")
    print(f"âœ… æŒ‰æ¨¡å¼é¡å‹åˆ†é¡å’Œå“è³ªè©•åˆ†æ’åº")
    print(f"âœ… å®Œæ•´çš„ CSV å„²å­˜åŠŸèƒ½")