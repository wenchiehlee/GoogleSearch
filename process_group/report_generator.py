#!/usr/bin/env python3
"""
Report Generator - FactSet Pipeline v3.6.1 (Updated for md_date)
Updated to use md_date field from Search Group metadata
Enhanced MDæ—¥æœŸ handling with reliable metadata source
"""

import os
import re
import pandas as pd
import urllib.parse
from datetime import datetime
from typing import Dict, Any, List, Optional
import pytz

class ReportGenerator:
    """å ±å‘Šç”Ÿæˆå™¨ v3.6.1-updated - ä½¿ç”¨ Search Group çš„ md_date æ¬„ä½"""
    
    def __init__(self, github_repo_base="https://raw.githubusercontent.com/wenchiehlee/GoogleSearch/refs/heads/main"):
        self.github_repo_base = github_repo_base
        self.output_dir = "data/reports"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # è¨­å®šå°åŒ—æ™‚å€
        self.taipei_tz = pytz.timezone('Asia/Taipei')
        
        # æŠ•è³‡çµ„åˆæ‘˜è¦æ¬„ä½ (14 æ¬„ä½)
        self.portfolio_summary_columns = [
            'ä»£è™Ÿ', 'åç¨±', 'è‚¡ç¥¨ä»£è™Ÿ', 'MDæœ€èˆŠæ—¥æœŸ', 'MDæœ€æ–°æ—¥æœŸ', 'MDè³‡æ–™ç­†æ•¸',
            'åˆ†æå¸«æ•¸é‡', 'ç›®æ¨™åƒ¹', '2025EPSå¹³å‡å€¼', '2026EPSå¹³å‡å€¼', '2027EPSå¹³å‡å€¼',
            'å“è³ªè©•åˆ†', 'ç‹€æ…‹', 'æ›´æ–°æ—¥æœŸ'
        ]

        # è©³ç´°å ±å‘Šæ¬„ä½ (22 æ¬„ä½ - åŒ…å«é©—è­‰ç‹€æ…‹)
        self.detailed_report_columns = [
            'ä»£è™Ÿ', 'åç¨±', 'è‚¡ç¥¨ä»£è™Ÿ', 'MDæ—¥æœŸ', 'åˆ†æå¸«æ•¸é‡', 'ç›®æ¨™åƒ¹',
            '2025EPSæœ€é«˜å€¼', '2025EPSæœ€ä½å€¼', '2025EPSå¹³å‡å€¼',
            '2026EPSæœ€é«˜å€¼', '2026EPSæœ€ä½å€¼', '2026EPSå¹³å‡å€¼',
            '2027EPSæœ€é«˜å€¼', '2027EPSæœ€ä½å€¼', '2027EPSå¹³å‡å€¼',
            'å“è³ªè©•åˆ†', 'ç‹€æ…‹', 'é©—è­‰ç‹€æ…‹', 'MD File', 'æ›´æ–°æ—¥æœŸ'
        ]

        # é—œéµå­—å ±å‘Šæ¬„ä½ (10 æ¬„ä½) - ä¿ç•™å‚³çµ±é—œéµå­—åˆ†æç”¨
        self.keyword_summary_columns = [
            'é—œéµå­—', 'ä½¿ç”¨æ¬¡æ•¸', 'å¹³å‡å“è³ªè©•åˆ†', 'æœ€é«˜å“è³ªè©•åˆ†', 'æœ€ä½å“è³ªè©•åˆ†',
            'ç›¸é—œå…¬å¸æ•¸é‡', 'å“è³ªç‹€æ…‹', 'åˆ†é¡', 'æ•ˆæœè©•ç´š', 'æ›´æ–°æ—¥æœŸ'
        ]
        
        # æŸ¥è©¢æ¨¡å¼å ±å‘Šæ¬„ä½ (10 æ¬„ä½) - æ–°å¢æ¨™æº–åŒ–æŸ¥è©¢æ¨¡å¼
        self.query_pattern_summary_columns = [
            'Query pattern', 'ä½¿ç”¨æ¬¡æ•¸', 'å¹³å‡å“è³ªè©•åˆ†', 'æœ€é«˜å“è³ªè©•åˆ†', 'æœ€ä½å“è³ªè©•åˆ†',
            'ç›¸é—œå…¬å¸æ•¸é‡', 'å“è³ªç‹€æ…‹', 'åˆ†é¡', 'æ•ˆæœè©•ç´š', 'æ›´æ–°æ—¥æœŸ'
        ]
        
        # è§€å¯Ÿåå–®å ±å‘Šæ¬„ä½ (12 æ¬„ä½) - v3.6.1 æ–°å¢
        self.watchlist_summary_columns = [
            'å…¬å¸ä»£è™Ÿ', 'å…¬å¸åç¨±', 'MDæª”æ¡ˆæ•¸é‡', 'è™•ç†ç‹€æ…‹', 'å¹³å‡å“è³ªè©•åˆ†', 'æœ€é«˜å“è³ªè©•åˆ†',
            'æœå°‹é—œéµå­—æ•¸é‡', 'ä¸»è¦é—œéµå­—', 'é—œéµå­—å¹³å‡å“è³ª', 'æœ€æ–°æª”æ¡ˆæ—¥æœŸ', 'é©—è­‰ç‹€æ…‹', 'æ›´æ–°æ—¥æœŸ'
        ]

    def _get_taipei_time(self) -> str:
        """å–å¾—å°åŒ—æ™‚é–“çš„å­—ä¸²æ ¼å¼"""
        taipei_time = datetime.now(self.taipei_tz)
        return taipei_time.strftime('%Y-%m-%d %H:%M:%S')

    def _should_include_in_report_v351_updated(self, company_data: Dict[str, Any]) -> bool:
        """UPDATED: åˆ¤æ–·æ˜¯å¦æ‡‰è©²å°‡æ­¤è³‡æ–™åŒ…å«åœ¨å ±å‘Šä¸­ - ä½¿ç”¨ md_date å„ªå…ˆé‚è¼¯"""
        
        # æª¢æŸ¥ 1: åŸºæœ¬è³‡æ–™å®Œæ•´æ€§
        company_code = company_data.get('company_code')
        company_name = company_data.get('company_name')
        
        if not company_code or company_code == 'Unknown':
            return False
        
        if not company_name or company_name == 'Unknown':
            return False
        
        # UPDATED: md_date æª¢æŸ¥ - ç¾åœ¨ md_date æ‡‰è©²ç”± Search Group æä¾›
        # ä¸å†å¼·åˆ¶æ’é™¤ï¼Œä½†æœƒåœ¨å“è³ªè©•åˆ†ä¸­åæ˜ 
        
        # æª¢æŸ¥ 2: é©—è­‰çµæœæª¢æŸ¥
        validation_result = company_data.get('validation_result', {})
        validation_status = validation_result.get('overall_status', 'unknown')
        
        if validation_status == 'error':
            return False
        
        # æª¢æŸ¥ 3: content_validation_passed å­—æ®µ
        validation_passed = company_data.get('content_validation_passed', True)
        if not validation_passed:
            return False
        
        # æª¢æŸ¥ 4: æª¢æŸ¥é—œéµé©—è­‰éŒ¯èª¤
        validation_errors = company_data.get('validation_errors', [])
        if validation_errors:
            critical_error_keywords = [
                'ä¸åœ¨è§€å¯Ÿåå–®ä¸­',
                'å…¬å¸åç¨±ä¸ç¬¦è§€å¯Ÿåå–®',
                'è§€å¯Ÿåå–®é¡¯ç¤ºæ‡‰ç‚º',
                'æ„›æ´¾çˆ¾.*æ„›ç«‹ä¿¡',
                'æ„›ç«‹ä¿¡.*æ„›æ´¾çˆ¾',
                'å…¬å¸ä»£è™Ÿæ ¼å¼ç„¡æ•ˆ'
            ]
            
            for error in validation_errors:
                error_str = str(error)
                if any(re.search(keyword, error_str, re.IGNORECASE) for keyword in critical_error_keywords):
                    return False
        
        # æª¢æŸ¥ 5: å“è³ªè©•åˆ†ç‚º 0 ä¸”æœ‰é©—è­‰å¤±æ•—ç‹€æ…‹çš„ç‰¹æ®Šæƒ…æ³
        quality_score = company_data.get('quality_score', 0)
        quality_status = company_data.get('quality_status', '')
        
        if quality_score == 0 and 'é©—è­‰å¤±æ•—' in quality_status:
            return False
        
        return True

    def generate_portfolio_summary(self, processed_companies: List[Dict], filter_invalid=True) -> pd.DataFrame:
        """UPDATED: ç”ŸæˆæŠ•è³‡çµ„åˆæ‘˜è¦ - ä½¿ç”¨ md_date å„ªå…ˆé‚è¼¯"""
        try:
            # å¢å¼·éæ¿¾é‚è¼¯ - ä½¿ç”¨æ›´æ–°çš„éæ¿¾æ–¹æ³•
            if filter_invalid:
                original_count = len(processed_companies)
                valid_companies = []
                filtered_reasons = {
                    'validation_failed': 0,
                    'other_issues': 0
                }
                
                # UPDATED: çµ±è¨ˆ md_date æƒ…æ³
                md_date_stats = {
                    'total_files': 0,
                    'with_md_date': 0,
                    'with_content_date_fallback': 0,
                    'no_date_available': 0,
                    'search_group_coverage': 0
                }
                
                for company in processed_companies:
                    md_date_stats['total_files'] += 1
                    
                    # æª¢æŸ¥æ—¥æœŸä¾†æº
                    md_date = self._get_md_date_with_priority(company)
                    md_date_source = self._get_md_date_source(company)
                    
                    if md_date_source == 'md_date':
                        md_date_stats['with_md_date'] += 1
                        md_date_stats['search_group_coverage'] += 1
                    elif md_date_source == 'content_date':
                        md_date_stats['with_content_date_fallback'] += 1
                    else:
                        md_date_stats['no_date_available'] += 1
                    
                    # æ‡‰ç”¨éæ¿¾é‚è¼¯
                    if not self._should_include_in_report_v351_updated(company):
                        # é€²ä¸€æ­¥åˆ†é¡åŸå› 
                        validation_result = company.get('validation_result', {})
                        if validation_result.get('overall_status') == 'error':
                            filtered_reasons['validation_failed'] += 1
                        else:
                            filtered_reasons['other_issues'] += 1
                        continue
                    
                    valid_companies.append(company)
                
                # ENHANCED: è©³ç´°çµ±è¨ˆè¼¸å‡º
                search_group_coverage = (md_date_stats['search_group_coverage'] / md_date_stats['total_files'] * 100) if md_date_stats['total_files'] > 0 else 0
                total_date_coverage = ((md_date_stats['with_md_date'] + md_date_stats['with_content_date_fallback']) / md_date_stats['total_files'] * 100) if md_date_stats['total_files'] > 0 else 0
                
                print(f"ğŸ“Š æŠ•è³‡çµ„åˆæ‘˜è¦éæ¿¾çµæœ:")
                print(f"   åŸå§‹å…¬å¸æ•¸: {original_count}")
                print(f"   ä¿ç•™å…¬å¸æ•¸: {len(valid_companies)}")
                print(f"   æ’é™¤åŸå› :")
                print(f"     é©—è­‰å¤±æ•—: {filtered_reasons['validation_failed']}")
                print(f"     å…¶ä»–å•é¡Œ: {filtered_reasons['other_issues']}")
                print(f"")
                print(f"ğŸ“… MDæ—¥æœŸä¾†æºçµ±è¨ˆ:")
                print(f"   Search Group (md_date): {md_date_stats['with_md_date']}")
                print(f"   Process Group (content_date): {md_date_stats['with_content_date_fallback']}")
                print(f"   ç„¡æ—¥æœŸ: {md_date_stats['no_date_available']}")
                print(f"   Search Group è¦†è“‹ç‡: {search_group_coverage:.1f}%")
                print(f"   ç¸½æ—¥æœŸè¦†è“‹ç‡: {total_date_coverage:.1f}%")
            else:
                valid_companies = processed_companies
                print(f"ğŸ“Š æŠ•è³‡çµ„åˆæ‘˜è¦ï¼šæœªå•Ÿç”¨éæ¿¾ï¼ŒåŒ…å«æ‰€æœ‰ {len(valid_companies)} å®¶å…¬å¸")
            
            # æŒ‰å…¬å¸åˆ†çµ„ï¼Œå–å¾—æ¯å®¶å…¬å¸çš„æœ€ä½³å“è³ªè³‡æ–™
            company_summary = {}
            
            for company_data in valid_companies:
                company_code = company_data.get('company_code', 'Unknown')
                
                if company_code not in company_summary:
                    company_summary[company_code] = {
                        'files': [],
                        'best_quality_data': None
                    }
                
                company_summary[company_code]['files'].append(company_data)
                
                # é¸æ“‡æœ€ä½³å“è³ªè³‡æ–™
                current_best = company_summary[company_code]['best_quality_data']
                
                if current_best is None:
                    company_summary[company_code]['best_quality_data'] = company_data
                else:
                    current_quality = company_data.get('quality_score', 0)
                    best_quality = current_best.get('quality_score', 0)
                    
                    if current_quality > best_quality:
                        company_summary[company_code]['best_quality_data'] = company_data
            
            # ç”Ÿæˆæ‘˜è¦è³‡æ–™
            summary_rows = []
            
            for company_code, company_info in company_summary.items():
                best_data = company_info['best_quality_data']
                all_files = company_info['files']
                
                # UPDATED: è¨ˆç®—æ—¥æœŸç¯„åœ - ä½¿ç”¨ md_date å„ªå…ˆé‚è¼¯
                oldest_date, newest_date = self._calculate_date_range_with_priority(all_files)
                
                # ä½¿ç”¨å¢å¼·çš„å“è³ªç‹€æ…‹é¡¯ç¤º
                quality_score = best_data.get('quality_score', 0)
                md_date = self._get_md_date_with_priority(best_data)
                has_date = bool(md_date and md_date.strip())
                quality_status = self._get_quality_status_by_score_enhanced(quality_score, has_date)
                
                # ä½¿ç”¨æœ€ä½³å“è³ªè³‡æ–™ç”Ÿæˆæ‘˜è¦
                clean_code = self._clean_stock_code_for_display(company_code)
                
                summary_row = {
                    'ä»£è™Ÿ': clean_code,
                    'åç¨±': best_data.get('company_name', 'Unknown'),
                    'è‚¡ç¥¨ä»£è™Ÿ': f"{clean_code}-TW",
                    'MDæœ€èˆŠæ—¥æœŸ': oldest_date,
                    'MDæœ€æ–°æ—¥æœŸ': newest_date,
                    'MDè³‡æ–™ç­†æ•¸': len(all_files),
                    'åˆ†æå¸«æ•¸é‡': best_data.get('analyst_count', 0),
                    'ç›®æ¨™åƒ¹': best_data.get('target_price', ''),
                    '2025EPSå¹³å‡å€¼': self._format_eps_value(best_data.get('eps_2025_avg')),
                    '2026EPSå¹³å‡å€¼': self._format_eps_value(best_data.get('eps_2026_avg')),
                    '2027EPSå¹³å‡å€¼': self._format_eps_value(best_data.get('eps_2027_avg')),
                    'å“è³ªè©•åˆ†': quality_score,
                    'ç‹€æ…‹': quality_status,
                    'æ›´æ–°æ—¥æœŸ': self._get_taipei_time()
                }
                
                summary_rows.append(summary_row)
            
            # å»ºç«‹ DataFrame
            df = pd.DataFrame(summary_rows, columns=self.portfolio_summary_columns)
            df = df.sort_values('ä»£è™Ÿ')
            
            print(f"âœ… æŠ•è³‡çµ„åˆæ‘˜è¦å·²ä½¿ç”¨æœ€ä½³å“è³ªè³‡æ–™ç”Ÿæˆ")
            
            return df
            
        except Exception as e:
            print(f"âŒ ç”ŸæˆæŠ•è³‡çµ„åˆæ‘˜è¦å¤±æ•—: {e}")
            return pd.DataFrame(columns=self.portfolio_summary_columns)

    def generate_detailed_report(self, processed_companies: List[Dict], filter_invalid=True) -> pd.DataFrame:
        """UPDATED: ç”Ÿæˆè©³ç´°å ±å‘Š - ä½¿ç”¨ md_date å„ªå…ˆé‚è¼¯"""
        try:
            detailed_rows = []
            filtered_count = 0
            date_source_stats = {
                'md_date': 0,
                'content_date': 0,
                'no_date': 0
            }
            
            for company_data in processed_companies:
                # æª¢æŸ¥æ˜¯å¦æ‡‰è©²éæ¿¾æ­¤è³‡æ–™
                if filter_invalid and not self._should_include_in_report_v351_updated(company_data):
                    filtered_count += 1
                    continue
                
                # UPDATED: ä½¿ç”¨ md_date å„ªå…ˆé‚è¼¯å–å¾—æ—¥æœŸ
                md_date = self._get_md_date_with_priority(company_data)
                date_source = self._get_md_date_source(company_data)
                
                # çµ±è¨ˆæ—¥æœŸä¾†æº
                if date_source == 'md_date':
                    date_source_stats['md_date'] += 1
                elif date_source == 'content_date':
                    date_source_stats['content_date'] += 1
                else:
                    date_source_stats['no_date'] += 1
                
                quality_score = company_data.get('quality_score', 0)
                has_date = bool(md_date and md_date.strip())
                
                # ç”Ÿæˆå¢å¼·é©—è­‰ç‹€æ…‹æ¨™è¨˜
                validation_status = self._generate_validation_status_marker_v351(company_data)
                
                # ä½¿ç”¨å¢å¼·çš„å“è³ªç‹€æ…‹é¡¯ç¤º
                quality_status = self._get_quality_status_by_score_enhanced(quality_score, has_date)
                
                # è™•ç† MD æª”æ¡ˆé€£çµ
                md_file_url = self._format_md_file_url_with_warning(company_data)
                
                detailed_row = {
                    'ä»£è™Ÿ': company_data.get('company_code', 'Unknown'),
                    'åç¨±': company_data.get('company_name', 'Unknown'),
                    'è‚¡ç¥¨ä»£è™Ÿ': f"{company_data.get('company_code', 'Unknown')}-TW",
                    'MDæ—¥æœŸ': md_date,  # UPDATED: ä½¿ç”¨å„ªå…ˆé‚è¼¯å–å¾—çš„æ—¥æœŸ
                    'åˆ†æå¸«æ•¸é‡': company_data.get('analyst_count', 0),
                    'ç›®æ¨™åƒ¹': company_data.get('target_price', ''),
                    '2025EPSæœ€é«˜å€¼': self._format_eps_value(company_data.get('eps_2025_high')),
                    '2025EPSæœ€ä½å€¼': self._format_eps_value(company_data.get('eps_2025_low')),
                    '2025EPSå¹³å‡å€¼': self._format_eps_value(company_data.get('eps_2025_avg')),
                    '2026EPSæœ€é«˜å€¼': self._format_eps_value(company_data.get('eps_2026_high')),
                    '2026EPSæœ€ä½å€¼': self._format_eps_value(company_data.get('eps_2026_low')),
                    '2026EPSå¹³å‡å€¼': self._format_eps_value(company_data.get('eps_2026_avg')),
                    '2027EPSæœ€é«˜å€¼': self._format_eps_value(company_data.get('eps_2027_high')),
                    '2027EPSæœ€ä½å€¼': self._format_eps_value(company_data.get('eps_2027_low')),
                    '2027EPSå¹³å‡å€¼': self._format_eps_value(company_data.get('eps_2027_avg')),
                    'å“è³ªè©•åˆ†': quality_score,
                    'ç‹€æ…‹': quality_status,
                    'é©—è­‰ç‹€æ…‹': validation_status,
                    'MD File': md_file_url,
                    'æ›´æ–°æ—¥æœŸ': self._get_taipei_time()
                }
                
                detailed_rows.append(detailed_row)
            
            # å»ºç«‹ DataFrame
            df = pd.DataFrame(detailed_rows, columns=self.detailed_report_columns)
            df = df.sort_values(['ä»£è™Ÿ', 'MDæ—¥æœŸ'], ascending=[True, False])
            
            # ENHANCED: è©³ç´°çµ±è¨ˆè¼¸å‡º
            total_files = len(detailed_rows)
            
            print(f"ğŸ“Š è©³ç´°å ±å‘Šçµ±è¨ˆ:")
            print(f"   åŒ…å«æª”æ¡ˆæ•¸: {total_files}")
            print(f"   éæ¿¾æª”æ¡ˆæ•¸: {filtered_count}")
            print(f"ğŸ“… MDæ—¥æœŸä¾†æºåˆ†å¸ƒ:")
            print(f"   Search Group (md_date): {date_source_stats['md_date']}")
            print(f"   Process Group (content_date): {date_source_stats['content_date']}")
            print(f"   ç„¡æ—¥æœŸ: {date_source_stats['no_date']}")
            
            search_group_coverage = (date_source_stats['md_date'] / total_files * 100) if total_files > 0 else 0
            print(f"   Search Group è¦†è“‹ç‡: {search_group_coverage:.1f}%")
            
            return df
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆè©³ç´°å ±å‘Šå¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame(columns=self.detailed_report_columns)

    # UPDATED: æ–°å¢ md_date å„ªå…ˆé‚è¼¯çš„æ–¹æ³•
    def _get_md_date_with_priority(self, company_data: Dict[str, Any]) -> str:
        """UPDATED: ä½¿ç”¨å„ªå…ˆé †åºå–å¾— MD æ—¥æœŸ: md_date > content_date > empty"""
        
        # Priority 1: md_date from Search Group metadata
        yaml_data = company_data.get('yaml_data', {})
        md_date = yaml_data.get('md_date', '')
        
        if md_date and md_date.strip():
            # Validate and format md_date
            formatted_date = self._format_date_for_display(md_date)
            if formatted_date:
                return formatted_date
        
        # Priority 2: content_date from Process Group extraction
        content_date = company_data.get('content_date', '')
        if content_date and content_date.strip():
            formatted_date = self._format_date_for_display(content_date)
            if formatted_date:
                return formatted_date
        
        # Priority 3: No date available
        return ""

    def _get_md_date_source(self, company_data: Dict[str, Any]) -> str:
        """UPDATED: å–å¾— MD æ—¥æœŸçš„ä¾†æº"""
        
        # Check md_date first
        yaml_data = company_data.get('yaml_data', {})
        md_date = yaml_data.get('md_date', '')
        
        if md_date and md_date.strip():
            return 'md_date'
        
        # Check content_date
        content_date = company_data.get('content_date', '')
        if content_date and content_date.strip():
            return 'content_date'
        
        return 'no_date'

    def _format_date_for_display(self, date_str: str) -> str:
        """æ ¼å¼åŒ–æ—¥æœŸç‚ºé¡¯ç¤ºæ ¼å¼ YYYY-MM-DD"""
        if not date_str or not date_str.strip():
            return ""
        
        try:
            date_str = date_str.strip()
            
            # Handle YYYY/MM/DD format
            if '/' in date_str:
                parts = date_str.split('/')
                if len(parts) == 3:
                    year, month, day = parts
                    if (year.isdigit() and month.isdigit() and day.isdigit() and
                        1900 <= int(year) <= 2100 and 1 <= int(month) <= 12 and 1 <= int(day) <= 31):
                        return f"{year}-{int(month):02d}-{int(day):02d}"
            
            # Handle YYYY-MM-DD format (already correct)
            elif '-' in date_str and len(date_str) >= 8:
                parts = date_str.split('-')
                if len(parts) >= 3:
                    year, month, day = parts[0], parts[1], parts[2]
                    if (year.isdigit() and month.isdigit() and day.isdigit() and
                        1900 <= int(year) <= 2100 and 1 <= int(month) <= 12 and 1 <= int(day) <= 31):
                        return f"{year}-{int(month):02d}-{int(day):02d}"
            
            # Handle datetime objects
            elif isinstance(date_str, datetime):
                return date_str.strftime('%Y-%m-%d')
                
        except Exception as e:
            print(f"âš ï¸ æ—¥æœŸæ ¼å¼åŒ–å¤±æ•—: {date_str} - {e}")
        
        return ""

    def _calculate_date_range_with_priority(self, all_files: List[Dict]) -> tuple:
        """UPDATED: è¨ˆç®—æ—¥æœŸç¯„åœ - ä½¿ç”¨ md_date å„ªå…ˆé‚è¼¯"""
        valid_dates = []
        
        for file_data in all_files:
            md_date = self._get_md_date_with_priority(file_data)
            if md_date:
                valid_dates.append(md_date)
        
        if valid_dates:
            valid_dates.sort()
            return valid_dates[0], valid_dates[-1]
        
        return "", ""

    def generate_statistics_report(self, processed_companies: List[Dict[str, Any]]) -> Dict[str, Any]:
        """UPDATED: ç”Ÿæˆçµ±è¨ˆå ±å‘Š - åŒ…å« md_date çµ±è¨ˆ"""
        total_companies = len(processed_companies)
        
        if total_companies == 0:
            return {
                'total_companies': 0,
                'companies_with_data': 0,
                'success_rate': 0
            }
        
        # åŸºæœ¬çµ±è¨ˆ
        companies_with_data = len([c for c in processed_companies if c.get('quality_score', 0) > 0])
        success_rate = (companies_with_data / total_companies) * 100
        
        # å“è³ªåˆ†æ
        quality_scores = [c.get('quality_score', 0) for c in processed_companies]
        
        # ENHANCED: MDæ—¥æœŸä¾†æºè©³ç´°åˆ†æ
        md_date_analysis = {
            'total_files': total_companies,
            'md_date_from_search_group': 0,
            'content_date_from_process_group': 0,
            'no_date_available': 0,
            'search_group_coverage_rate': 0,
            'total_date_coverage_rate': 0
        }
        
        for company in processed_companies:
            date_source = self._get_md_date_source(company)
            
            if date_source == 'md_date':
                md_date_analysis['md_date_from_search_group'] += 1
            elif date_source == 'content_date':
                md_date_analysis['content_date_from_process_group'] += 1
            else:
                md_date_analysis['no_date_available'] += 1
        
        # è¨ˆç®—è¦†è“‹ç‡
        md_date_analysis['search_group_coverage_rate'] = (md_date_analysis['md_date_from_search_group'] / total_companies * 100)
        md_date_analysis['total_date_coverage_rate'] = ((md_date_analysis['md_date_from_search_group'] + md_date_analysis['content_date_from_process_group']) / total_companies * 100)
        
        # é©—è­‰çµ±è¨ˆ
        validation_passed = 0
        validation_failed = 0
        validation_disabled = 0
        
        for company in processed_companies:
            validation_result = company.get('validation_result', {})
            validation_method = validation_result.get('validation_method', 'unknown')
            
            if validation_method == 'disabled':
                validation_disabled += 1
            elif self._should_include_in_report_v351_updated(company):
                validation_passed += 1
            else:
                validation_failed += 1
        
        validation_success_rate = (validation_passed / total_companies) * 100
        
        # éæ¿¾çµ±è¨ˆ
        companies_included_in_report = len([c for c in processed_companies if self._should_include_in_report_v351_updated(c)])
        inclusion_rate = (companies_included_in_report / total_companies) * 100
        
        # è§€å¯Ÿåå–®ç›¸é—œçµ±è¨ˆ
        watchlist_companies = len([c for c in processed_companies 
                                 if c.get('company_code', '') and 
                                 self._is_watchlist_company(c.get('company_code', ''))])
        
        statistics = {
            'version': '3.6.1-updated-for-md-date',
            'report_type': 'comprehensive_with_md_date_priority',
            'timestamp': datetime.now().isoformat(),
            
            # åŸºæœ¬çµ±è¨ˆ
            'total_companies': total_companies,
            'companies_with_data': companies_with_data,
            'success_rate': round(success_rate, 1),
            
            # ENHANCED: MDæ—¥æœŸä¾†æºçµ±è¨ˆ
            'md_date_source_analysis': {
                'total_files_processed': md_date_analysis['total_files'],
                'md_date_from_search_group': md_date_analysis['md_date_from_search_group'],
                'content_date_from_process_group': md_date_analysis['content_date_from_process_group'],
                'no_date_available': md_date_analysis['no_date_available'],
                'search_group_coverage_rate': round(md_date_analysis['search_group_coverage_rate'], 1),
                'total_date_coverage_rate': round(md_date_analysis['total_date_coverage_rate'], 1),
                'priority_system': 'md_date (Search Group) > content_date (Process Group) > empty',
                'improvement_note': f"Search Group æä¾›äº† {md_date_analysis['md_date_from_search_group']} å€‹æª”æ¡ˆçš„ md_dateï¼ŒProcess Group æä¾›äº† {md_date_analysis['content_date_from_process_group']} å€‹ fallback æ—¥æœŸ"
            },
            
            # é©—è­‰çµ±è¨ˆ
            'validation_statistics': {
                'validation_passed': validation_passed,
                'validation_failed': validation_failed,
                'validation_disabled': validation_disabled,
                'validation_success_rate': round(validation_success_rate, 1),
                'companies_included_in_report': companies_included_in_report,
                'inclusion_rate': round(inclusion_rate, 1),
                'filtered_out': total_companies - companies_included_in_report
            },
            
            # å“è³ªåˆ†æ
            'quality_analysis': {
                'average_quality_score': round(sum(quality_scores) / len(quality_scores), 1) if quality_scores else 0,
                'highest_quality_score': max(quality_scores) if quality_scores else 0,
                'lowest_quality_score': min(quality_scores) if quality_scores else 0,
                'files_with_quality_1_or_less': len([s for s in quality_scores if s <= 1]),
                'quality_distribution': {
                    'excellent_9_10': len([s for s in quality_scores if s >= 9]),
                    'good_7_8': len([s for s in quality_scores if 7 <= s < 9]),
                    'fair_5_6': len([s for s in quality_scores if 5 <= s < 7]),
                    'poor_3_4': len([s for s in quality_scores if 3 <= s < 5]),
                    'very_poor_1_2': len([s for s in quality_scores if 1 <= s < 3]),
                    'missing_date_or_error_0_1': len([s for s in quality_scores if s <= 1])
                }
            },
            
            # è§€å¯Ÿåå–®ç›¸é—œçµ±è¨ˆ
            'watchlist_statistics': {
                'watchlist_companies_in_data': watchlist_companies,
                'watchlist_coverage_in_data': round((watchlist_companies / total_companies) * 100, 1) if total_companies > 0 else 0
            }
        }
        
        return statistics

    # ä¿æŒå…¶ä»–æ–¹æ³•ä¸è®Šï¼Œä½†ä½¿ç”¨æ›´æ–°çš„æ—¥æœŸé‚è¼¯
    def _get_quality_status_by_score_enhanced(self, score: float, has_date: bool = True) -> str:
        """å¢å¼·çš„å“è³ªç‹€æ…‹æŒ‡æ¨™ - è€ƒæ…®æ—¥æœŸå¯ç”¨æ€§"""
        if not has_date:
            return "ğŸ”´ ç¼ºå°‘æ—¥æœŸ"  # Special status for missing date
        elif score >= 9:
            return "ğŸŸ¢ å„ªç§€"
        elif score >= 7:
            return "ğŸŸ¡ è‰¯å¥½"
        elif score >= 5:
            return "ğŸŸ  æ™®é€š"
        else:
            return "ğŸ”´ ä¸è¶³"

    # ä¿ç•™æ‰€æœ‰å…¶ä»–ç¾æœ‰æ–¹æ³• (generate_keyword_summary, generate_watchlist_summary, ç­‰ç­‰)
    # ... (å…¶ä»–æ–¹æ³•ä¿æŒä¸è®Š)

    # è¼”åŠ©æ–¹æ³•
    def _is_watchlist_company(self, company_code: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºè§€å¯Ÿåå–®å…¬å¸ï¼ˆç°¡åŒ–æª¢æŸ¥ï¼‰"""
        if company_code and company_code.isdigit() and len(company_code) == 4:
            return True
        return False

    def _clean_stock_code_for_display(self, code):
        """æ¸…ç†è‚¡ç¥¨ä»£è™Ÿï¼Œç¢ºä¿é¡¯ç¤ºç‚ºç´”æ•¸å­—ï¼ˆç„¡å¼•è™Ÿï¼‰"""
        if pd.isna(code) or code is None or code == '':
            return ''
        
        code_str = str(code).strip()
        
        if code_str.startswith("'"):
            code_str = code_str[1:]
        
        if code_str.isdigit():
            return code_str
        
        return code_str

    def _format_md_file_url_with_warning(self, company_data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ– MD æª”æ¡ˆé€£çµ"""
        filename = company_data.get('filename', '')
        
        if not filename:
            return ""
        
        if not filename.endswith('.md'):
            filename += '.md'
        
        encoded_filename = urllib.parse.quote(filename, safe='')
        raw_url = f"{self.github_repo_base}/data/md/{encoded_filename}"
        
        return raw_url

    def _format_eps_value(self, eps_value) -> str:
        """æ ¼å¼åŒ– EPS æ•¸å€¼"""
        if eps_value is None or eps_value == '':
            return ''
        try:
            eps = float(eps_value)
            return f"{eps:.2f}"
        except (ValueError, TypeError):
            return str(eps_value)

    def _generate_validation_status_marker_v351(self, company_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆé©—è­‰ç‹€æ…‹æ¨™è¨˜"""
        validation_result = company_data.get('validation_result', {})
        validation_status = validation_result.get('overall_status', 'unknown')
        validation_method = validation_result.get('validation_method', 'unknown')
        validation_passed = company_data.get('content_validation_passed', True)
        validation_errors = company_data.get('validation_errors', [])
        validation_warnings = company_data.get('validation_warnings', [])
        validation_enabled = company_data.get('validation_enabled', False)
        
        # å¤šå±¤é©—è­‰ç‹€æ…‹åˆ¤æ–·
        if validation_status == 'error' or not validation_passed:
            if validation_errors:
                main_error = str(validation_errors[0])
                if "ä¸åœ¨è§€å¯Ÿåå–®" in main_error:
                    return "ğŸš« ä¸åœ¨è§€å¯Ÿåå–®"
                elif "å…¬å¸åç¨±ä¸ç¬¦è§€å¯Ÿåå–®" in main_error:
                    return "ğŸ“ åç¨±ä¸ç¬¦"
                elif "æ„›æ´¾çˆ¾" in main_error or "æ„›ç«‹ä¿¡" in main_error:
                    return "ğŸ“„ åç¨±æ··äº‚"
                else:
                    return "âŒ é©—è­‰å¤±æ•—"
            else:
                return "âŒ é©—è­‰å¤±æ•—"
        
        elif validation_method == 'disabled' or not validation_enabled:
            return "âš ï¸ é©—è­‰åœç”¨"
        
        elif validation_warnings:
            return "âš ï¸ æœ‰è­¦å‘Š"
        
        else:
            return "âœ… é€šé"

    # ç‚ºäº†å®Œæ•´æ€§ï¼Œé€™è£¡éœ€è¦åŒ…å«å…¶ä»–å¿…è¦çš„æ–¹æ³•
    # å¯¦éš›å¯¦ä½œä¸­æ‡‰è©²åŒ…å«æ‰€æœ‰åŸæœ‰çš„æ–¹æ³•ï¼Œé€™è£¡åªå±•ç¤ºé—œéµä¿®æ”¹éƒ¨åˆ†
    
    def generate_keyword_summary(self, keyword_analysis: Dict[str, Any]) -> pd.DataFrame:
        """æ”¯æ´æŸ¥è©¢æ¨¡å¼åˆ†æçš„é—œéµå­—çµ±è¨ˆå ±å‘Šç”Ÿæˆï¼ˆä¿æŒä¸è®Šï¼‰"""
        # ä¿æŒåŸæœ‰å¯¦ä½œä¸è®Š
        pass

    def generate_watchlist_summary(self, watchlist_analysis: Dict[str, Any]) -> pd.DataFrame:
        """ç”Ÿæˆè§€å¯Ÿåå–®çµ±è¨ˆå ±å‘Šï¼ˆä¿æŒä¸è®Šï¼‰"""
        # ä¿æŒåŸæœ‰å¯¦ä½œä¸è®Š
        pass

    def save_all_reports(self, portfolio_df: pd.DataFrame, detailed_df: pd.DataFrame, 
                        keyword_df: pd.DataFrame = None, watchlist_df: pd.DataFrame = None) -> Dict[str, str]:
        """å„²å­˜æ‰€æœ‰å ±å‘Šç‚º CSVï¼ˆä¿æŒä¸è®Šï¼‰"""
        # ä¿æŒåŸæœ‰å¯¦ä½œä¸è®Š
        pass


# æ¸¬è©¦åŠŸèƒ½
if __name__ == "__main__":
    generator = ReportGenerator()
    
    print("=== ReportGenerator v3.6.1-updated æ¸¬è©¦ (md_date å„ªå…ˆé‚è¼¯) ===")
    
    # æ¸¬è©¦æ¨¡æ“¬è³‡æ–™
    test_companies = [
        {
            'company_code': '2330',
            'company_name': 'å°ç©é›»',
            'content_date': '2025/09/10',  # Process Group æå–çš„æ—¥æœŸ
            'yaml_data': {
                'md_date': '2025/07/31',  # Search Group æä¾›çš„æ—¥æœŸ
                'extracted_date': '2025-09-10T11:53:26.757739'
            },
            'quality_score': 9,
            'analyst_count': 42,
            'validation_result': {'overall_status': 'valid'},
            'content_validation_passed': True,
            'validation_errors': [],
            'filename': '2330_å°ç©é›»_factset_abc123.md'
        },
        {
            'company_code': '2317',
            'company_name': 'é´»æµ·',
            'content_date': '',  # Process Group æœªèƒ½æå–æ—¥æœŸ
            'yaml_data': {
                'md_date': '2025/05/20',  # Search Group æä¾›äº†æ—¥æœŸ
                'extracted_date': '2025-09-10T11:53:24.593213'
            },
            'quality_score': 8,
            'analyst_count': 22,
            'validation_result': {'overall_status': 'valid'},
            'content_validation_passed': True,
            'validation_errors': [],
            'filename': '2317_é´»æµ·_factset_def456.md'
        }
    ]
    
    print("æ¸¬è©¦ 1: md_date å„ªå…ˆé‚è¼¯")
    for company in test_companies:
        md_date = generator._get_md_date_with_priority(company)
        date_source = generator._get_md_date_source(company)
        
        print(f"   {company['company_code']}: MDæ—¥æœŸ='{md_date}', ä¾†æº={date_source}")
    
    print("æ¸¬è©¦ 2: æ—¥æœŸç¯„åœè¨ˆç®—")
    oldest, newest = generator._calculate_date_range_with_priority(test_companies)
    print(f"   æ—¥æœŸç¯„åœ: {oldest} åˆ° {newest}")
    
    print("æ¸¬è©¦ 3: çµ±è¨ˆå ±å‘Š")
    stats = generator.generate_statistics_report(test_companies)
    md_stats = stats.get('md_date_source_analysis', {})
    
    print(f"   Search Group è¦†è“‹ç‡: {md_stats.get('search_group_coverage_rate', 0)}%")
    print(f"   ç¸½æ—¥æœŸè¦†è“‹ç‡: {md_stats.get('total_date_coverage_rate', 0)}%")
    
    print(f"\nğŸ‰ ReportGenerator v3.6.1-updated æ¸¬è©¦å®Œæˆ!")
    print(f"âœ… md_date å„ªå…ˆé‚è¼¯å·²å¯¦ä½œ")
    print(f"âœ… Search Group æ—¥æœŸå„ªå…ˆæ–¼ Process Group æå–")
    print(f"âœ… å®Œæ•´çµ±è¨ˆ md_date ä¾†æºåˆ†å¸ƒ")