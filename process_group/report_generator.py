#!/usr/bin/env python3
"""
Report Generator - FactSet Pipeline v3.5.0 (Enhanced with Validation Filtering)
éæ¿¾é©—è­‰å¤±æ•—çš„è³‡æ–™ï¼Œæ¨™è¨˜éœ€è¦äººå·¥ç¢ºèªçš„é …ç›®
"""

import os
import re
import pandas as pd
import urllib.parse
from datetime import datetime
from typing import Dict, Any, List, Optional

class ReportGenerator:
    """å ±å‘Šç”Ÿæˆå™¨ - é©—è­‰éæ¿¾å¢å¼·ç‰ˆ"""
    
    def __init__(self, github_repo_base="https://raw.githubusercontent.com/wenchiehlee/GoogleSearch/refs/heads/main"):
        self.github_repo_base = github_repo_base
        self.output_dir = "data/reports"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # æŠ•è³‡çµ„åˆæ‘˜è¦æ¬„ä½ (14 æ¬„ä½)
        self.portfolio_summary_columns = [
            'ä»£è™Ÿ', 'åç¨±', 'è‚¡ç¥¨ä»£è™Ÿ', 'MDæœ€èˆŠæ—¥æœŸ', 'MDæœ€æ–°æ—¥æœŸ', 'MDè³‡æ–™ç­†æ•¸',
            'åˆ†æå¸«æ•¸é‡', 'ç›®æ¨™åƒ¹', '2025EPSå¹³å‡å€¼', '2026EPSå¹³å‡å€¼', '2027EPSå¹³å‡å€¼',
            'å“è³ªè©•åˆ†', 'ç‹€æ…‹', 'æ›´æ–°æ—¥æœŸ'
        ]
        
        # ğŸ†• æ“´å±•è©³ç´°å ±å‘Šæ¬„ä½ (22 æ¬„ä½ - æ–°å¢é©—è­‰ç‹€æ…‹)
        self.detailed_report_columns = [
            'ä»£è™Ÿ', 'åç¨±', 'è‚¡ç¥¨ä»£è™Ÿ', 'MDæ—¥æœŸ', 'åˆ†æå¸«æ•¸é‡', 'ç›®æ¨™åƒ¹',
            '2025EPSæœ€é«˜å€¼', '2025EPSæœ€ä½å€¼', '2025EPSå¹³å‡å€¼',
            '2026EPSæœ€é«˜å€¼', '2026EPSæœ€ä½å€¼', '2026EPSå¹³å‡å€¼',
            '2027EPSæœ€é«˜å€¼', '2027EPSæœ€ä½å€¼', '2027EPSå¹³å‡å€¼',
            'å“è³ªè©•åˆ†', 'ç‹€æ…‹', 'é©—è­‰ç‹€æ…‹', 'MD File', 'æ›´æ–°æ—¥æœŸ'  # ğŸ†• æ–°å¢é©—è­‰ç‹€æ…‹æ¬„ä½
        ]

    def generate_detailed_report(self, processed_companies: List[Dict], filter_invalid=True) -> pd.DataFrame:
        """ç”Ÿæˆè©³ç´°å ±å‘Š - ğŸ†• é»˜èªéæ¿¾ç„¡æ•ˆè³‡æ–™"""
        try:
            detailed_rows = []
            filtered_count = 0
            
            for company_data in processed_companies:
                # ğŸ†• æª¢æŸ¥æ˜¯å¦æ‡‰è©²éæ¿¾æ­¤è³‡æ–™
                if filter_invalid and not self._should_include_in_report(company_data):
                    filtered_count += 1
                    continue
                
                # åŸæœ‰çš„å ±å‘Šç”Ÿæˆé‚è¼¯
                md_date = self._get_content_date_only(company_data)
                
                # ğŸ†• ç”Ÿæˆé©—è­‰ç‹€æ…‹æ¨™è¨˜
                validation_status = self._generate_validation_status_marker(company_data)
                
                # ğŸ†• è™•ç†æœ‰å•é¡Œçš„ MD æª”æ¡ˆé€£çµ
                md_file_url = self._format_md_file_url_with_warning(company_data)
                
                detailed_row = {
                    'ä»£è™Ÿ': company_data.get('company_code', 'Unknown'),
                    'åç¨±': company_data.get('company_name', 'Unknown'),
                    'è‚¡ç¥¨ä»£è™Ÿ': f"{company_data.get('company_code', 'Unknown')}-TW",
                    'MDæ—¥æœŸ': md_date,
                    'åˆ†æå¸«æ•¸é‡': company_data.get('analyst_count', 0),
                    'ç›®æ¨™åƒ¹': company_data.get('target_price', ''),
                    '2025EPSæœ€é«˜å€¼': self._format_eps_value(company_data.get('eps_2025_high')),
                    '2025EPSæœ€ä½å€¼': self._format_eps_value(company_data.get('eps_2025_low')),
                    '2025EPSå¹³å‡å€¼': self._format_eps_value(company_data.get('eps_2025_avg')),
                    '2026EPSæœ€é«˜å€¼': self._format_eps_value(company_data.get('eps_2026_high')),
                    '2026EPSæœ€ä½å€¼': self._format_eps_value(company_data.get('eps_2026_low')),
                    '2026EPSå¹³å‡å€¼': self._format_eps_value(company_data.get('eps_2026_avg')),
                    '2027EPSæœ€é«˜å€¼': self._format_eps_value(company_data.get('eps_2027_high')),
                    '2027EPSæœ€ä½å€¼': self._format_eps_value(company_data.get('eps_2027_low')),
                    '2027EPSå¹³å‡å€¼': self._format_eps_value(company_data.get('eps_2027_avg')),
                    'å“è³ªè©•åˆ†': company_data.get('quality_score', 0),
                    'ç‹€æ…‹': company_data.get('quality_status', 'ğŸ”´ ä¸è¶³'),
                    'é©—è­‰ç‹€æ…‹': validation_status,  # ğŸ†• æ–°å¢é©—è­‰ç‹€æ…‹æ¬„ä½
                    'MD File': md_file_url,
                    'æ›´æ–°æ—¥æœŸ': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                
                detailed_rows.append(detailed_row)
            
            # å»ºç«‹ DataFrame
            df = pd.DataFrame(detailed_rows, columns=self.detailed_report_columns)
            df = df.sort_values(['ä»£è™Ÿ', 'MDæ—¥æœŸ'], ascending=[True, False])
            
            # ğŸ†• è¨˜éŒ„éæ¿¾çµ±è¨ˆ
            if filter_invalid and filtered_count > 0:
                print(f"ğŸ“Š éæ¿¾äº† {filtered_count} ç­†é©—è­‰å¤±æ•—çš„è³‡æ–™")
            
            return df
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆè©³ç´°å ±å‘Šå¤±æ•—: {e}")
            return pd.DataFrame(columns=self.detailed_report_columns)

    def generate_portfolio_summary(self, processed_companies: List[Dict], filter_invalid=True) -> pd.DataFrame:
        """ç”ŸæˆæŠ•è³‡çµ„åˆæ‘˜è¦ - ğŸ†• å¯é¸æ“‡éæ¿¾ç„¡æ•ˆè³‡æ–™"""
        try:
            # ğŸ†• å…ˆéæ¿¾ç„¡æ•ˆè³‡æ–™
            if filter_invalid:
                valid_companies = [c for c in processed_companies if self._should_include_in_report(c)]
                print(f"ğŸ“Š æŠ•è³‡çµ„åˆæ‘˜è¦ï¼šä¿ç•™ {len(valid_companies)}/{len(processed_companies)} ç­†æœ‰æ•ˆè³‡æ–™")
            else:
                valid_companies = processed_companies
            
            # æŒ‰å…¬å¸åˆ†çµ„ï¼Œå–å¾—æ¯å®¶å…¬å¸çš„æœ€æ–°è³‡æ–™
            company_summary = {}
            
            for company_data in valid_companies:
                company_code = company_data.get('company_code', 'Unknown')
                
                if company_code not in company_summary:
                    company_summary[company_code] = {
                        'files': [],
                        'latest_data': None
                    }
                
                company_summary[company_code]['files'].append(company_data)
                
                # æ›´æ–°æœ€æ–°è³‡æ–™
                if (company_summary[company_code]['latest_data'] is None or
                    self._is_more_recent_content_date_only(company_data, company_summary[company_code]['latest_data'])):
                    company_summary[company_code]['latest_data'] = company_data
            
            # ç”Ÿæˆæ‘˜è¦è³‡æ–™
            summary_rows = []
            
            for company_code, company_info in company_summary.items():
                latest_data = company_info['latest_data']
                all_files = company_info['files']
                
                best_content_date = self._get_content_date_only(latest_data)
                
                # è¨ˆç®—æ—¥æœŸç¯„åœï¼ˆåªè€ƒæ…®æœ‰ content_date çš„æª”æ¡ˆï¼‰
                oldest_date, newest_date = self._calculate_date_range_content_date_only(all_files)
                
                summary_row = {
                    'ä»£è™Ÿ': company_code,
                    'åç¨±': latest_data.get('company_name', 'Unknown'),
                    'è‚¡ç¥¨ä»£è™Ÿ': f"{company_code}-TW",
                    'MDæœ€èˆŠæ—¥æœŸ': oldest_date or best_content_date,
                    'MDæœ€æ–°æ—¥æœŸ': newest_date or best_content_date,
                    'MDè³‡æ–™ç­†æ•¸': len(all_files),
                    'åˆ†æå¸«æ•¸é‡': latest_data.get('analyst_count', 0),
                    'ç›®æ¨™åƒ¹': latest_data.get('target_price', ''),
                    '2025EPSå¹³å‡å€¼': self._format_eps_value(latest_data.get('eps_2025_avg')),
                    '2026EPSå¹³å‡å€¼': self._format_eps_value(latest_data.get('eps_2026_avg')),
                    '2027EPSå¹³å‡å€¼': self._format_eps_value(latest_data.get('eps_2027_avg')),
                    'å“è³ªè©•åˆ†': latest_data.get('quality_score', 0),
                    'ç‹€æ…‹': latest_data.get('quality_status', 'ğŸ”´ ä¸è¶³'),
                    'æ›´æ–°æ—¥æœŸ': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                
                summary_rows.append(summary_row)
            
            # å»ºç«‹ DataFrame
            df = pd.DataFrame(summary_rows, columns=self.portfolio_summary_columns)
            df = df.sort_values('ä»£è™Ÿ')
            
            return df
            
        except Exception as e:
            print(f"âŒ ç”ŸæˆæŠ•è³‡çµ„åˆæ‘˜è¦å¤±æ•—: {e}")
            return pd.DataFrame(columns=self.portfolio_summary_columns)

    def _should_include_in_report(self, company_data: Dict[str, Any]) -> bool:
        """ğŸ†• åˆ¤æ–·æ˜¯å¦æ‡‰è©²å°‡æ­¤è³‡æ–™åŒ…å«åœ¨å ±å‘Šä¸­ - åªæ’é™¤çœŸæ­£åš´é‡çš„å•é¡Œ"""
        
        # ğŸš¨ åªæœ‰çœŸæ­£åš´é‡çš„é©—è­‰éŒ¯èª¤æ‰æ’é™¤
        validation_errors = company_data.get('validation_errors', [])
        
        # ğŸ†• åš´é‡éŒ¯èª¤é¡å‹ï¼š
        # 1. æ„›æ´¾å¸/æ„›ç«‹ä¿¡å•é¡Œ
        # 2. è§€å¯Ÿåå–®ä¸ç¬¦å•é¡Œ
        # ç§»é™¤äº†å° "Oops, something went wrong" çš„æª¢æŸ¥ - é€™å¾ˆå¸¸è¦‹ï¼Œä¸éœ€è¦ç‰¹åˆ¥è™•ç†
        critical_error_keywords = [
            r'æ„›æ´¾å¸.*æ„›ç«‹ä¿¡', 
            r'æ„›ç«‹ä¿¡.*æ„›æ´¾å¸', 
            r'company_mismatch',
            r'å…¬å¸åç¨±ä¸ç¬¦è§€å¯Ÿåå–®',  
            r'è§€å¯Ÿåå–®é¡¯ç¤ºæ‡‰ç‚º'
        ]
        
        has_critical_error = any(
            any(re.search(keyword, str(error), re.IGNORECASE) for keyword in critical_error_keywords)
            for error in validation_errors
        )
        
        if has_critical_error:
            print(f"ğŸš¨ æ’é™¤åš´é‡é©—è­‰éŒ¯èª¤: {company_data.get('company_name', 'Unknown')} - {validation_errors[0][:50]}...")
            return False
        
        # æª¢æŸ¥åŸºæœ¬è³‡æ–™å®Œæ•´æ€§ - æ›´å¯¬é¬†çš„æ¢ä»¶
        company_code = company_data.get('company_code')
        company_name = company_data.get('company_name')
        
        if not company_code or company_code == 'Unknown':
            print(f"ğŸ“ æ’é™¤ç¼ºå°‘å…¬å¸ä»£è™Ÿçš„è³‡æ–™: {company_name}")
            return False
        
        if not company_name or company_name == 'Unknown':
            print(f"ğŸ“ æ’é™¤ç¼ºå°‘å…¬å¸åç¨±çš„è³‡æ–™: {company_code}")
            return False
        
        # ğŸ”§ ç§»é™¤éæ–¼åš´æ ¼çš„æ¢ä»¶ï¼š
        # - ä¸å†å› ç‚ºå“è³ªè©•åˆ†ä½è€Œæ’é™¤
        # - ä¸å†å› ç‚ºå…§å®¹çŸ­è€Œæ’é™¤
        # - ä¸å†å› ç‚ºä¸€èˆ¬é©—è­‰è­¦å‘Šè€Œæ’é™¤
        
        return True

    def _generate_validation_status_marker(self, company_data: Dict[str, Any]) -> str:
        """ğŸ†• ç”Ÿæˆé©—è­‰ç‹€æ…‹æ¨™è¨˜"""
        validation_passed = company_data.get('content_validation_passed', True)
        validation_errors = company_data.get('validation_errors', [])
        validation_warnings = company_data.get('validation_warnings', [])
        
        if not validation_passed and validation_errors:
            # æª¢æŸ¥æ˜¯å¦ç‚ºåš´é‡éŒ¯èª¤
            critical_keywords = ['æ„›æ´¾å¸', 'æ„›ç«‹ä¿¡', 'company_mismatch']
            has_critical = any(
                any(keyword in str(error) for keyword in critical_keywords)
                for error in validation_errors
            )
            
            if has_critical:
                return "ğŸš¨ åš´é‡éŒ¯èª¤"
            else:
                return "âŒ é©—è­‰å¤±æ•—"
        
        elif validation_warnings:
            return "âš ï¸ æœ‰è­¦å‘Š"
        
        else:
            return "âœ… é€šé"

    def _format_md_file_url_with_warning(self, company_data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ– MD æª”æ¡ˆé€£çµ - ç§»é™¤è­¦å‘Šæ¨™è¨˜ï¼Œä¿æŒ URL ä¹¾æ·¨"""
        filename = company_data.get('filename', '')
        
        if not filename:
            return ""
        
        # ç¢ºä¿æª”æ¡ˆåç¨±æœ‰ .md å‰¯æª”å
        if not filename.endswith('.md'):
            filename += '.md'
        
        # URL ç·¨ç¢¼æª”æ¡ˆåç¨±
        encoded_filename = urllib.parse.quote(filename, safe='')
        raw_url = f"{self.github_repo_base}/data/md/{encoded_filename}"
        
        # ğŸ”§ ç§»é™¤è­¦å‘Šæ¨™è¨˜ - ä¿æŒ URL ä¹¾æ·¨
        return raw_url

    # ğŸ†• æ–°å¢ï¼šç”Ÿæˆé©—è­‰å ±å‘Š
    def generate_validation_report(self, processed_companies: List[Dict]) -> pd.DataFrame:
        """ğŸ†• ç”Ÿæˆå°ˆé–€çš„é©—è­‰å ±å‘Š"""
        try:
            validation_rows = []
            
            for company_data in processed_companies:
                validation_result = company_data.get('validation_result', {})
                
                validation_row = {
                    'ä»£è™Ÿ': company_data.get('company_code', 'Unknown'),
                    'åç¨±': company_data.get('company_name', 'Unknown'),
                    'æª”æ¡ˆåç¨±': company_data.get('filename', ''),
                    'é©—è­‰ç‹€æ…‹': validation_result.get('overall_status', 'unknown'),
                    'ä¿¡å¿ƒåˆ†æ•¸': validation_result.get('confidence_score', 0),
                    'éŒ¯èª¤æ•¸é‡': len(company_data.get('validation_errors', [])),
                    'è­¦å‘Šæ•¸é‡': len(company_data.get('validation_warnings', [])),
                    'å“è³ªè©•åˆ†': company_data.get('quality_score', 0),
                    'åŒ…å«åœ¨å ±å‘Š': self._should_include_in_report(company_data),
                    'ä¸»è¦å•é¡Œ': self._get_main_validation_issue(company_data),
                    'æª¢æŸ¥æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                
                validation_rows.append(validation_row)
            
            # å»ºç«‹ DataFrame
            validation_columns = [
                'ä»£è™Ÿ', 'åç¨±', 'æª”æ¡ˆåç¨±', 'é©—è­‰ç‹€æ…‹', 'ä¿¡å¿ƒåˆ†æ•¸', 'éŒ¯èª¤æ•¸é‡', 
                'è­¦å‘Šæ•¸é‡', 'å“è³ªè©•åˆ†', 'åŒ…å«åœ¨å ±å‘Š', 'ä¸»è¦å•é¡Œ', 'æª¢æŸ¥æ™‚é–“'
            ]
            
            df = pd.DataFrame(validation_rows, columns=validation_columns)
            df = df.sort_values(['é©—è­‰ç‹€æ…‹', 'ä¿¡å¿ƒåˆ†æ•¸'], ascending=[True, False])
            
            return df
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆé©—è­‰å ±å‘Šå¤±æ•—: {e}")
            return pd.DataFrame()

    def _get_main_validation_issue(self, company_data: Dict[str, Any]) -> str:
        """ğŸ†• å–å¾—ä¸»è¦é©—è­‰å•é¡Œ"""
        validation_errors = company_data.get('validation_errors', [])
        validation_warnings = company_data.get('validation_warnings', [])
        
        if validation_errors:
            # å–å¾—ç¬¬ä¸€å€‹éŒ¯èª¤ï¼Œæˆªæ–·éé•·çš„è¨Šæ¯
            main_error = str(validation_errors[0])[:80]
            return main_error + "..." if len(str(validation_errors[0])) > 80 else main_error
        
        elif validation_warnings:
            main_warning = str(validation_warnings[0])[:80]
            return main_warning + "..." if len(str(validation_warnings[0])) > 80 else main_warning
        
        else:
            return "ç„¡å•é¡Œ"

    # åŸæœ‰æ–¹æ³•ä¿æŒä¸è®Š
    def _get_content_date_only(self, company_data: Dict[str, Any]) -> str:
        """çµ•å°åªä½¿ç”¨ content_dateï¼Œç„¡ä»»ä½• fallback"""
        content_date = company_data.get('content_date')
        
        if content_date and str(content_date).strip() and str(content_date).strip().lower() != 'none':
            try:
                if isinstance(content_date, str) and '/' in content_date:
                    parts = content_date.split('/')
                    if len(parts) == 3:
                        year, month, day = parts
                        if (year.isdigit() and month.isdigit() and day.isdigit() and
                            1900 <= int(year) <= 2100 and 1 <= int(month) <= 12 and 1 <= int(day) <= 31):
                            return f"{year}-{int(month):02d}-{int(day):02d}"
                
                elif isinstance(content_date, datetime):
                    return content_date.strftime('%Y-%m-%d')
                    
            except Exception as e:
                pass
        
        return ""

    def _calculate_date_range_content_date_only(self, all_files: List[Dict]) -> tuple:
        """è¨ˆç®—æ—¥æœŸç¯„åœ - åªè€ƒæ…®æœ‰ content_date çš„æª”æ¡ˆ"""
        valid_dates = []
        
        for file_data in all_files:
            content_date = self._get_content_date_only(file_data)
            if content_date:
                valid_dates.append(content_date)
        
        if valid_dates:
            valid_dates.sort()
            return valid_dates[0], valid_dates[-1]
        
        return "", ""

    def _is_more_recent_content_date_only(self, data1: Dict, data2: Dict) -> bool:
        """æ¯”è¼ƒå…©ç­†è³‡æ–™çš„æ–°èˆŠ - åªç”¨ content_date"""
        date1 = self._get_content_date_only(data1)
        date2 = self._get_content_date_only(data2)
        
        if not date1 and not date2:
            return True
        
        if date1 and not date2:
            return True
        if date2 and not date1:
            return False
        
        try:
            dt1 = datetime.strptime(date1, '%Y-%m-%d')
            dt2 = datetime.strptime(date2, '%Y-%m-%d')
            return dt1 > dt2
        except:
            return True

    def _format_eps_value(self, eps_value) -> str:
        """æ ¼å¼åŒ– EPS æ•¸å€¼"""
        if eps_value is None or eps_value == '':
            return ''
        try:
            eps = float(eps_value)
            return f"{eps:.2f}"
        except (ValueError, TypeError):
            return str(eps_value)

    def save_reports(self, portfolio_df: pd.DataFrame, detailed_df: pd.DataFrame, validation_df: pd.DataFrame = None) -> Dict[str, str]:
        """å„²å­˜å ±å‘Šç‚º CSV - ğŸ†• å¯é¸åŒ…å«é©—è­‰å ±å‘Š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # æª”æ¡ˆè·¯å¾‘
        portfolio_path = os.path.join(self.output_dir, f'portfolio_summary_{timestamp}.csv')
        detailed_path = os.path.join(self.output_dir, f'detailed_report_{timestamp}.csv')
        latest_portfolio_path = os.path.join(self.output_dir, 'portfolio_summary_latest.csv')
        latest_detailed_path = os.path.join(self.output_dir, 'detailed_report_latest.csv')
        
        try:
            # ç¢ºä¿ç©ºå­—ç¬¦ä¸²åœ¨ CSV ä¸­æ­£ç¢ºè™•ç†
            portfolio_df_clean = portfolio_df.copy()
            detailed_df_clean = detailed_df.copy()
            
            portfolio_df_clean = portfolio_df_clean.replace('', pd.NA)
            detailed_df_clean = detailed_df_clean.replace('', pd.NA)
            
            # å„²å­˜ä¸»è¦å ±å‘Š
            portfolio_df_clean.to_csv(portfolio_path, index=False, encoding='utf-8-sig')
            detailed_df_clean.to_csv(detailed_path, index=False, encoding='utf-8-sig')
            portfolio_df_clean.to_csv(latest_portfolio_path, index=False, encoding='utf-8-sig')
            detailed_df_clean.to_csv(latest_detailed_path, index=False, encoding='utf-8-sig')
            
            saved_files = {
                'portfolio_summary': portfolio_path,
                'detailed_report': detailed_path,
                'portfolio_summary_latest': latest_portfolio_path,
                'detailed_report_latest': latest_detailed_path
            }
            
            # ğŸ†• å„²å­˜é©—è­‰å ±å‘Š
            if validation_df is not None and not validation_df.empty:
                validation_path = os.path.join(self.output_dir, f'validation_report_{timestamp}.csv')
                latest_validation_path = os.path.join(self.output_dir, 'validation_report_latest.csv')
                
                validation_df_clean = validation_df.replace('', pd.NA)
                validation_df_clean.to_csv(validation_path, index=False, encoding='utf-8-sig')
                validation_df_clean.to_csv(latest_validation_path, index=False, encoding='utf-8-sig')
                
                saved_files['validation_report'] = validation_path
                saved_files['validation_report_latest'] = latest_validation_path
            
            print(f"ğŸ“ å ±å‘Šå·²å„²å­˜:")
            print(f"   æŠ•è³‡çµ„åˆæ‘˜è¦: {portfolio_path}")
            print(f"   è©³ç´°å ±å‘Š: {detailed_path}")
            if validation_df is not None:
                print(f"   é©—è­‰å ±å‘Š: {saved_files.get('validation_report', 'N/A')}")
            
            # æª¢æŸ¥ç©ºæ—¥æœŸè™•ç†
            empty_dates = detailed_df_clean[detailed_df_clean['MDæ—¥æœŸ'].isna()]
            if len(empty_dates) > 0:
                print(f"ğŸ“Š æª¢æ¸¬åˆ° {len(empty_dates)} å€‹ç©ºæ—¥æœŸæ¢ç›®")
            
            return saved_files
            
        except Exception as e:
            print(f"âŒ å„²å­˜å ±å‘Šå¤±æ•—: {e}")
            return {}

    def generate_statistics_report(self, processed_companies: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆçµ±è¨ˆå ±å‘Š - ğŸ†• åŒ…å«é©—è­‰çµ±è¨ˆ"""
        total_companies = len(processed_companies)
        
        if total_companies == 0:
            return {
                'total_companies': 0,
                'companies_with_data': 0,
                'success_rate': 0
            }
        
        # åŸºæœ¬çµ±è¨ˆ
        companies_with_data = len([c for c in processed_companies if c.get('quality_score', 0) > 0])
        success_rate = (companies_with_data / total_companies) * 100
        
        # å“è³ªåˆ†æ
        quality_scores = [c.get('quality_score', 0) for c in processed_companies]
        
        # content_date æå–åˆ†æ
        companies_with_content_date = len([c for c in processed_companies if self._get_content_date_only(c)])
        content_date_success_rate = (companies_with_content_date / total_companies) * 100
        
        # ğŸ†• é©—è­‰çµ±è¨ˆ
        validation_passed = len([c for c in processed_companies if c.get('content_validation_passed', True)])
        validation_failed = total_companies - validation_passed
        validation_success_rate = (validation_passed / total_companies) * 100
        
        # ğŸ†• éæ¿¾çµ±è¨ˆ
        companies_included_in_report = len([c for c in processed_companies if self._should_include_in_report(c)])
        inclusion_rate = (companies_included_in_report / total_companies) * 100
        
        # ğŸ†• é—œéµå•é¡Œçµ±è¨ˆ
        critical_issues = 0
        for company in processed_companies:
            validation_errors = company.get('validation_errors', [])
            if any('æ„›æ´¾å¸' in str(error) and 'æ„›ç«‹ä¿¡' in str(error) for error in validation_errors):
                critical_issues += 1
        
        statistics = {
            'version': '3.5.0_validation_enhanced',
            'report_type': 'content_validation_filtered',
            'timestamp': datetime.now().isoformat(),
            
            # åŸºæœ¬çµ±è¨ˆ
            'total_companies': total_companies,
            'companies_with_data': companies_with_data,
            'success_rate': round(success_rate, 1),
            'companies_with_content_date': companies_with_content_date,
            'content_date_success_rate': round(content_date_success_rate, 1),
            
            # ğŸ†• é©—è­‰çµ±è¨ˆ
            'validation_statistics': {
                'validation_passed': validation_passed,
                'validation_failed': validation_failed,
                'validation_success_rate': round(validation_success_rate, 1),
                'critical_issues': critical_issues,
                'companies_included_in_report': companies_included_in_report,
                'inclusion_rate': round(inclusion_rate, 1),
                'filtered_out': total_companies - companies_included_in_report
            },
            
            # å“è³ªåˆ†æ
            'quality_analysis': {
                'average_quality_score': round(sum(quality_scores) / len(quality_scores), 1) if quality_scores else 0,
                'highest_quality_score': max(quality_scores) if quality_scores else 0,
                'lowest_quality_score': min(quality_scores) if quality_scores else 0
            }
        }
        
        return statistics

    def save_statistics_report(self, statistics: Dict[str, Any]) -> str:
        """å„²å­˜çµ±è¨ˆå ±å‘Šç‚º JSON æª”æ¡ˆ"""
        import json
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        stats_path = os.path.join(self.output_dir, f'statistics_{timestamp}.json')
        latest_stats_path = os.path.join(self.output_dir, 'statistics_latest.json')
        
        try:
            with open(stats_path, 'w', encoding='utf-8') as f:
                json.dump(statistics, f, ensure_ascii=False, indent=2, default=str)
            
            with open(latest_stats_path, 'w', encoding='utf-8') as f:
                json.dump(statistics, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"ğŸ“Š çµ±è¨ˆå ±å‘Šå·²å„²å­˜: {stats_path}")
            
            # é¡¯ç¤ºé‡è¦çµ±è¨ˆ
            validation_stats = statistics.get('validation_statistics', {})
            print(f"ğŸ“Š é©—è­‰æˆåŠŸç‡: {validation_stats.get('validation_success_rate', 0)}%")
            print(f"ğŸ“Š å ±å‘ŠåŒ…å«ç‡: {validation_stats.get('inclusion_rate', 0)}%")
            if validation_stats.get('critical_issues', 0) > 0:
                print(f"ğŸš¨ é—œéµå•é¡Œ: {validation_stats['critical_issues']} å€‹")
            
            return stats_path
            
        except Exception as e:
            print(f"âŒ å„²å­˜çµ±è¨ˆå ±å‘Šå¤±æ•—: {e}")
            return ""


# æ¸¬è©¦åŠŸèƒ½
if __name__ == "__main__":
    generator = ReportGenerator()
    
    print("=== ğŸ”’ é©—è­‰éæ¿¾å¢å¼·ç‰ˆå ±å‘Šç”Ÿæˆå™¨æ¸¬è©¦ ===")
    
    # æ¸¬è©¦æ•¸æ“š - åŒ…å«æ­£å¸¸å’Œæœ‰å•é¡Œçš„è³‡æ–™
    test_companies = [
        # æ­£å¸¸è³‡æ–™
        {
            'company_name': 'å°ç©é›»',
            'company_code': '2330',
            'filename': '2330_å°ç©é›»_factset_abc123.md',
            'content_date': '2025/6/24',
            'analyst_count': 42,
            'target_price': 650.5,
            'eps_2025_avg': 46.00,
            'quality_score': 10.0,
            'quality_status': 'ğŸŸ¢ å®Œæ•´',
            'content_validation_passed': True,
            'validation_errors': [],
            'validation_warnings': [],
            'content_length': 5000
        },
        
        # æ„›æ´¾å¸/æ„›ç«‹ä¿¡éŒ¯èª¤è³‡æ–™
        {
            'company_name': 'æ„›æ´¾å¸',
            'company_code': '6918',
            'filename': '6918_æ„›æ´¾å¸_yahoo_def456.md',
            'content_date': '2025/6/19',
            'analyst_count': 18,
            'target_price': 8.6,
            'eps_2025_avg': 6.00,
            'quality_score': 2.0,  # ä½åˆ†
            'quality_status': 'ğŸ”´ ä¸è¶³',
            'content_validation_passed': False,
            'validation_errors': ['æª”æ¡ˆæ¨™ç¤ºç‚ºæ„›æ´¾å¸(6918)ä½†å…§å®¹åŒ…å«æ„›ç«‹ä¿¡ç›¸é—œè³‡è¨Š'],
            'validation_warnings': [],
            'validation_result': {
                'overall_status': 'error',
                'confidence_score': 0.0
            },
            'content_length': 2000
        },
        
        # ä¸€èˆ¬å“è³ªè¼ƒä½è³‡æ–™
        {
            'company_name': 'æ¸¬è©¦å…¬å¸',
            'company_code': '1234',
            'filename': '1234_æ¸¬è©¦å…¬å¸_yahoo_ghi789.md',
            'content_date': None,  # æ²’æœ‰å…§å®¹æ—¥æœŸ
            'analyst_count': 0,
            'target_price': None,
            'eps_2025_avg': None,
            'quality_score': 3.5,
            'quality_status': 'ğŸŸ  éƒ¨åˆ†',
            'content_validation_passed': True,
            'validation_errors': [],
            'validation_warnings': ['å…§å®¹éçŸ­'],
            'content_length': 800
        }
    ]
    
    print("æ¸¬è©¦ 1: ç”Ÿæˆè©³ç´°å ±å‘Š (éæ¿¾ç„¡æ•ˆè³‡æ–™)")
    detailed_df = generator.generate_detailed_report(test_companies, filter_invalid=True)
    print(f"   åŸå§‹è³‡æ–™: {len(test_companies)} ç­†")
    print(f"   å ±å‘ŠåŒ…å«: {len(detailed_df)} ç­†")
    print(f"   éæ¿¾çµæœ: {'âœ… æ­£ç¢ºéæ¿¾æ„›æ´¾å¸éŒ¯èª¤' if len(detailed_df) == 2 else 'âŒ éæ¿¾ç•°å¸¸'}")
    
    print("\næ¸¬è©¦ 2: ç”ŸæˆæŠ•è³‡çµ„åˆæ‘˜è¦")
    portfolio_df = generator.generate_portfolio_summary(test_companies, filter_invalid=True)
    print(f"   æ‘˜è¦åŒ…å«: {len(portfolio_df)} å®¶å…¬å¸")
    
    print("\næ¸¬è©¦ 3: ç”Ÿæˆé©—è­‰å ±å‘Š")
    validation_df = generator.generate_validation_report(test_companies)
    print(f"   é©—è­‰å ±å‘Š: {len(validation_df)} ç­†è¨˜éŒ„")
    
    # æª¢æŸ¥é©—è­‰ç‹€æ…‹æ¬„ä½
    if not detailed_df.empty:
        print(f"\né©—è­‰ç‹€æ…‹æ¬„ä½æ¸¬è©¦:")
        for idx, row in detailed_df.iterrows():
            company = row['åç¨±']
            validation_status = row['é©—è­‰ç‹€æ…‹']
            print(f"   {company}: {validation_status}")
    
    print(f"\nâœ… é æœŸçµæœ:")
    print(f"   å°ç©é›»: âœ… é€šé")
    print(f"   æ¸¬è©¦å…¬å¸: âš ï¸ æœ‰è­¦å‘Š")
    print(f"   æ„›æ´¾å¸: æ‡‰è¢«éæ¿¾ (ä¸å‡ºç¾åœ¨å ±å‘Šä¸­)")
    
    print(f"\nğŸ‰ æ¸¬è©¦å®Œæˆ!")