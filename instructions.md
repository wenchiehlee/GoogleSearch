# Google Search FactSet Pipeline æŒ‡å—

## Version
{Guideline version}=3.3.0

# Google Search FactSet Pipeline å®Œæ•´æŒ‡å—

## ğŸ¯ ç³»çµ±æ¦‚è¿°

**ç›®æ¨™**: è‡ªå‹•åŒ–æ”¶é›†å°ç£è‚¡å¸‚å…¬å¸çš„ FactSet è²¡å‹™æ•¸æ“šï¼Œç”ŸæˆæŠ•è³‡çµ„åˆåˆ†æå ±å‘Š

**è¼¸å…¥**: `è§€å¯Ÿåå–®.csv` (ä»£è™Ÿ,åç¨± æ ¼å¼ï¼Œ116+ å®¶å…¬å¸)
**è¼¸å‡º**: Google Sheets æŠ•è³‡çµ„åˆå„€è¡¨æ¿ + çµæ§‹åŒ–æ•¸æ“šæª”æ¡ˆ

## ğŸ“Š æ ¸å¿ƒæ¶æ§‹

```
GitHub Actions â†’ ç›®æ¨™å…¬å¸è¼‰å…¥ â†’ æœå°‹å¼•æ“ â†’ è³‡æ–™è™•ç† â†’ Google Sheets
      â†“              â†“           â†“         â†“           â†“
   å®šæ™‚åŸ·è¡Œ     è§€å¯Ÿåå–®.csv   factsetæœå°‹   MDâ†’CSV    æŠ•è³‡çµ„åˆå ±å‘Š
```

## ğŸ—ï¸ æ¨¡çµ„è¨­è¨ˆè¦ç¯„

### 1. ä¸»æ§åˆ¶å™¨ (`factset_pipeline.py`)

ç‰ˆæœ¬={Guideline version}
```python
# æ ¸å¿ƒè·è²¬
class ProductionFactSetPipeline:
    def __init__(self):
        self.config = ProductionConfig()
        self.rate_protector = RateLimitProtector()
        self.state = WorkflowState()
    
    # å¿…è¦æ–¹æ³•
    def run_complete_pipeline(self, strategy="intelligent"):
        # 1. åˆ†æç¾æœ‰è³‡æ–™
        # 2. æ±ºå®šåŸ·è¡Œç­–ç•¥
        # 3. åŸ·è¡Œæœå°‹éšæ®µ (with 429 protection)
        # 4. åŸ·è¡Œè³‡æ–™è™•ç†
        # 5. ä¸Šå‚³åˆ° Google Sheets
    
    def analyze_existing_data(self):
        # åˆ†æç¾æœ‰ MD æª”æ¡ˆå“è³ªå’Œæ•¸é‡
        return file_count, quality_status
    
    def determine_strategy(self, existing_data):
        # conservative: é™åˆ¶æœå°‹
        # process_existing: åªè™•ç†ç¾æœ‰è³‡æ–™  
        # comprehensive: å®Œæ•´æœå°‹
        return strategy
```

**é—œéµç‰¹æ€§**:
- âœ… ç«‹å³åµæ¸¬ 429 éŒ¯èª¤ä¸¦åœæ­¢æœå°‹
- âœ… å›é€€åˆ°ç¾æœ‰è³‡æ–™è™•ç†
- âœ… ç‹€æ…‹ç®¡ç†å’Œæ¢å¾©æ©Ÿåˆ¶
- âœ… å¤šç¨®åŸ·è¡Œç­–ç•¥

### 2. æœå°‹å¼•æ“ (`factset_search.py`)

ç‰ˆæœ¬={Guideline version}

```python
# æ ¸å¿ƒè·è²¬
def search_company_factset_data(company_name, stock_code, search_type="factset"):
    # æœå°‹ç­–ç•¥
    patterns = {
        'factset': ['{company} factset EPS é ä¼°', '{company} ç›®æ¨™åƒ¹'],
        'financial': ['{company} è²¡å ± åˆ†æå¸« é ä¼°'],
        'missing': ['{company} è‚¡åƒ¹ åˆ†æ']
    }
    
    # 429 ä¿è­·æ©Ÿåˆ¶
    try:
        results = google_search(query)
        if "429" in response or "rate limit" in response:
            raise RateLimitException("åœæ­¢æ‰€æœ‰æœå°‹")
    except RateLimitException:
        return []  # ç«‹å³åœæ­¢
    
    # URL æ¸…ç†å’Œé©—è­‰
    for url in results:
        clean_url = clean_and_validate_url(url)
        content = download_webpage_content(clean_url)
        save_as_markdown(content, unique_filename)

# å¿…è¦åŠŸèƒ½
def clean_and_validate_url(url):
    # è™•ç† Google redirect URLs
    # é©—è­‰ URL æ ¼å¼
    # å›å‚³æ¸…ç†å¾Œçš„ URL

def generate_unique_filename(company, stock_code, url, index):
    # æ ¼å¼: {company}_{stock}_{domain}_{hash}.md
    # hash is based on file content. é¿å…æª”æ¡ˆè¦†è“‹

def immediate_stop_on_429():
    # åµæ¸¬åˆ° 429 å¾Œç«‹å³åœæ­¢æ‰€æœ‰æœå°‹
    # å›å‚³ç¾æœ‰æ”¶é›†çš„è³‡æ–™
```

**é—œéµç‰¹æ€§**:
- ğŸš¨ **ç«‹å³åœæ­¢ç­–ç•¥**: åµæ¸¬åˆ° 429 å¾Œç«‹å³åœæ­¢
- ğŸ”„ **URL æ¸…ç†**: è™•ç† Google redirect å’Œæƒ¡æ„ URL
- ğŸ’¾ **é˜²è¦†è“‹**: å”¯ä¸€æª”åç”Ÿæˆ
- ğŸ“Š **å¤šæœå°‹ç­–ç•¥**: factset, financial, missing

### 3. è³‡æ–™è™•ç†å™¨ (`data_processor.py`)

ç‰ˆæœ¬={Guideline version}

```python
# æ ¸å¿ƒè·è²¬
def process_all_data(parse_md=True):
    # 1. æ•´åˆ CSV æª”æ¡ˆ
    consolidated_df = consolidate_csv_files()
    
    # 2. è§£æ MD æª”æ¡ˆè²¡å‹™æ•¸æ“š
    if parse_md:
        md_data = parse_markdown_files()
        consolidated_df = apply_md_data_to_csv(consolidated_df, md_data)
    
    # 3. ä¿®å¾©å…¬å¸åç¨±å°æ‡‰
    consolidated_df = fix_company_mapping(consolidated_df)
    
    # 4. ç”ŸæˆæŠ•è³‡çµ„åˆæ‘˜è¦
    summary_df = generate_portfolio_summary(consolidated_df)
    
    # 5. ç”¢ç”Ÿçµ±è¨ˆå ±å‘Š
    stats = generate_statistics(consolidated_df, summary_df)

# è²¡å‹™æ•¸æ“šæå–æ¨¡å¼
FACTSET_PATTERNS = {
    'eps_current': [r'EPS[ï¼š:\s]*([0-9]+\.?[0-9]*)'],
    'target_price': [r'ç›®æ¨™åƒ¹[ï¼š:\s]*([0-9]+\.?[0-9]*)'],
    'analyst_count': [r'åˆ†æå¸«[ï¼š:\s]*([0-9]+)']
}

def extract_company_info_from_title(title, watchlist_df):
    # å¾æœå°‹çµæœæ¨™é¡Œæå–å…¬å¸è³‡è¨Š
    # ä½¿ç”¨è§€å¯Ÿåå–®é€²è¡Œå°æ‡‰
    return company_name, stock_code
```

**é—œéµç‰¹æ€§**:
- ğŸ“„ **MD æª”æ¡ˆè§£æ**: æå– EPSã€ç›®æ¨™åƒ¹ã€åˆ†æå¸«æ•¸é‡
- ğŸ¢ **å…¬å¸åç¨±ä¿®å¾©**: ä½¿ç”¨è§€å¯Ÿåå–®é€²è¡Œå°æ‡‰
- ğŸ“Š **è³‡æ–™å“è³ªé©—è­‰**: æª¢æŸ¥å®Œæ•´æ€§å’Œæº–ç¢ºæ€§
- ğŸ“ˆ **çµ±è¨ˆç”Ÿæˆ**: æˆåŠŸç‡ã€è¦†è“‹ç‡åˆ†æ

### 4. é…ç½®ç®¡ç† (`config.py`)

```python
# è§€å¯Ÿåå–®è¼‰å…¥
def download_target_companies():
    """å¾ GitHub è¼‰å…¥è§€å¯Ÿåå–®.csv"""
    url = "https://raw.githubusercontent.com/wenchiehlee/GoPublic/refs/heads/main/è§€å¯Ÿåå–®.csv"
    response = requests.get(url)
    
    # è§£æ CSV: ä»£è™Ÿ,åç¨±
    companies = []
    for row in csv.reader(io.StringIO(response.text)):
        code, name = row[0].strip(), row[1].strip()
        if code.isdigit() and len(code) == 4:
            companies.append({"code": code, "name": name})
    
    return companies

# é…ç½®çµæ§‹
DEFAULT_CONFIG = {
    "target_companies": download_target_companies(),
    "search": {
        "max_results": 10,
        "rate_limit_delay": 45,
        "circuit_breaker_threshold": 1  # ç«‹å³åœæ­¢
    },
    "output": {
        "csv_dir": "data/csv",
        "md_dir": "data/md", 
        "processed_dir": "data/processed"
    }
}
```

### 5. Google Sheets æ•´åˆ (`sheets_uploader.py`)

```python
def upload_all_sheets(config):
    # 1. è¼‰å…¥è™•ç†å¾Œçš„è³‡æ–™
    data = {
        'summary': pd.read_csv('data/processed/portfolio_summary.csv'),
        'consolidated': pd.read_csv('data/processed/consolidated_factset.csv'),
        'statistics': json.load('data/processed/statistics.json')
    }
    
    # 2. æ›´æ–°ä¸‰å€‹å·¥ä½œè¡¨
    update_portfolio_summary_sheet(client, data)      # æŠ•è³‡çµ„åˆæ‘˜è¦
    update_detailed_data_sheet(client, data)          # è©³ç´°è³‡æ–™
    update_statistics_sheet(client, data)             # çµ±è¨ˆå ±å‘Š

# é©—è­‰è³‡æ–™å“è³ª
def validate_data_quality(data):
    issues = []
    if len(data['summary']) == 0:
        issues.append("ç„¡æŠ•è³‡çµ„åˆè³‡æ–™")
    companies_with_eps = len(data['summary'][data['summary']['ç•¶å‰EPSé ä¼°'].notna()])
    if companies_with_eps == 0:
        issues.append("ç„¡ EPS è³‡æ–™")
    return len(issues) == 0
```

## ğŸš¨ é€Ÿç‡é™åˆ¶ä¿è­·æ©Ÿåˆ¶

### ç«‹å³åœæ­¢ç­–ç•¥

```python
class RateLimitProtector:
    def __init__(self):
        self.should_stop_searching = False
        self.circuit_breaker_threshold = 1  # ç¬¬ä¸€æ¬¡ 429 å°±åœæ­¢
    
    def record_429_error(self):
        self.should_stop_searching = True
        logger.error("ğŸ›‘ åµæ¸¬åˆ°é€Ÿç‡é™åˆ¶ - ç«‹å³åœæ­¢æœå°‹")
        logger.info("ğŸ“„ å°‡è™•ç†ç¾æœ‰è³‡æ–™")
        return True
    
    def should_stop_immediately(self):
        return self.should_stop_searching
```

### å›é€€ç­–ç•¥

```python
def fallback_to_existing_data():
    """ç•¶æœå°‹è¢«é™åˆ¶æ™‚ï¼Œè™•ç†ç¾æœ‰è³‡æ–™"""
    existing_files = count_existing_md_files()
    if existing_files > 0:
        logger.info(f"ğŸ“„ ä½¿ç”¨ç¾æœ‰è³‡æ–™: {existing_files} å€‹æª”æ¡ˆ")
        return process_existing_data()
    else:
        logger.warning("âš ï¸ ç„¡ç¾æœ‰è³‡æ–™å¯è™•ç†")
        return False
```

## âš™ï¸ GitHub Actions å·¥ä½œæµç¨‹

### Actions.yml çµæ§‹

```yaml
name: FactSet Pipeline - Rate Limiting Aware

on:
  schedule:
    - cron: "10 2 * * *"  # æ¯æ—¥ 2:10 AM (é€Ÿç‡é™åˆ¶æœ€å¯èƒ½é‡ç½®æ™‚)
  workflow_dispatch:
    inputs:
      execution_mode:
        type: choice
        options: ['conservative', 'process_only', 'test_only', 'force_search']
      priority_focus:
        type: choice  
        options: ['high_only', 'top_30', 'balanced']

jobs:
  # é æª¢éšæ®µ
  preflight:
    runs-on: ubuntu-latest
    outputs:
      rate_limited: ${{ steps.check.outputs.rate_limited }}
      has_existing_data: ${{ steps.check.outputs.has_existing_data }}
    steps:
      - name: æ¸¬è©¦æœå°‹ API
        run: |
          # ç°¡å–®æ¸¬è©¦æŸ¥è©¢ä»¥æª¢æŸ¥ 429 ç‹€æ…‹
          python test_search_api.py
  
  # ä¸»è¦ç®¡ç·š
  pipeline:
    needs: preflight
    steps:
      - name: è¼‰å…¥ç›®æ¨™å…¬å¸
        run: python config.py --download-csv
      
      - name: è™•ç†ç¾æœ‰è³‡æ–™ (å¦‚æœé€Ÿç‡é™åˆ¶)
        if: needs.preflight.outputs.rate_limited == 'true'
        run: python data_processor.py --force --parse-md
      
      - name: ä¿å®ˆæœå°‹ (å¦‚æœå®‰å…¨)
        if: needs.preflight.outputs.rate_limited != 'true'
        run: |
          python factset_search.py --priority-focus high_only
          # ç«‹å³åœæ­¢å¦‚æœé‡åˆ° 429
      
      - name: æœ€çµ‚è³‡æ–™è™•ç†
        run: python data_processor.py --force --parse-md
      
      - name: é©—è­‰ä¸¦æäº¤çµæœ
        run: |
          # é©—è­‰è³‡æ–™å“è³ª
          valid_files=$(check_data_quality.py)
          if [ "$valid_files" -gt "5" ]; then
            git add data/
            git commit -m "ğŸ“Š Pipeline: $valid_files files"
            git push
          fi
      
      - name: ä¸Šå‚³åˆ° Google Sheets
        env:
          GOOGLE_SHEETS_CREDENTIALS: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}
          GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
        run: python sheets_uploader.py
```

### ç’°å¢ƒè®Šæ•¸éœ€æ±‚

```bash
# GitHub Secrets
GOOGLE_SEARCH_API_KEY=your_api_key
GOOGLE_SEARCH_CSE_ID=your_cse_id  
GOOGLE_SHEETS_CREDENTIALS='{"type":"service_account",...}'
GOOGLE_SHEET_ID=your_spreadsheet_id
```

## ğŸ“ æª”æ¡ˆçµæ§‹è¦ç¯„

```
FactSet-Pipeline/
â”œâ”€â”€ factset_pipeline.py        # ä¸»æ§åˆ¶å™¨
â”œâ”€â”€ factset_search.py          # æœå°‹å¼•æ“  
â”œâ”€â”€ data_processor.py          # è³‡æ–™è™•ç†å™¨
â”œâ”€â”€ sheets_uploader.py         # Google Sheets æ•´åˆ
â”œâ”€â”€ config.py                  # é…ç½®ç®¡ç†
â”œâ”€â”€ utils.py                   # å·¥å…·å‡½æ•¸
â”œâ”€â”€ setup_validator.py         # å®‰è£é©—è­‰
â”œâ”€â”€ è§€å¯Ÿåå–®.csv                # ç›®æ¨™å…¬å¸ (ä»£è™Ÿ,åç¨±)
â”œâ”€â”€ .github/workflows/Actions.yml  # GitHub Actions
â”œâ”€â”€ requirements.txt           # Python ä¾è³´
â”œâ”€â”€ .env.example              # ç’°å¢ƒè®Šæ•¸ç¯„æœ¬
â”œâ”€â”€ data/                     # ç”Ÿæˆçš„è³‡æ–™
â”‚   â”œâ”€â”€ csv/                  # æœå°‹çµæœ
â”‚   â”œâ”€â”€ md/                   # ä¸‹è¼‰çš„å…§å®¹  
â”‚   â”œâ”€â”€ pdf/                  # PDF æª”æ¡ˆ
â”‚   â””â”€â”€ processed/            # è™•ç†å¾Œè³‡æ–™
â”‚       â”œâ”€â”€ consolidated_factset.csv
â”‚       â”œâ”€â”€ portfolio_summary.csv
â”‚       â””â”€â”€ statistics.json
â””â”€â”€ logs/                     # æ—¥èªŒæª”æ¡ˆ
```

## ğŸ”„ è³‡æ–™æµç¨‹

### 1. è¼¸å…¥éšæ®µ
```
è§€å¯Ÿåå–®.csv â†’ è¼‰å…¥ 116+ å…¬å¸ â†’ é¸æ“‡å„ªå…ˆç´šç­–ç•¥
```

### 2. æœå°‹éšæ®µ  
```
ç›®æ¨™å…¬å¸ â†’ Google æœå°‹ â†’ 429 æª¢æŸ¥ â†’ MD æª”æ¡ˆå„²å­˜
         â†“ (å¦‚æœ 429)
         ç«‹å³åœæ­¢ â†’ å›é€€åˆ°ç¾æœ‰è³‡æ–™
```

### 3. è™•ç†éšæ®µ
```
MD æª”æ¡ˆ â†’ è²¡å‹™æ•¸æ“šæå– â†’ å…¬å¸åç¨±å°æ‡‰ â†’ CSV æ•´åˆ
```

### 4. è¼¸å‡ºéšæ®µ
```
è™•ç†å¾Œè³‡æ–™ â†’ æŠ•è³‡çµ„åˆæ‘˜è¦ â†’ Google Sheets â†’ å„€è¡¨æ¿
```

## ğŸ“Š è¼¸å‡ºè¦æ ¼

### Portfolio Summary (æŠ•è³‡çµ„åˆæ‘˜è¦)

```csv
ä»£è™Ÿ,åç¨±,è‚¡ç¥¨ä»£è™Ÿ,MDæœ€èˆŠæ—¥æœŸ,MDæœ€æ–°æ—¥æœŸ,MDè³‡æ–™ç­†æ•¸,åˆ†æå¸«æ•¸é‡,ç›®æ¨™åƒ¹,2025EPSå¹³å‡å€¼,2026EPSå¹³å‡å€¼,2027EPSå¹³å‡å€¼,å“è³ªè©•åˆ†,ç‹€æ…‹,æ›´æ–°æ—¥æœŸ
1587,å‰èŒ‚,1587-TW,2025/1/22,2025/6/22,6,23,102.3,20,20,20,4,ğŸŸ¢ å®Œæ•´,2025-06-22 10:45:00
```

* Since there is several MD files with the same "ä»£è™Ÿ" and "åç¨±"(like å‰èŒ‚_1587_xxxxxx.md) so that 
(1) "MDè³‡æ–™ç­†æ•¸" is the numbers of MD files and 
(2) "MDæœ€èˆŠæ—¥æœŸ" are the oldest date of the MD file
(3) "MDæœ€æ–°æ—¥æœŸ"  are the latest date of the MD file
(4) In "MDè³‡æ–™ç­†æ•¸", the md files are parsed and make sure whether it is the same data that from different news source. If the same data, then "åˆ†æå¸«æ•¸é‡","ç›®æ¨™åƒ¹", "2025EPSæœ€é«˜å€¼","2025EPSæœ€ä½å€¼","2025EPSå¹³å‡å€¼","2026EPSæœ€é«˜å€¼","2026EPSæœ€ä½å€¼","2026EPSå¹³å‡å€¼","2027EPSæœ€é«˜å€¼","2027EPSæœ€ä½å€¼","2027EPSå¹³å‡å€¼" will have the same value

### Detailed Data (è©³ç´°è³‡æ–™)
```csv
ä»£è™Ÿ,åç¨±,è‚¡ç¥¨ä»£è™Ÿ,MDæ—¥æœŸ,åˆ†æå¸«æ•¸é‡,ç›®æ¨™åƒ¹,2025EPSæœ€é«˜å€¼,2025EPSæœ€ä½å€¼,2025EPSå¹³å‡å€¼,2026EPSæœ€é«˜å€¼,2026EPSæœ€ä½å€¼,2026EPSå¹³å‡å€¼,2027EPSæœ€é«˜å€¼,2027EPSæœ€ä½å€¼,2027EPSå¹³å‡å€¼,å“è³ªè©•åˆ†,ç‹€æ…‹,MD File,æ›´æ–°æ—¥æœŸ
1587,å‰èŒ‚,1587-TW,2025/1/22,2025/6/22,6,23,102.3,20,18,23,20,18,23,20,18,23,4,ğŸŸ¢ å®Œæ•´,data/md/å‰èŒ‚_1587_xxxxx.md,2025-06-22 10:45:00
```
* Every row is only from one MD file (Filename can be found from "MD File"=data/md/å‰èŒ‚_1587_xxxxx.md so å“è³ªè©•åˆ†,ç‹€æ…‹ is verified on the "MD File" only)


### Statistics (çµ±è¨ˆè³‡æ–™)
```json
{
  "total_companies": 116,
  "companies_with_data": 45,
  "success_rate": 38.8,
  "rate_limited": false,
  "last_updated": "2025-06-22T10:45:00"
}
```

## ğŸ› ï¸ å¯¦ä½œé‡é»

### 1. é€Ÿç‡é™åˆ¶è™•ç†
```python
# åœ¨æ¯å€‹æœå°‹è«‹æ±‚ä¸­
try:
    response = requests.get(search_url)
    if response.status_code == 429:
        raise RateLimitException("ç«‹å³åœæ­¢")
except RateLimitException:
    logger.error("ğŸ›‘ é€Ÿç‡é™åˆ¶ - åœæ­¢æ‰€æœ‰æœå°‹")
    return process_existing_data()
```

### 2. è³‡æ–™å“è³ªé©—è­‰
```python
def validate_data_before_commit():
    md_files = count_valid_md_files()
    factset_files = count_factset_content_files()
    
    if md_files > 20 and factset_files > 5:
        return "high_quality"
    elif md_files > 10:
        return "acceptable"
    else:
        return "insufficient"
```

### 3. éŒ¯èª¤è™•ç†æ¨¡å¼
```python
def safe_operation_with_fallback():
    try:
        return primary_operation()
    except RateLimitException:
        logger.warning("é€Ÿç‡é™åˆ¶ - ä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ")
        return fallback_operation()
    except Exception as e:
        logger.error(f"ä¸€èˆ¬éŒ¯èª¤: {e}")
        return default_result()
```

## ğŸ§ª æ¸¬è©¦å’Œé©—è­‰

### å–®å…ƒæ¸¬è©¦
```python
def test_company_mapping():
    title = "å°ç©é›»(2330) Q4è²¡å ±é ä¼°"
    company, code = extract_company_info(title)
    assert company == "å°ç©é›»"
    assert code == "2330"

def test_rate_limit_protection():
    protector = RateLimitProtector()
    protector.record_429_error()
    assert protector.should_stop_immediately() == True
```

### æ•´åˆæ¸¬è©¦
```bash
# æ¸¬è©¦å®Œæ•´æµç¨‹ (ç„¡æœå°‹)
python factset_pipeline.py --mode process_only

# æ¸¬è©¦å€‹åˆ¥æ¨¡çµ„
python config.py --show
python data_processor.py --check-data
python sheets_uploader.py --test-connection
```

## ğŸš€ éƒ¨ç½²æŒ‡ä»¤

### æœ¬åœ°é–‹ç™¼
```bash
# 1. å®‰è£ä¾è³´
pip install -r requirements.txt

# 2. é…ç½®ç’°å¢ƒè®Šæ•¸
cp .env.example .env
# ç·¨è¼¯ .env æª”æ¡ˆ

# 3. ä¸‹è¼‰ç›®æ¨™å…¬å¸
python config.py --download-csv

# 4. é©—è­‰è¨­å®š
python setup_validator.py

# 5. åŸ·è¡Œç®¡ç·š
python factset_pipeline.py --mode conservative
```

### GitHub Actions éƒ¨ç½²
```bash
# 1. è¨­å®š GitHub Secrets
# 2. æ¨é€ç¨‹å¼ç¢¼åˆ° main åˆ†æ”¯
# 3. æ‰‹å‹•è§¸ç™¼æˆ–ç­‰å¾…å®šæ™‚åŸ·è¡Œ
```

## ğŸ“‹ ç¶­è­·æª¢æŸ¥æ¸…å–®

### æ¯æ—¥æª¢æŸ¥
- [ ] æª¢æŸ¥ GitHub Actions åŸ·è¡Œç‹€æ…‹
- [ ] ç¢ºèª Google Sheets å„€è¡¨æ¿æ›´æ–°
- [ ] ç›£æ§é€Ÿç‡é™åˆ¶è­¦å‘Š

### æ¯é€±æª¢æŸ¥  
- [ ] æª¢æŸ¥è§€å¯Ÿåå–®.csv æ˜¯å¦æ›´æ–°
- [ ] é©—è­‰è³‡æ–™å“è³ªçµ±è¨ˆ
- [ ] æ¸…ç†èˆŠçš„å‚™ä»½æª”æ¡ˆ

### æ¯æœˆæª¢æŸ¥
- [ ] æ›´æ–° Python ä¾è³´
- [ ] æª¢æŸ¥ Google API é…é¡ä½¿ç”¨é‡
- [ ] åˆ†ææŠ•è³‡çµ„åˆè¦†è“‹ç‡

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ
1. **429 é€Ÿç‡é™åˆ¶**: ç­‰å¾… 4-12 å°æ™‚ï¼Œä½¿ç”¨ä¿å®ˆæ¨¡å¼
2. **ç„¡æœå°‹çµæœ**: æª¢æŸ¥ API é‡‘é‘°å’Œ CSE ID
3. **Google Sheets éŒ¯èª¤**: é©—è­‰æœå‹™å¸³æˆ¶æ¬Šé™
4. **è³‡æ–™å“è³ªå·®**: èª¿æ•´æœå°‹æ¨¡å¼ï¼Œå¢åŠ å»¶é²

### æ¢å¾©ç­–ç•¥
```bash
# å¦‚æœæœå°‹å®Œå…¨å¤±æ•—
python data_processor.py --force --parse-md
python sheets_uploader.py

# å¦‚æœéƒ¨åˆ†å¤±æ•—  
python factset_pipeline.py --recover

# é‡ç½®ç‹€æ…‹
python factset_pipeline.py --reset
```

---

**é€™å€‹æŒ‡å—æä¾›äº†é‡å»ºæ•´å€‹ FactSet Pipeline æ‰€éœ€çš„å®Œæ•´æ¶æ§‹ã€å¯¦ä½œç´°ç¯€å’Œæœ€ä½³å¯¦å‹™ã€‚æŒ‰ç…§æ­¤æŒ‡å—å¯ä»¥ç”Ÿæˆå…·å‚™é€Ÿç‡é™åˆ¶ä¿è­·ã€è³‡æ–™å“è³ªé©—è­‰å’Œæ™ºèƒ½å›é€€æ©Ÿåˆ¶çš„å®Œæ•´ç³»çµ±ã€‚**