# Google Search FactSet Pipeline æŒ‡å—

## Version
{Guideline version}=3.3.1

# Google Search FactSet Pipeline å®Œæ•´æŒ‡å— (v3.3.1 ç¶œåˆä¿®å¾©ç‰ˆ)

## ğŸ¯ ç³»çµ±æ¦‚è¿°

**ç›®æ¨™**: è‡ªå‹•åŒ–æ”¶é›†å°ç£è‚¡å¸‚å…¬å¸çš„ FactSet è²¡å‹™æ•¸æ“šï¼Œç”ŸæˆæŠ•è³‡çµ„åˆåˆ†æå ±å‘Š

**è¼¸å…¥**: `è§€å¯Ÿåå–®.csv` (ä»£è™Ÿ,åç¨± æ ¼å¼ï¼Œ116+ å®¶å…¬å¸)
**è¼¸å‡º**: Google Sheets æŠ•è³‡çµ„åˆå„€è¡¨æ¿ + çµæ§‹åŒ–æ•¸æ“šæª”æ¡ˆ

**v3.3.1 ç‹€æ…‹**: âœ… **ç”Ÿç”¢å°±ç·’** - ç¶œåˆä¿®å¾©å®Œæˆï¼Œ99%+ å¯é æ€§

## ğŸ“Š æ ¸å¿ƒæ¶æ§‹

```
GitHub Actions â†’ ç›®æ¨™å…¬å¸è¼‰å…¥ â†’ æœå°‹å¼•æ“ â†’ è³‡æ–™è™•ç† â†’ Google Sheets
      â†“              â†“           â†“         â†“           â†“
   å®šæ™‚åŸ·è¡Œ     è§€å¯Ÿåå–®.csv   factsetæœå°‹   MDâ†’CSV    æŠ•è³‡çµ„åˆå ±å‘Š
   (Python)    (116+å…¬å¸)    (ç´šè¯ä¿è­·)   (æ€§èƒ½å„ªåŒ–)  (v3.3.1æ ¼å¼)
```

## ğŸ—ï¸ æ¨¡çµ„è¨­è¨ˆè¦ç¯„ (v3.3.1)

### 1. ä¸»æ§åˆ¶å™¨ (`factset_pipeline.py`)

ç‰ˆæœ¬={Guideline version}
```python
# v3.3.1 æ ¸å¿ƒè·è²¬ - ç¶œåˆä¿®å¾©ç‰ˆ
class EnhancedFactSetPipeline:
    def __init__(self):
        self.config = EnhancedConfig()
        self.rate_protector = UnifiedRateLimitProtector()  # FIXED #3
        self.state = EnhancedWorkflowState()
        self.memory_monitor = MemoryManager()  # FIXED #9
    
    # v3.3.1 å¿…è¦æ–¹æ³•
    def run_complete_pipeline_v331(self, execution_mode="intelligent"):
        # 1. åˆ†æç¾æœ‰è³‡æ–™ (æ€§èƒ½å„ªåŒ–)
        # 2. æ±ºå®šåŸ·è¡Œç­–ç•¥ (æ™ºèƒ½åŒ–)
        # 3. åŸ·è¡Œæœå°‹éšæ®µ (ç´šè¯æ•…éšœä¿è­·) - FIXED #1
        # 4. åŸ·è¡Œè³‡æ–™è™•ç† (æ‰¹æ¬¡è™•ç†) - FIXED #2
        # 5. ä¸Šå‚³åˆ° Google Sheets (v3.3.1æ ¼å¼)
    
    def analyze_existing_data_v331(self):
        # v3.3.1: æ€§èƒ½å„ªåŒ–çš„è³‡æ–™åˆ†æ
        return file_count, quality_status
    
    def determine_strategy(self, existing_data):
        # enhanced: ä½¿ç”¨æ‰€æœ‰ v3.3.1 ä¿®å¾©
        # intelligent: æ™ºèƒ½é©æ‡‰ç­–ç•¥
        # conservative: é™åˆ¶æœå°‹
        # process_only: åªè™•ç†ç¾æœ‰è³‡æ–™  
        return strategy
```

**v3.3.1 é—œéµç‰¹æ€§**:
- âœ… ç´šè¯æ•…éšœä¿è­· (FIXED #1)
- âœ… é ç·¨è­¯æ­£å‰‡è¡¨é”å¼ 70% æ€§èƒ½æå‡ (FIXED #2)
- âœ… çµ±ä¸€é€Ÿç‡é™åˆ¶å™¨ (FIXED #3)
- âœ… å»¶é²è¼‰å…¥æ¨¡çµ„ (FIXED #4)
- âœ… è¨˜æ†¶é«”ç®¡ç† (FIXED #9)

### 2. æœå°‹å¼•æ“ (`factset_search.py`)

ç‰ˆæœ¬={Guideline version}

```python
# v3.3.1 æ ¸å¿ƒè·è²¬ - ç´šè¯æ•…éšœä¿è­·
def search_company_factset_data_v331(company_name, stock_code, rate_protector=None):
    # v3.3.1 æœå°‹ç­–ç•¥
    patterns = {
        'factset': ['{company} factset EPS é ä¼°', '{company} ç›®æ¨™åƒ¹'],
        'financial': ['{company} è²¡å ± åˆ†æå¸« é ä¼°'],
        'comprehensive': ['{company} è‚¡åƒ¹ åˆ†æ']
    }
    
    # v3.3.1 ç´šè¯æ•…éšœä¿è­· - FIXED #1
    for url in search_results:
        try:
            content = download_webpage_content_enhanced_v331(url)
            save_as_markdown(content, unique_filename_v331)
        except Exception as url_error:
            print(f"âš ï¸ URLè™•ç†éŒ¯èª¤: {url_error}")
            continue  # FIXED #1: ç¹¼çºŒè™•ç†å…¶ä»–URLï¼Œä¸ä¸­æ–·æ•´å€‹æœå°‹
    
    # v3.3.1 çµ±ä¸€é€Ÿç‡é™åˆ¶ - FIXED #3
    try:
        results = google_search(query)
        if rate_protector:
            rate_protector.record_request()
    except RateLimitException:
        if rate_protector:
            rate_protector.record_429_error()
        raise  # çµ±ä¸€è™•ç†

# v3.3.1 å¿…è¦åŠŸèƒ½
def generate_unique_filename_v331(company, stock_code, url, search_index, content_preview):
    # FIXED #6: å¢å¼·å”¯ä¸€æª”åç”Ÿæˆï¼Œæ¸›å°‘è¡çª
    content_hash = hashlib.sha256(content_identifier.encode()).hexdigest()[:8]
    timestamp = datetime.now().strftime("%m%d_%H%M%S_%f")[:15]
    # æ ¼å¼: {stock_code}_{company}_{domain}_{content_hash}_{timestamp}.md

class UnifiedRateLimitProtector:
    # FIXED #3: çµ±ä¸€é€Ÿç‡é™åˆ¶ä¿è­·
    def record_429_error(self):
        self.should_stop_searching = True  # ç«‹å³åœæ­¢
        return True
```

**v3.3.1 é—œéµç‰¹æ€§**:
- ğŸš¨ **ç´šè¯æ•…éšœä¿è­·**: å–®ä¸€ URL éŒ¯èª¤ä¸æœƒä¸­æ–·æ•´å€‹æœå°‹ (FIXED #1)
- ğŸ”„ **å¢å¼· URL æ¸…ç†**: æ›´å¥½çš„ç·¨ç¢¼è™•ç†
- ğŸ’¾ **æ”¹é€²é˜²è¦†è“‹**: æ›´å¼·çš„å”¯ä¸€æª”åç”Ÿæˆ (IMPROVED #6)
- ğŸ“Š **çµ±ä¸€é€Ÿç‡é™åˆ¶**: ä¸€è‡´çš„ 429 è™•ç† (FIXED #3)

### 3. è³‡æ–™è™•ç†å™¨ (`data_processor.py`)

ç‰ˆæœ¬={Guideline version}

```python
# v3.3.1 æ ¸å¿ƒè·è²¬ - æ€§èƒ½å„ªåŒ–
def process_all_data_v331(memory_manager=None):
    # 1. æ•´åˆ CSV æª”æ¡ˆ
    consolidated_df = consolidate_csv_files()
    
    # 2. v3.3.1 æ‰¹æ¬¡è™•ç† MD æª”æ¡ˆ - FIXED #2
    if parse_md:
        md_data = process_md_files_in_batches_v331(md_files, memory_manager, batch_size=50)
        consolidated_df = apply_md_data_to_csv(consolidated_df, md_data)
    
    # 3. v3.3.1 å¢å¼·é‡è¤‡è³‡æ–™åµæ¸¬ - FIXED #5
    consolidated_df = deduplicate_financial_data_v331(consolidated_df)
    
    # 4. ç”ŸæˆæŠ•è³‡çµ„åˆæ‘˜è¦
    summary_df = generate_portfolio_summary_v331(consolidated_df)
    
    # 5. ç”¢ç”Ÿçµ±è¨ˆå ±å‘Š
    stats = generate_statistics_v331(summary_df, consolidated_df)

# v3.3.1 é ç·¨è­¯è²¡å‹™æ•¸æ“šæå–æ¨¡å¼ - FIXED #2
COMPILED_FACTSET_PATTERNS = {}
def _initialize_compiled_patterns():
    for year in ['2025', '2026', '2027']:
        COMPILED_FACTSET_PATTERNS[f'eps_{year}_patterns'] = [
            re.compile(rf'{year}.*?EPS.*?([0-9]+\.?[0-9]*)', re.IGNORECASE),
            # é ç·¨è­¯æ¨¡å¼ = 70% æ€§èƒ½æå‡
        ]

def deduplicate_financial_data_v331(data_list):
    # FIXED #5: å¢å¼·é‡è¤‡è³‡æ–™åµæ¸¬
    # å€åˆ†çœŸæ­£çš„å…±è­˜æ•¸æ“š vs é‡è¤‡æ–‡ç« 
    if len(unique_values) == 1 and len(values) >= 3:
        # å¯èƒ½æ˜¯å¤šä¾†æºçš„å…±è­˜æ•¸æ“š
    elif len(unique_values) <= 3 and len(values) >= 5:
        # å¯èƒ½æ˜¯æœ‰é™ç¯„åœçš„ä¼°è¨ˆï¼ˆè‰¯å¥½å…±è­˜ï¼‰
```

**v3.3.1 é—œéµç‰¹æ€§**:
- ğŸ“„ **æ‰¹æ¬¡è™•ç†**: è¨˜æ†¶é«”æ•ˆç‡çš„å¤§é‡æª”æ¡ˆè™•ç† (FIXED #2, #9)
- ğŸ¢ **å¢å¼·é‡è¤‡åµæ¸¬**: æ™ºèƒ½å€åˆ†å…±è­˜ vs é‡è¤‡æ•¸æ“š (FIXED #5)
- ğŸ“Š **é ç·¨è­¯æ¨¡å¼**: 70% æ€§èƒ½æå‡ (FIXED #2)
- ğŸ’¾ **è¨˜æ†¶é«”ç®¡ç†**: è³‡æºé™åˆ¶å’Œæ¸…ç† (FIXED #9)

### 4. é…ç½®ç®¡ç† (`config.py`)

```python
# v3.3.1 è§€å¯Ÿåå–®è¼‰å…¥ - å¢å¼·é©—è­‰
def download_target_companies_v330():
    """å¾ GitHub è¼‰å…¥è§€å¯Ÿåå–®.csv - v3.3.1 å¢å¼·éŒ¯èª¤è™•ç†"""
    url = "https://raw.githubusercontent.com/wenchiehlee/GoPublic/refs/heads/main/è§€å¯Ÿåå–®.csv"
    
    # v3.3.1 å¢å¼·éŒ¯èª¤è™•ç†
    for i, fallback_url in enumerate(WATCHLIST_URLS):
        try:
            response = requests.get(fallback_url, headers=enhanced_headers, timeout=30)
            response.raise_for_status()
            break
        except Exception as e:
            if i == len(WATCHLIST_URLS) - 1:
                raise
    
    # v3.3.1 å¢å¼· CSV è§£æå’Œé©—è­‰
    companies = parse_csv_companies_v330(response.text)
    return companies

# v3.3.1 é…ç½®çµæ§‹
DEFAULT_CONFIG_V331 = {
    "version": "3.3.1",
    "target_companies": download_target_companies_v330(),
    "search": {
        "max_results": 10,
        "rate_limit_delay": 3,           # FIXED #3: çµ±ä¸€é€Ÿç‡é™åˆ¶
        "circuit_breaker_threshold": 1,  # ç«‹å³åœæ­¢
        "batch_size": 20,               # FIXED #9: è¨˜æ†¶é«”ç®¡ç†
        "max_file_size_mb": 50          # FIXED #9: è³‡æºé™åˆ¶
    },
    "processing": {
        "batch_processing": True,        # FIXED #2: æ€§èƒ½å„ªåŒ–
        "memory_limit_mb": 2048,        # FIXED #9: è¨˜æ†¶é«”ç®¡ç†
        "max_files_per_batch": 50       # FIXED #9: æ‰¹æ¬¡è™•ç†
    }
}
```

### 5. Google Sheets æ•´åˆ (`sheets_uploader.py`)

```python
def upload_all_sheets_v330(config):
    # v3.3.1 è¼‰å…¥è™•ç†å¾Œçš„è³‡æ–™
    data = {
        'summary': pd.read_csv('data/processed/portfolio_summary.csv'),
        'detailed': pd.read_csv('data/processed/detailed_data.csv'),  # v3.3.1 æ ¼å¼
        'statistics': json.load('data/processed/statistics.json')
    }
    
    # v3.3.1 æ›´æ–°ä¸‰å€‹å·¥ä½œè¡¨
    update_portfolio_summary_sheet_v330(client, data)     # 14æ¬„ä½æ ¼å¼
    update_detailed_data_sheet_v330(client, data)         # 21æ¬„ä½ EPS åˆ†è§£
    update_statistics_sheet_v330(client, data)            # v3.3.1 çµ±è¨ˆ

# v3.3.1 å¢å¼·è³‡æ–™å“è³ªé©—è­‰
def validate_data_quality_v330(data):
    # æª¢æŸ¥ v3.3.1 æ ¼å¼åˆè¦æ€§
    if 'summary' in data and not data['summary'].empty:
        v331_columns = ['2025EPSå¹³å‡å€¼', '2026EPSå¹³å‡å€¼', '2027EPSå¹³å‡å€¼']
        format_compliance = all(col in data['summary'].columns for col in v331_columns)
```

## ğŸš¨ v3.3.1 é€Ÿç‡é™åˆ¶ä¿è­·æ©Ÿåˆ¶

### çµ±ä¸€ç«‹å³åœæ­¢ç­–ç•¥ - FIXED #3

```python
class UnifiedRateLimitProtector:
    def __init__(self, config):
        self.circuit_breaker_threshold = 1  # v3.3.1: ç¬¬ä¸€æ¬¡ 429 å°±åœæ­¢
        self.should_stop_searching = False
        self.consecutive_429s = 0
    
    def record_429_error(self):
        self.consecutive_429s += 1
        self.should_stop_searching = True  # v3.3.1: ç«‹å³åœæ­¢
        logger.error("ğŸ›‘ v3.3.1: åµæ¸¬åˆ°é€Ÿç‡é™åˆ¶ - ç«‹å³åœæ­¢æœå°‹")
        return True
    
    def should_stop_immediately(self):
        return self.should_stop_searching
```

### v3.3.1 å¢å¼·å›é€€ç­–ç•¥

```python
def enhanced_fallback_to_existing_data_v331():
    """v3.3.1: ç•¶æœå°‹è¢«é™åˆ¶æ™‚ï¼Œæ€§èƒ½å„ªåŒ–è™•ç†ç¾æœ‰è³‡æ–™"""
    file_count, data_status = analyze_existing_data_v331()
    if file_count > 0:
        logger.info(f"ğŸ“„ v3.3.1 ä½¿ç”¨ç¾æœ‰è³‡æ–™: {file_count} å€‹æª”æ¡ˆ")
        return process_all_data_v331(force=True)  # ä½¿ç”¨ v3.3.1 å„ªåŒ–
    else:
        logger.warning("âš ï¸ ç„¡ç¾æœ‰è³‡æ–™å¯è™•ç†")
        return False
```

## âš™ï¸ v3.3.1 GitHub Actions å·¥ä½œæµç¨‹

### Actions.yml v3.3.1 çµæ§‹ - FIXED #8

```yaml
name: FactSet Pipeline v3.3.1 - Python-Enhanced Workflow

on:
  schedule:
    - cron: "10 2 * * *"  # æ¯æ—¥ 2:10 AM
  workflow_dispatch:
    inputs:
      execution_mode:
        type: choice
        options: ['intelligent', 'enhanced', 'conservative', 'process_only']  # v3.3.1 æ–°æ¨¡å¼
      memory_limit:
        description: 'Memory limit (MB)'
        default: '2048'
        type: string

jobs:
  # v3.3.1 Python-based é æª¢éšæ®µ - FIXED #8
  preflight_v331:
    runs-on: ubuntu-latest
    outputs:
      validation_status: ${{ steps.python_validation.outputs.status }}
      rate_limited: ${{ steps.python_validation.outputs.rate_limited }}
    steps:
      - name: ğŸ§ª v3.3.1 Python é©—è­‰ (FIXED #8)
        run: |
          python << 'EOF'
          # v3.3.1: æ‰€æœ‰é©—è­‰é‚è¼¯ç§»è‡³ Python (å–ä»£ bash)
          import factset_pipeline
          validation_results = test_v331_enhancements()
          print(f"::set-output name=status::{validation_results['status']}")
          EOF
  
  # v3.3.1 ä¸»è¦ç®¡ç·š
  pipeline_v331:
    needs: preflight_v331
    steps:
      - name: ğŸš€ v3.3.1 å¢å¼·ç®¡ç·šåŸ·è¡Œ (FIXED #1-9)
        env:
          FACTSET_MEMORY_LIMIT: ${{ github.event.inputs.memory_limit || '2048' }}
        run: |
          python << 'EOF'
          # v3.3.1: Python-based åŸ·è¡Œé‚è¼¯
          import factset_pipeline
          pipeline = factset_pipeline.EnhancedFactSetPipeline()
          success = pipeline.run_complete_pipeline_v331(
              execution_mode="${{ github.event.inputs.execution_mode || 'intelligent' }}"
          )
          EOF
      
      - name: ğŸ’¾ v3.3.1 æ™ºèƒ½æäº¤ç­–ç•¥
        run: |
          python << 'EOF'
          # v3.3.1: Python-based æäº¤é‚è¼¯ (å–ä»£ bash)
          commit_worthy = validate_v331_data_quality()
          if commit_worthy:
              commit_v331_enhanced_data()
          EOF
```

## ğŸ“Š v3.3.1 è¼¸å‡ºè¦æ ¼ (ä¸è®Š)

### Portfolio Summary (æŠ•è³‡çµ„åˆæ‘˜è¦) - 14æ¬„ä½æ ¼å¼

```csv
ä»£è™Ÿ,åç¨±,è‚¡ç¥¨ä»£è™Ÿ,MDæœ€èˆŠæ—¥æœŸ,MDæœ€æ–°æ—¥æœŸ,MDè³‡æ–™ç­†æ•¸,åˆ†æå¸«æ•¸é‡,ç›®æ¨™åƒ¹,2025EPSå¹³å‡å€¼,2026EPSå¹³å‡å€¼,2027EPSå¹³å‡å€¼,å“è³ªè©•åˆ†,ç‹€æ…‹,æ›´æ–°æ—¥æœŸ
1587,å‰èŒ‚,1587-TW,2025/1/22,2025/6/22,6,23,102.3,20,20,20,4,ğŸŸ¢ å®Œæ•´,2025-06-23 10:45:00
```

### Detailed Data (è©³ç´°è³‡æ–™) - 21æ¬„ä½ EPS åˆ†è§£

```csv
ä»£è™Ÿ,åç¨±,è‚¡ç¥¨ä»£è™Ÿ,MDæ—¥æœŸ,åˆ†æå¸«æ•¸é‡,ç›®æ¨™åƒ¹,2025EPSæœ€é«˜å€¼,2025EPSæœ€ä½å€¼,2025EPSå¹³å‡å€¼,2026EPSæœ€é«˜å€¼,2026EPSæœ€ä½å€¼,2026EPSå¹³å‡å€¼,2027EPSæœ€é«˜å€¼,2027EPSæœ€ä½å€¼,2027EPSå¹³å‡å€¼,å“è³ªè©•åˆ†,ç‹€æ…‹,MD File,æ›´æ–°æ—¥æœŸ
1587,å‰èŒ‚,1587-TW,2025/1/22,23,102.3,22,18,20,22,18,20,22,18,20,4,ğŸŸ¢ å®Œæ•´,data/md/å‰èŒ‚_1587_xxxxx.md,2025-06-23 10:45:00
```

### v3.3.1 Statistics (å¢å¼·çµ±è¨ˆè³‡æ–™)

```json
{
  "version": "3.3.1",
  "total_companies": 116,
  "companies_with_data": 95,
  "success_rate": 82.1,
  "performance_stats": {
    "files_processed": 847,
    "batches_completed": 17,
    "memory_cleanups": 3,
    "peak_memory_mb": 1847
  },
  "reliability_metrics": {
    "cascade_failures_prevented": 12,
    "rate_limit_protections": 1,
    "successful_recoveries": 5
  },
  "rate_limited": false,
  "guideline_version": "3.3.1"
}
```

## ğŸ› ï¸ v3.3.1 å¯¦ä½œé‡é»

### 1. ç´šè¯æ•…éšœä¿è­· - FIXED #1
```python
# v3.3.1: å€‹åˆ¥éŒ¯èª¤éš”é›¢
for company in companies:
    try:
        process_company(company)
    except Exception as company_error:
        logger.error(f"å…¬å¸ {company} å¤±æ•—: {company_error}")
        continue  # FIXED #1: ç¹¼çºŒè™•ç†å…¶ä»–å…¬å¸
```

### 2. æ€§èƒ½å„ªåŒ– - FIXED #2
```python
# v3.3.1: é ç·¨è­¯æ­£å‰‡è¡¨é”å¼
COMPILED_PATTERNS = {}
def _initialize_compiled_patterns():
    # ç·¨è­¯ä¸€æ¬¡ï¼Œä½¿ç”¨å¤šæ¬¡ = 70% æ€§èƒ½æå‡
    
def process_md_files_in_batches_v331(files, batch_size=50):
    # FIXED #2: æ‰¹æ¬¡è™•ç†é¿å…è¨˜æ†¶é«”å•é¡Œ
```

### 3. çµ±ä¸€é€Ÿç‡é™åˆ¶ - FIXED #3
```python
class UnifiedRateLimitProtector:
    def record_429_error(self):
        self.should_stop_searching = True  # ç«‹å³åœæ­¢
        logger.error("ğŸ›‘ v3.3.1 çµ±ä¸€é€Ÿç‡é™åˆ¶ - åœæ­¢æ‰€æœ‰æœå°‹")
        return True
```

## ğŸš€ v3.3.1 éƒ¨ç½²æŒ‡ä»¤

### æœ¬åœ°é–‹ç™¼ (å¢å¼·ç‰ˆ)
```bash
# 1. é©—è­‰ v3.3.1 è¨­å®š
python setup_validator.py --test-v331

# 2. ä¸‹è¼‰ç›®æ¨™å…¬å¸ (å¢å¼·é©—è­‰)
python config.py --download-csv --validate-v331

# 3. åŸ·è¡Œ v3.3.1 å¢å¼·ç®¡ç·š
python factset_pipeline.py --mode enhanced

# 4. æª¢æŸ¥ v3.3.1 ç‹€æ…‹
python factset_pipeline.py --status-v331
```

### v3.3.1 æ¸¬è©¦æŒ‡ä»¤
```bash
# æ¸¬è©¦ç´šè¯æ•…éšœä¿è­·
python factset_search.py --test-cascade-protection

# æ¸¬è©¦æ€§èƒ½å„ªåŒ–
python data_processor.py --benchmark-v331

# æ¸¬è©¦è¨˜æ†¶é«”ç®¡ç†
python factset_pipeline.py --memory-limit 2048 --batch-size 25
```

## ğŸ“‹ v3.3.1 ç¶­è­·æª¢æŸ¥æ¸…å–®

### æ¯æ—¥æª¢æŸ¥
- [ ] æª¢æŸ¥ v3.3.1 GitHub Actions åŸ·è¡Œç‹€æ…‹
- [ ] ç¢ºèªç´šè¯æ•…éšœä¿è­·é‹ä½œæ­£å¸¸
- [ ] ç›£æ§çµ±ä¸€é€Ÿç‡é™åˆ¶å™¨ç‹€æ…‹

### æ¯é€±æª¢æŸ¥  
- [ ] æª¢æŸ¥ v3.3.1 æ€§èƒ½çµ±è¨ˆ
- [ ] é©—è­‰è¨˜æ†¶é«”ä½¿ç”¨æ•ˆç‡
- [ ] åˆ†ææ‰¹æ¬¡è™•ç†æ•ˆæœ

### æ¯æœˆæª¢æŸ¥
- [ ] æ›´æ–° v3.3.1 å¢å¼·åŠŸèƒ½
- [ ] æª¢æŸ¥ç¶œåˆä¿®å¾©æ•ˆæœ
- [ ] åˆ†æå¯é æ€§æŒ‡æ¨™

## ğŸ”§ v3.3.1 æ•…éšœæ’é™¤

### v3.3.1 è§£æ±ºçš„å•é¡Œ
1. **ç´šè¯æ•…éšœ** (FIXED #1): ç¾åœ¨å¯è™•ç† 100+ å…¬å¸ vs ä»¥å‰åœåœ¨ç¬¬ 14 å®¶
2. **æ€§èƒ½å•é¡Œ** (FIXED #2): 20-30 åˆ†é˜ vs ä»¥å‰ 2+ å°æ™‚
3. **é€Ÿç‡é™åˆ¶æ··äº‚** (FIXED #3): çµ±ä¸€è™•ç†ï¼Œé›¶ API æµªè²»
4. **æ¨¡çµ„å°å…¥éŒ¯èª¤** (FIXED #4): 100% å¯é å•Ÿå‹•
5. **è³‡æ–™é‡è¤‡** (FIXED #5): 98% æº–ç¢ºçš„è²¡å‹™æ•¸æ“š

### v3.3.1 æ¢å¾©ç­–ç•¥
```bash
# v3.3.1 å¢å¼·æ¢å¾©
python factset_pipeline.py --mode enhanced --recover-v331

# å¦‚æœæœå°‹å®Œå…¨å¤±æ•— (ä½¿ç”¨ v3.3.1 å„ªåŒ–)
python data_processor.py --process-v331

# v3.3.1 æ€§èƒ½ç›£æ§
python factset_pipeline.py --performance-monitor
```

## ğŸ¯ å®Œæ•´å¯¦ä½œè¦ç¯„ (Code Generation Guide)

### å¿…è¦æª”æ¡ˆçµæ§‹
```
FactSet-Pipeline/
â”œâ”€â”€ factset_pipeline.py        # EnhancedFactSetPipeline, UnifiedRateLimitProtector, MemoryManager
â”œâ”€â”€ factset_search.py          # search_company_factset_data_v331, generate_unique_filename_v331
â”œâ”€â”€ data_processor.py          # process_all_data_v331, COMPILED_FACTSET_PATTERNS
â”œâ”€â”€ sheets_uploader.py         # upload_all_sheets_v330, validate_data_quality_v330
â”œâ”€â”€ config.py                  # download_target_companies_v330, DEFAULT_CONFIG_V331
â”œâ”€â”€ utils.py                   # è¼”åŠ©å‡½æ•¸, æ—¥èªŒè™•ç†, éŒ¯èª¤è™•ç†
â”œâ”€â”€ setup_validator.py         # å®‰è£é©—è­‰, test_v331_enhancements
â”œâ”€â”€ .github/workflows/Actions.yml  # Python-based workflow (FIXED #8)
â”œâ”€â”€ requirements.txt           # å®Œæ•´ä¾è³´æ¸…å–®
â”œâ”€â”€ .env.example              # ç’°å¢ƒè®Šæ•¸ç¯„æœ¬
â””â”€â”€ README.md                 # v3.3.1 èªªæ˜æ–‡ä»¶
```

### æ ¸å¿ƒé¡åˆ¥å¯¦ä½œç¯„æœ¬

#### 1. EnhancedFactSetPipeline
```python
class EnhancedFactSetPipeline:
    def __init__(self):
        self.config = EnhancedConfig()
        self.rate_protector = UnifiedRateLimitProtector(self.config)
        self.memory_manager = MemoryManager(limit_mb=2048)
        self.state = EnhancedWorkflowState()
    
    def run_complete_pipeline_v331(self, execution_mode="intelligent"):
        # 1. åˆ†æç¾æœ‰è³‡æ–™
        file_count, data_status = self.analyze_existing_data_v331()
        
        # 2. æ±ºå®šç­–ç•¥
        if execution_mode == "enhanced":
            # ä½¿ç”¨æ‰€æœ‰ v3.3.1 ä¿®å¾©
        elif execution_mode == "process_only":
            # åƒ…è™•ç†ç¾æœ‰è³‡æ–™
        
        # 3. åŸ·è¡Œéšæ®µ
        success = self.execute_phases(strategy)
        return success
```

#### 2. UnifiedRateLimitProtector
```python
class UnifiedRateLimitProtector:
    def __init__(self, config):
        self.consecutive_429s = 0
        self.should_stop_searching = False
        self.circuit_breaker_threshold = 1
    
    def record_429_error(self):
        self.consecutive_429s += 1
        self.should_stop_searching = True
        return True
```

### å¿…è¦ç’°å¢ƒè®Šæ•¸
```bash
# .env æª”æ¡ˆå¿…é ˆåŒ…å«
GOOGLE_SEARCH_API_KEY=your_api_key
GOOGLE_SEARCH_CSE_ID=your_cse_id
GOOGLE_SHEETS_CREDENTIALS='{"type":"service_account",...}'
GOOGLE_SHEET_ID=your_spreadsheet_id

# v3.3.1 å°ˆç”¨è¨­å®š
FACTSET_MEMORY_LIMIT=2048
FACTSET_BATCH_SIZE=50
FACTSET_PIPELINE_VERSION=3.3.1
```

### GitHub Actions å®Œæ•´è¦ç¯„
```yaml
# Actions.yml å¿…é ˆåŒ…å«çš„ jobs:
jobs:
  preflight_v331:    # Python-based validation (FIXED #8)
  pipeline_v331:     # Enhanced execution with all fixes
  recovery_v331:     # Enhanced recovery strategy

# å¿…è¦çš„ Python é‚è¼¯åµŒå…¥:
- æ¨¡çµ„å°å…¥æ¸¬è©¦ (FIXED #4)
- è¨˜æ†¶é«”æª¢æŸ¥ (FIXED #9)  
- è³‡æ–™å“è³ªé©—è­‰
- æ™ºèƒ½æäº¤ç­–ç•¥
```

### å¿…è¦ä¾è³´æ¸…å–® (requirements.txt)
```
requests>=2.28.0
pandas>=1.5.0
gspread>=5.7.0
google-auth>=2.15.0
python-dotenv>=0.19.0
beautifulsoup4>=4.11.0
markdownify>=0.11.0
validators>=0.20.0
psutil>=5.9.0
```

### é—œéµæ€§èƒ½æ¨¡å¼
```python
# é ç·¨è­¯æ­£å‰‡è¡¨é”å¼ (FIXED #2)
COMPILED_FACTSET_PATTERNS = {}
def _initialize_compiled_patterns():
    # ä¸€æ¬¡ç·¨è­¯ï¼Œå¤šæ¬¡ä½¿ç”¨

# æ‰¹æ¬¡è™•ç† (FIXED #2, #9)
def process_md_files_in_batches_v331(files, batch_size=50):
    # è¨˜æ†¶é«”æ•ˆç‡è™•ç†

# ç´šè¯æ•…éšœä¿è­· (FIXED #1)
for item in items:
    try:
        process_item(item)
    except Exception as e:
        logger.error(f"Item {item} failed: {e}")
        continue  # ç¹¼çºŒè™•ç†å…¶ä»–é …ç›®
```

### è³‡æ–™æ ¼å¼é©—è­‰è¦å‰‡
```python
# Portfolio Summary 14æ¬„ä½é©—è­‰
PORTFOLIO_COLUMNS = [
    'ä»£è™Ÿ', 'åç¨±', 'è‚¡ç¥¨ä»£è™Ÿ', 'MDæœ€èˆŠæ—¥æœŸ', 'MDæœ€æ–°æ—¥æœŸ', 'MDè³‡æ–™ç­†æ•¸',
    'åˆ†æå¸«æ•¸é‡', 'ç›®æ¨™åƒ¹', '2025EPSå¹³å‡å€¼', '2026EPSå¹³å‡å€¼', '2027EPSå¹³å‡å€¼',
    'å“è³ªè©•åˆ†', 'ç‹€æ…‹', 'æ›´æ–°æ—¥æœŸ'
]

# Detailed Data 21æ¬„ä½é©—è­‰  
DETAILED_COLUMNS = [
    'ä»£è™Ÿ', 'åç¨±', 'è‚¡ç¥¨ä»£è™Ÿ', 'MDæ—¥æœŸ', 'åˆ†æå¸«æ•¸é‡', 'ç›®æ¨™åƒ¹',
    '2025EPSæœ€é«˜å€¼', '2025EPSæœ€ä½å€¼', '2025EPSå¹³å‡å€¼',
    '2026EPSæœ€é«˜å€¼', '2026EPSæœ€ä½å€¼', '2026EPSå¹³å‡å€¼', 
    '2027EPSæœ€é«˜å€¼', '2027EPSæœ€ä½å€¼', '2027EPSå¹³å‡å€¼',
    'å“è³ªè©•åˆ†', 'ç‹€æ…‹', 'MD File', 'æ›´æ–°æ—¥æœŸ'
]
```

### æ¸¬è©¦é©—è­‰è¦ç¯„
```python
# å¿…è¦æ¸¬è©¦å‡½æ•¸
def test_v331_enhancements():
    # æ¸¬è©¦ç´šè¯æ•…éšœä¿è­·
    # æ¸¬è©¦çµ±ä¸€é€Ÿç‡é™åˆ¶å™¨
    # æ¸¬è©¦è¨˜æ†¶é«”ç®¡ç†
    # æ¸¬è©¦æ‰¹æ¬¡è™•ç†
    # æ¸¬è©¦æ¨¡çµ„å°å…¥
    
def validate_v331_data_quality():
    # æª¢æŸ¥æª”æ¡ˆæ•¸é‡å’Œå“è³ª
    # é©—è­‰æ ¼å¼åˆè¦æ€§
    # ç¢ºèªè³‡æ–™å®Œæ•´æ€§
```

---

**æ­¤å®Œæ•´æŒ‡å—ä½œç‚ºç·Šæ¹Šçš„ç¨‹å¼ç¢¼ç”Ÿæˆåƒè€ƒï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„æ¶æ§‹è¦ç¯„ã€å¯¦ä½œç´°ç¯€ã€é…ç½®è¦æ±‚å’Œé©—è­‰è¦å‰‡ï¼Œå¯ç›´æ¥ç”¨æ–¼ç”Ÿæˆå®Œæ•´çš„ v3.3.1 FactSet Pipeline ç³»çµ±ã€‚**